# ğŸš€ Reinforcement Learning å®æˆ˜é¡¹ç›®

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-success.svg)]()

**ä»åŸºç¡€åˆ°å‰æ²¿çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ**

[å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [ç®—æ³•å®ç°](#-ç®—æ³•å®ç°) â€¢ [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„) â€¢ [æ–‡æ¡£](#-æ–‡æ¡£)

</div>

---

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°é›†åˆ**ï¼Œæ¶µç›–ä»ç»å…¸ç®—æ³•åˆ°æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚é¡¹ç›®åˆ†ä¸ºä¸¤å¤§éƒ¨åˆ†ï¼š

- **RL_basic**: ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆQ-Learning, SARSA, DQN, Policy Gradientï¼‰
- **RL_HAND**: å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPO, GRPO, DAPO, GSPOç­‰ï¼‰

æ‰€æœ‰ä»£ç éƒ½ç»è¿‡å®é™…æµ‹è¯•ï¼Œé…æœ‰è¯¦ç»†æ–‡æ¡£ï¼Œé€‚åˆå­¦ä¹ ã€ç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚

---

## ğŸ“ ç®—æ³•å®ç°

### ğŸ“˜ ç»å…¸å¼ºåŒ–å­¦ä¹ ç®—æ³• (RL_basic)

åŸºäºè¿·å®«ç¯å¢ƒçš„ç»å…¸ç®—æ³•å®ç°ï¼Œé€‚åˆå…¥é—¨å­¦ä¹ ï¼š

| ç®—æ³• | ç±»å‹ | ç‰¹ç‚¹ | æ–‡ä»¶ |
|------|------|------|------|
| **Q-Learning** | Value-Based | Off-policy, è¡¨æ ¼æ³• | `maze_value_iteration_q_learning.py` |
| **SARSA** | Value-Based | On-policy, è¡¨æ ¼æ³• | `maze_value_iteration_TD_sarsa.py` |
| **Expected SARSA** | Value-Based | On-policy, æœŸæœ›æ›´æ–° | `maze_value_iteration_TD_expected_sarsa.py` |
| **DQN** | Value-Based | Deep Q-Network | `DQN_cartpole.py` |
| **Policy Gradient** | Policy-Based | ç­–ç•¥æ¢¯åº¦ | `maze_policy_gradient.py` |
| **Deep PG** | Policy-Based | æ·±åº¦ç­–ç•¥æ¢¯åº¦ | `deep_policy_gradient_cartpole.py` |

**é€‚ç”¨åœºæ™¯**: å­¦ä¹ å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µï¼Œç†è§£ç®—æ³•åŸç†

---

### ğŸ”¥ å¤§è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹  (RL_HAND)


---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
- **Python**: 3.8+
- **CUDA**: 11.8+ (æ¨è)
- **GPU**: 16GB+ æ˜¾å­˜ï¼ˆæ¨è RTX 4090/5060Tiï¼‰
- **å†…å­˜**: 32GB+

#### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/yourusername/Reinforcement_Learning.git
cd Reinforcement_Learning

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–æ‰‹åŠ¨å®‰è£…
pip install torch>=2.0.0 transformers>=4.30.0 datasets>=2.0.0 \
    pyarrow>=12.0.0 matplotlib>=3.5.0 tqdm>=4.64.0
```

---

### 2. ç»å…¸ç®—æ³•å¿«é€Ÿä½“éªŒ

```bash
# è¿›å…¥åŸºç¡€ç®—æ³•ç›®å½•
cd RL_basic

# è¿è¡Œ Q-Learning
python maze_value_iteration_q_learning.py

# è¿è¡Œ DQN
python DQN_cartpole.py

# è¿è¡Œç­–ç•¥æ¢¯åº¦
python maze_policy_gradient.py
```

---

### 3. å¤§è¯­è¨€æ¨¡å‹ RLHF è®­ç»ƒ

#### å‡†å¤‡æ¨¡å‹

ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼ˆæˆ–ä¿®æ”¹ä»£ç ä¸­çš„è·¯å¾„ï¼‰ï¼š
```python
# ç­–ç•¥æ¨¡å‹ï¼ˆå¦‚ Qwen, LLaMA ç­‰ï¼‰
POLICY_MODEL = "path/to/your/model"

# å¥–åŠ±æ¨¡å‹
REWARD_MODEL = "path/to/reward/model"
```

#### å‡†å¤‡æ•°æ®

æ”¯æŒä¸¤ç§æ ¼å¼ï¼š

**Parquet æ ¼å¼**:
```python
train_datasets = [{
    "path": "data.parquet",
    "type": "parquet",
    "input": "question",
    "output": "answer"
}]
```

**JSONL æ ¼å¼**:
```python
train_datasets = [{
    "path": "data.jsonl",
    "type": "jsonl",
    "input": "problem",
    "output": "solution"
}]
```

#### å¼€å§‹è®­ç»ƒ

```bash
# PPO è®­ç»ƒï¼ˆå­¦ä¹ ç‰ˆï¼‰
cd RL_HAND/ppo
python ppo.py

# PPO è®­ç»ƒï¼ˆç”Ÿäº§ç‰ˆï¼Œæ¨èï¼‰
python ppo_v1.py

# PPO è®­ç»ƒï¼ˆé«˜æ•ˆç‰ˆï¼Œå¸¦ç»éªŒå›æ”¾ï¼‰
python ppo_v2.py

# GRPO è®­ç»ƒ
cd ../grpo
python grpo.py

# DAPO è®­ç»ƒ
cd ../dapo
python dapo.py

# GSPO è®­ç»ƒ
cd ../gspo
python gspo.py
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®

æ‰€æœ‰ RL_HAND ç®—æ³•éƒ½æ”¯æŒä»¥ä¸‹é…ç½®ï¼š

```python
# æ¨¡å‹è·¯å¾„
POLICY_MODEL = "path/to/policy/model"
REWARD_MODEL = "path/to/reward/model"

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 4              # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 1e-6        # å­¦ä¹ ç‡
NUM_EPOCHS = 1              # è®­ç»ƒè½®æ•°
DTYPE = torch.bfloat16      # æ•°æ®ç±»å‹

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "output/path"
```

### ç®—æ³•ç‰¹å®šé…ç½®

è¯¦è§å„ç®—æ³•çš„æ–‡æ¡£ï¼š
- PPO: [ppo_v1]_ä½¿ç”¨æŒ‡å—.md
- GRPO: grpo/README.md
- DAPO: dapo/README.md
- GSPO: gspo/README.md

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### è‡ªåŠ¨ç”Ÿæˆçš„å†…å®¹

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ç”Ÿæˆï¼š
- âœ… è®­ç»ƒæŒ‡æ ‡å›¾è¡¨ï¼ˆPNGï¼‰
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹
- âœ… è®­ç»ƒæ—¥å¿—ï¼ˆJSONLï¼‰
- âœ… é…ç½®æ–‡ä»¶å¤‡ä»½

### å¯è§†åŒ–å·¥å…·

ä½¿ç”¨ `utils/plot_metrics.py` ç»˜åˆ¶è‡ªå®šä¹‰å›¾è¡¨ï¼š

```python
from utils.plot_metrics import plot_training_metrics

plot_training_metrics(
    metrics_history=your_metrics,
    save_path="metrics.png"
)
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### PPO v1 ç”Ÿäº§çº§åŠŸèƒ½
- âœ… å­¦ä¹ ç‡è°ƒåº¦ï¼ˆCosine/Plateauï¼‰
- âœ… æ¢¯åº¦ç´¯ç§¯
- âœ… æ£€æŸ¥ç‚¹æ¢å¤
- âœ… æ—©åœæœºåˆ¶
- âœ… éªŒè¯é›†è¯„ä¼°
- âœ… Wandb é›†æˆ
- âœ… è¯¦ç»†æ—¥å¿—

### PPO v2 ç»éªŒå›æ”¾
- âœ… ç»éªŒå›æ”¾ç¼“å†²åŒº
- âœ… ä¼˜å…ˆçº§é‡‡æ ·
- âœ… é‡è¦æ€§é‡‡æ ·ä¿®æ­£
- âœ… æ ·æœ¬ç®¡ç†ç­–ç•¥
- âœ… æ ·æœ¬æ•ˆç‡æå‡ 2-4x

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

### è´¡çŒ®æ–¹å‘
- ğŸ¯ æ·»åŠ æ–°çš„ç®—æ³•å®ç°
- ğŸ“š æ”¹è¿›æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸ› ä¿®å¤ Bug
- âœ¨ æ·»åŠ æ–°åŠŸèƒ½
- ğŸ§ª æ·»åŠ æµ‹è¯•ç”¨ä¾‹

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v2.0.0 (2026-01-20)
- âœ¨ æ–°å¢ PPO v2ï¼ˆç»éªŒå›æ”¾ç‰ˆæœ¬ï¼‰
- âœ¨ æ–°å¢ GSPO ç®—æ³•å®ç°
- ğŸ“š é‡ç»„ PPO æ–‡æ¡£ç»“æ„
- ğŸ“š æ·»åŠ  12 ä¸ªè¯¦ç»†æ–‡æ¡£
- ğŸ”§ ä¼˜åŒ–ä»£ç ç»“æ„

### v1.0.0 (2026-01-18)
- âœ¨ æ–°å¢ PPO v1ï¼ˆç”Ÿäº§çº§ç‰ˆæœ¬ï¼‰
- âœ¨ æ–°å¢ GRPO ç®—æ³•å®ç°
- âœ¨ æ–°å¢ DAPO ç®—æ³•å®ç°
- ğŸ“š å®Œå–„æ–‡æ¡£ç³»ç»Ÿ

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

### ç»å…¸ç®—æ³•
1. Sutton & Barto - Reinforcement Learning: An Introduction
2. Mnih et al. - Playing Atari with Deep Reinforcement Learning (DQN)
3. Schulman et al. - Proximal Policy Optimization Algorithms (PPO)

### å¤§è¯­è¨€æ¨¡å‹ RL
1. [PPO åŸè®ºæ–‡](https://arxiv.org/abs/1707.06347)
2. DeepSeekMath: Pushing the Limits of Mathematical Reasoning
3. [DAPO è®ºæ–‡](https://arxiv.org/abs/2512.07611)
4. GRPO: Group Relative Policy Optimization

---

## ğŸ“§ è”ç³»æ–¹å¼

- **ä½œè€…**: YoungL
- **é‚®ç®±**: lby15356@gmail.com
- **é¡¹ç›®**: [GitHub Repository](https://github.com/yourusername/Reinforcement_Learning)

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [chunhuizhang/bilibili_vlogs](https://github.com/chunhuizhang/bilibili_vlogs) æä¾›çš„åŸºç¡€ç®—æ³•å‚è€ƒ
- æ„Ÿè°¢ OpenAIã€Qwen ç­‰å›¢é˜Ÿçš„å¼€æºè´¡çŒ®

---

## â­ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star â­ï¸

---

<div align="center">

**[â¬† å›åˆ°é¡¶éƒ¨](#-reinforcement-learning-å®æˆ˜é¡¹ç›®)**

Made with â¤ï¸ by YoungL

</div>
