#!/usr/bin/env python3
"""
VAPO (Value-based Augmented Proximal Policy Optimization) è®­ç»ƒè„šæœ¬ - åŸºäºQwen2-0.5B
VAPOæ˜¯PPOçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸“ä¸ºé•¿CoTæ¨ç†ä»»åŠ¡è®¾è®¡ï¼Œé€šè¿‡ä»¥ä¸‹æŠ€æœ¯æå‡æ€§èƒ½ï¼š
1. Value-Pretraining: ç¼“è§£ä»·å€¼æ¨¡å‹åˆå§‹åŒ–åå·®
2. Decoupled-GAE: è§£è€¦ä»·å€¼å’Œç­–ç•¥çš„ä¼˜åŠ¿è®¡ç®—
3. Length-Adaptive GAE: æ ¹æ®åºåˆ—é•¿åº¦è‡ªé€‚åº”è°ƒæ•´Î»å‚æ•°
4. Token-Level Loss: tokençº§åˆ«çš„ç­–ç•¥æ¢¯åº¦æŸå¤±
5. Clip-Higher: éå¯¹ç§°è£å‰ªèŒƒå›´
6. Group-Sampling: ç»„å†…é‡‡æ ·å¢å¼ºå¯¹æ¯”ä¿¡å·
7. Positive Example LM Loss: è‡ªæ¨¡ä»¿å­¦ä¹ 
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, pipeline
)
from datasets import load_dataset
import numpy as np
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
import wandb
from tqdm import tqdm
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class VAPOConfig:
    """VAPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    policy_model_name: str = "Qwen/Qwen2-0.5B"
    critic_model_name: str = "Qwen/Qwen2-0.5B"
    reward_model_name: str = "OpenAssistant/reward-model-deberta-v3-large-v2"
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    mini_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    learning_rate: float = 1e-6  # VAPOä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
    critic_learning_rate: float = 2e-6  # criticå­¦ä¹ ç‡æ›´å¤§ï¼Œéœ€è¦æ›´å¿«æ›´æ–°
    num_epochs: int = 3
    max_length: int = 512
    
    # VAPOç‰¹æœ‰è¶…å‚æ•°
    vapo_epochs: int = 4
    clip_range_low: float = 0.2  # ğŸ”¥ Clip-Higher: ä¸‹ç•Œè£å‰ªèŒƒå›´
    clip_range_high: float = 0.28  # ğŸ”¥ Clip-Higher: ä¸Šç•Œè£å‰ªèŒƒå›´æ›´å¤§
    entropy_coef: float = 0.01
    vf_coef: float = 0.1
    gamma: float = 1.0  # VAPOä½¿ç”¨Î³=1.0
    
    # ğŸ”¥ Decoupled-GAEå‚æ•°
    lambda_critic: float = 1.0  # criticä½¿ç”¨Î»=1.0ï¼Œæ— åä¼°è®¡
    lambda_policy_base: float = 0.95  # policyçš„åŸºç¡€Î»å€¼
    use_decoupled_gae: bool = True  # æ˜¯å¦ä½¿ç”¨è§£è€¦GAE
    
    # ğŸ”¥ Length-Adaptive GAEå‚æ•°
    use_length_adaptive_gae: bool = True  # æ˜¯å¦ä½¿ç”¨é•¿åº¦è‡ªé€‚åº”GAE
    length_threshold: int = 100  # é•¿åº¦é˜ˆå€¼
    lambda_policy_long: float = 0.99  # é•¿åºåˆ—ä½¿ç”¨æ›´å¤§çš„Î»
    
    # ğŸ”¥ Token-Level Loss
    use_token_level_loss: bool = True  # æ˜¯å¦ä½¿ç”¨tokençº§åˆ«æŸå¤±
    
    # ğŸ”¥ Group-Samplingå‚æ•°
    group_size: int = 4  # æ¯ä¸ªprompté‡‡æ ·çš„å›å¤æ•°é‡
    use_group_normalization: bool = True
    
    # ğŸ”¥ Value-Pretrainingå‚æ•°
    use_value_pretraining: bool = True  # æ˜¯å¦ä½¿ç”¨ä»·å€¼é¢„è®­ç»ƒ
    value_pretrain_steps: int = 100  # ä»·å€¼é¢„è®­ç»ƒæ­¥æ•°
    
    # ğŸ”¥ Positive Example LM Loss (Self-Imitation Learning)
    use_sil: bool = True  # æ˜¯å¦ä½¿ç”¨è‡ªæ¨¡ä»¿å­¦ä¹ 
    sil_coef: float = 0.1  # SILæŸå¤±ç³»æ•°
    sil_reward_threshold: float = 0.5  # æ­£æ ·æœ¬å¥–åŠ±é˜ˆå€¼
    
    # å…¶ä»–é…ç½®
    save_steps: int = 500
    eval_steps: int = 100
    output_dir: str = "./vapo_output"
    use_wandb: bool = True
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class VAPODataset(Dataset):
    """VAPOè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, prompts: List[str], tokenizer, max_length: int = 512):
        self.prompts = prompts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        prompt = self.prompts[idx]
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "prompt": prompt
        }

class VAPOTrainer:
    """VAPOè®­ç»ƒå™¨"""
    
    def __init__(self, config: VAPOConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizers()
        
        # ä»·å€¼é¢„è®­ç»ƒæ ‡å¿—
        self.value_pretrained = False
        
        # åˆå§‹åŒ–wandb
        if config.use_wandb:
            wandb.init(project="vapo-qwen", config=config.__dict__)
    
    def _init_models(self):
        """åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹ã€criticæ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ç­–ç•¥æ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        
        # Criticæ¨¡å‹ï¼ˆæ·»åŠ value headï¼‰
        self.critic_model = AutoModelForCausalLM.from_pretrained(
            self.config.critic_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        
        # ä¸ºcriticæ¨¡å‹æ·»åŠ value head
        hidden_size = self.critic_model.config.hidden_size
        self.value_head = nn.Linear(hidden_size, 1).to(self.device)
        
        # å¥–åŠ±æ¨¡å‹
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(
            self.config.reward_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.reward_tokenizer = AutoTokenizer.from_pretrained(self.config.reward_model_name)
        
        # ğŸ”¥ VAPO: ä½¿ç”¨å¥–åŠ±æ¨¡å‹åˆå§‹åŒ–criticæ¨¡å‹
        if self.config.use_value_pretraining:
            logger.info("ä½¿ç”¨å¥–åŠ±æ¨¡å‹åˆå§‹åŒ–criticæ¨¡å‹...")
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å¤åˆ¶å¥–åŠ±æ¨¡å‹çš„æƒé‡åˆ°critic
            # ç”±äºæ¶æ„å¯èƒ½ä¸åŒï¼Œè¿™é‡Œåªæ˜¯æ ‡è®°éœ€è¦é¢„è®­ç»ƒ
        
        # å‚è€ƒç­–ç•¥æ¨¡å‹
        self.ref_policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.ref_policy_model.eval()
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _init_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.policy_optimizer = torch.optim.AdamW(
            self.policy_model.parameters(),
            lr=self.config.learning_rate
        )
        
        critic_params = list(self.critic_model.parameters()) + list(self.value_head.parameters())
        self.critic_optimizer = torch.optim.AdamW(
            critic_params,
            lr=self.config.critic_learning_rate
        )
    
    def value_pretrain_step(self, prompts: List[str], responses: List[str], rewards: torch.Tensor):
        """
        ğŸ”¥ VAPO Value-Pretraining: é¢„è®­ç»ƒä»·å€¼æ¨¡å‹
        ä½¿ç”¨å¥–åŠ±ä¿¡å·è®­ç»ƒä»·å€¼æ¨¡å‹ï¼Œç¼“è§£åˆå§‹åŒ–åå·®
        """
        all_values = []
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}
            
            # è®¡ç®—ä»·å€¼
            critic_outputs = self.critic_model(**full_inputs, output_hidden_states=True)
            hidden_states = critic_outputs.hidden_states[-1]
            values = self.value_head(hidden_states)
            all_values.append(values[0, -1, 0])
        
        all_values = torch.stack(all_values)
        
        # ä»·å€¼æŸå¤±ï¼šä½¿ä»·å€¼ä¼°è®¡æ¥è¿‘å®é™…å¥–åŠ±
        value_loss = F.mse_loss(all_values, rewards)
        
        # æ›´æ–°critic
        self.critic_optimizer.zero_grad()
        value_loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(self.critic_model.parameters()) + list(self.value_head.parameters()), 1.0
        )
        self.critic_optimizer.step()
        
        return value_loss.item()
    
    def generate_responses_with_group_sampling(self, prompts: List[str]) -> Tuple[List[str], List[str], List[int]]:
        """
        ğŸ”¥ VAPO Group-Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        è¿”å›ï¼š(æ‰€æœ‰å›å¤, å¯¹åº”çš„prompt, å›å¤é•¿åº¦)
        """
        self.policy_model.eval()
        
        all_responses = []
        all_prompts_expanded = []
        all_response_lengths = []
        
        for prompt in prompts:
            for _ in range(self.config.group_size):
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,
                        temperature=0.7,
                        pad_token_id=self.tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                        output_scores=True
                    )
                
                generated_ids = outputs.sequences[0][inputs["input_ids"].shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                response_length = len(generated_ids)
                
                all_responses.append(response)
                all_prompts_expanded.append(prompt)
                all_response_lengths.append(response_length)
        
        return all_responses, all_prompts_expanded, all_response_lengths
    
    def compute_log_probs_and_values(self, prompts: List[str], responses: List[str],
                                    return_per_token: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—logæ¦‚ç‡å’Œä»·å€¼å‡½æ•°
        
        Args:
            return_per_token: ğŸ”¥ æ˜¯å¦è¿”å›tokençº§åˆ«çš„logæ¦‚ç‡ï¼ˆç”¨äºtoken-level lossï¼‰
        """
        all_log_probs = []
        all_token_log_probs = []
        all_values = []
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            with torch.no_grad():
                # è®¡ç®—logæ¦‚ç‡
                policy_outputs = self.policy_model(**full_inputs)
                logits = policy_outputs.logits
                
                log_probs = F.log_softmax(logits, dim=-1)
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)
                
                response_log_probs = token_log_probs[0, response_start-1:-1]
                
                if return_per_token:
                    all_token_log_probs.append(response_log_probs)
                
                all_log_probs.append(response_log_probs.sum())
                
                # è®¡ç®—ä»·å€¼å‡½æ•°
                critic_outputs = self.critic_model(**full_inputs, output_hidden_states=True)
                hidden_states = critic_outputs.hidden_states[-1]
                values = self.value_head(hidden_states)
                all_values.append(values[0, -1, 0])
        
        if return_per_token:
            return all_token_log_probs, torch.stack(all_values)
        else:
            return torch.stack(all_log_probs), torch.stack(all_values)
    
    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å¥–åŠ±"""
        rewards = []
        
        for prompt, response in zip(prompts, responses):
            full_text = f"{prompt} {response}"
            
            inputs = self.reward_tokenizer(
                full_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                reward_outputs = self.reward_model(**inputs)
                reward = reward_outputs.logits[0, 0]
                rewards.append(reward)
        
        return torch.stack(rewards)
    
    def compute_length_adaptive_lambda(self, response_length: int) -> float:
        """
        ğŸ”¥ VAPO Length-Adaptive GAE: æ ¹æ®åºåˆ—é•¿åº¦è‡ªé€‚åº”è°ƒæ•´Î»
        
        å¯¹äºçŸ­åºåˆ—ï¼šä½¿ç”¨è¾ƒå°çš„Î»ï¼ˆå¦‚0.95ï¼‰ï¼Œå‡å°‘æ–¹å·®
        å¯¹äºé•¿åºåˆ—ï¼šä½¿ç”¨è¾ƒå¤§çš„Î»ï¼ˆå¦‚0.99ï¼‰ï¼Œå‡å°‘åå·®
        """
        if not self.config.use_length_adaptive_gae:
            return self.config.lambda_policy_base
        
        if response_length > self.config.length_threshold:
            # é•¿åºåˆ—ä½¿ç”¨æ›´å¤§çš„Î»ï¼Œå‡å°‘åå·®
            return self.config.lambda_policy_long
        else:
            # çŸ­åºåˆ—ä½¿ç”¨åŸºç¡€Î»
            return self.config.lambda_policy_base
    
    def compute_gae_advantages(self, rewards: torch.Tensor, values: torch.Tensor,
                              response_lengths: List[int], for_critic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ğŸ”¥ VAPO Decoupled-GAE + Length-Adaptive GAE
        
        è®¡ç®—GAEä¼˜åŠ¿å‡½æ•°ï¼Œæ”¯æŒï¼š
        1. Decoupled-GAE: criticå’Œpolicyä½¿ç”¨ä¸åŒçš„Î»
        2. Length-Adaptive GAE: æ ¹æ®åºåˆ—é•¿åº¦è°ƒæ•´Î»
        
        Args:
            for_critic: æ˜¯å¦ä¸ºcriticè®¡ç®—ï¼ˆä½¿ç”¨Î»=1.0ï¼‰
        """
        batch_size = rewards.shape[0]
        advantages = torch.zeros_like(rewards)
        returns = torch.zeros_like(rewards)
        
        # ğŸ”¥ Decoupled-GAE: criticä½¿ç”¨Î»=1.0ï¼ˆæ— åä¼°è®¡ï¼‰
        if for_critic and self.config.use_decoupled_gae:
            lambda_gae = self.config.lambda_critic
        else:
            # ğŸ”¥ Length-Adaptive GAE: æ ¹æ®åºåˆ—é•¿åº¦è°ƒæ•´Î»
            lambda_gae = self.config.lambda_policy_base
        
        # ç®€åŒ–çš„GAEè®¡ç®—ï¼ˆå‡è®¾ç¨€ç–å¥–åŠ±ï¼Œåªåœ¨ç»ˆæ­¢æ—¶æœ‰å¥–åŠ±ï¼‰
        for i in range(batch_size):
            # ğŸ”¥ Length-Adaptive: ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—è‡ªé€‚åº”Î»
            if not for_critic and self.config.use_length_adaptive_gae:
                lambda_gae = self.compute_length_adaptive_lambda(response_lengths[i])
            
            # TD error: Î´ = r + Î³V(s') - V(s)
            # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾ç»ˆæ­¢çŠ¶æ€V(s')=0
            td_error = rewards[i] - values[i]
            
            # GAE: A = Î´ (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ˜¯å¤šæ­¥ç´¯ç§¯)
            advantages[i] = td_error
            returns[i] = rewards[i]
        
        return advantages, returns
    
    def compute_relative_rewards(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ğŸ”¥ VAPO Group-Sampling: è®¡ç®—ç»„å†…ç›¸å¯¹å¥–åŠ±
        ä¸GRPO/DAPOç›¸åŒçš„ç›¸å¯¹å¥–åŠ±è®¡ç®—
        """
        if group_size is None:
            group_size = self.config.group_size
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        rewards_grouped = rewards.view(-1, group_size)
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
        relative_rewards = rewards_grouped - group_baselines
        
        if self.config.use_group_normalization:
            group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
            relative_rewards = relative_rewards / group_std
        
        relative_rewards = relative_rewards.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_rewards, group_baselines
    
    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor],
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """
        ğŸ”¥ VAPO Token-Level Loss: å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—PPOæŸå¤±
        æ‰€æœ‰tokenæƒé‡ç›¸åŒï¼Œé¿å…é•¿åºåˆ—è´¡çŒ®ä¸è¶³
        """
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            advantage = advantages[i]
            
            # è®¡ç®—æ¯ä¸ªtokençš„æ¦‚ç‡æ¯”ç‡
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)
            
            # ğŸ”¥ VAPO Clip-Higher: éå¯¹ç§°è£å‰ª
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios,
                               1 - self.config.clip_range_low,
                               1 + self.config.clip_range_high) * advantage
            
            token_loss = -torch.min(surr1, surr2).sum()
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        return total_token_loss / total_tokens
    
    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                          advantages: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ğŸ”¥ VAPOç­–ç•¥æŸå¤±è®¡ç®—ï¼ˆæ”¯æŒToken-Level Losså’ŒClip-Higherï¼‰
        """
        # ğŸ”¥ Token-Level Loss
        if self.config.use_token_level_loss and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            # Sample-level loss with Clip-Higher
            ratio = torch.exp(log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio,
                               1 - self.config.clip_range_low,
                               1 + self.config.clip_range_high) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
        
        # ç†µæŸå¤±
        entropy = -log_probs.mean()
        entropy_loss = -self.config.entropy_coef * entropy
        
        return policy_loss, entropy_loss
    
    def compute_sil_loss(self, prompts: List[str], responses: List[str], rewards: torch.Tensor) -> torch.Tensor:
        """
        ğŸ”¥ VAPO Positive Example LM Loss (Self-Imitation Learning)
        å¯¹é«˜å¥–åŠ±æ ·æœ¬è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œæé«˜åˆ©ç”¨æ•ˆç‡
        """
        if not self.config.use_sil:
            return torch.tensor(0.0, device=self.device)
        
        # ç­›é€‰é«˜å¥–åŠ±æ ·æœ¬
        positive_mask = rewards > self.config.sil_reward_threshold
        if not positive_mask.any():
            return torch.tensor(0.0, device=self.device)
        
        positive_prompts = [p for i, p in enumerate(prompts) if positive_mask[i]]
        positive_responses = [r for i, r in enumerate(responses) if positive_mask[i]]
        
        sil_loss = 0.0
        for prompt, response in zip(positive_prompts, positive_responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            # è®¡ç®—è¯­è¨€æ¨¡å‹æŸå¤±
            outputs = self.policy_model(**full_inputs, labels=full_inputs["input_ids"])
            
            # åªè®¡ç®—responseéƒ¨åˆ†çš„æŸå¤±
            logits = outputs.logits[0, response_start-1:-1, :]
            labels = full_inputs["input_ids"][0, response_start:]
            
            loss = F.cross_entropy(logits, labels)
            sil_loss += loss
        
        if len(positive_prompts) > 0:
            sil_loss = sil_loss / len(positive_prompts)
        
        return sil_loss
    
    def compute_value_loss(self, values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä»·å€¼å‡½æ•°æŸå¤±"""
        return F.mse_loss(values, returns)
    
    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ­¥VAPOè®­ç»ƒ
        
        VAPOè®­ç»ƒæµç¨‹ï¼š
        1. Group-Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        2. è®¡ç®—å¥–åŠ±å’Œç›¸å¯¹å¥–åŠ±
        3. Decoupled-GAE + Length-Adaptive GAE: è®¡ç®—ä¼˜åŠ¿
        4. Token-Level Loss + Clip-Higher: æ›´æ–°ç­–ç•¥
        5. SIL: å¯¹é«˜å¥–åŠ±æ ·æœ¬è¿›è¡Œç›‘ç£å­¦ä¹ 
        """
        # ğŸ”¥ 1. Group-Sampling: ç”Ÿæˆå¤šä¸ªå›å¤
        responses, prompts_expanded, response_lengths = self.generate_responses_with_group_sampling(batch_prompts)
        
        # 2. è®¡ç®—å¥–åŠ±
        raw_rewards = self.compute_rewards(prompts_expanded, responses)
        
        # ğŸ”¥ Value-Pretraining: å¦‚æœæœªé¢„è®­ç»ƒï¼Œå…ˆé¢„è®­ç»ƒä»·å€¼æ¨¡å‹
        if self.config.use_value_pretraining and not self.value_pretrained:
            logger.info("æ‰§è¡Œä»·å€¼é¢„è®­ç»ƒ...")
            for _ in range(self.config.value_pretrain_steps):
                pretrain_loss = self.value_pretrain_step(prompts_expanded, responses, raw_rewards)
            self.value_pretrained = True
            logger.info(f"ä»·å€¼é¢„è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {pretrain_loss:.4f}")
        
        # 3. è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼ˆç»„å†…å½’ä¸€åŒ–ï¼‰
        relative_rewards, group_baselines = self.compute_relative_rewards(raw_rewards)
        
        # æˆªæ–­æ•°æ®
        prompts_truncated = prompts_expanded[:len(relative_rewards)]
        responses_truncated = responses[:len(relative_rewards)]
        response_lengths_truncated = response_lengths[:len(relative_rewards)]
        
        # 4. è®¡ç®—logæ¦‚ç‡å’Œä»·å€¼
        log_probs, values = self.compute_log_probs_and_values(
            prompts_truncated, responses_truncated, return_per_token=False
        )
        
        # ğŸ”¥ å¦‚æœä½¿ç”¨token-level lossï¼Œè·å–tokençº§åˆ«çš„logæ¦‚ç‡
        old_token_log_probs_list = None
        if self.config.use_token_level_loss:
            old_token_log_probs_list, _ = self.compute_log_probs_and_values(
                prompts_truncated, responses_truncated, return_per_token=True
            )
            old_token_log_probs_list = [t.detach() for t in old_token_log_probs_list]
        
        # ğŸ”¥ 5. Decoupled-GAE + Length-Adaptive GAE: è®¡ç®—ä¼˜åŠ¿
        # ä¸ºpolicyè®¡ç®—ä¼˜åŠ¿ï¼ˆä½¿ç”¨è‡ªé€‚åº”Î»ï¼‰
        advantages_policy, returns_policy = self.compute_gae_advantages(
            relative_rewards, values, response_lengths_truncated, for_critic=False
        )
        
        # ä¸ºcriticè®¡ç®—ä¼˜åŠ¿ï¼ˆä½¿ç”¨Î»=1.0ï¼‰
        advantages_critic, returns_critic = self.compute_gae_advantages(
            relative_rewards, values, response_lengths_truncated, for_critic=True
        )
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages_policy = (advantages_policy - advantages_policy.mean()) / (advantages_policy.std() + 1e-8)
        
        # ä¿å­˜æ—§çš„logæ¦‚ç‡
        old_log_probs = log_probs.detach()
        
        # VAPOæ›´æ–°å¾ªç¯
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy_loss = 0
        total_sil_loss = 0
        
        for vapo_step in range(self.config.vapo_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡å’Œå€¼
            new_log_probs, new_values = self.compute_log_probs_and_values(
                prompts_truncated, responses_truncated, return_per_token=False
            )
            
            # ğŸ”¥ å¦‚æœä½¿ç”¨token-level lossï¼Œè·å–å½“å‰ç­–ç•¥çš„tokençº§åˆ«logæ¦‚ç‡
            new_token_log_probs_list = None
            if self.config.use_token_level_loss:
                new_token_log_probs_list, _ = self.compute_log_probs_and_values(
                    prompts_truncated, responses_truncated, return_per_token=True
                )
            
            # ğŸ”¥ 6. è®¡ç®—æŸå¤±ï¼ˆToken-Level Loss + Clip-Higherï¼‰
            policy_loss, entropy_loss = self.compute_policy_loss(
                new_log_probs, old_log_probs, advantages_policy,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            # ğŸ”¥ ä½¿ç”¨criticçš„ä¼˜åŠ¿è®¡ç®—ä»·å€¼æŸå¤±
            value_loss = self.compute_value_loss(new_values, returns_critic)
            
            # ğŸ”¥ 7. SILæŸå¤±
            sil_loss = self.compute_sil_loss(prompts_truncated, responses_truncated, raw_rewards[:len(relative_rewards)])
            
            # æ€»æŸå¤±
            total_loss = policy_loss + self.config.vf_coef * value_loss + entropy_loss + self.config.sil_coef * sil_loss
            
            # æ›´æ–°ç­–ç•¥æ¨¡å‹
            self.policy_optimizer.zero_grad()
            total_loss.backward(retain_graph=True)
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.policy_optimizer.step()
            
            # æ›´æ–°Criticæ¨¡å‹
            self.critic_optimizer.zero_grad()
            value_loss.backward()
            torch.nn.utils.clip_grad_norm_(
                list(self.critic_model.parameters()) + list(self.value_head.parameters()), 1.0
            )
            self.critic_optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_sil_loss += sil_loss.item()
        
        return {
            "policy_loss": total_policy_loss / self.config.vapo_epochs,
            "value_loss": total_value_loss / self.config.vapo_epochs,
            "entropy_loss": total_entropy_loss / self.config.vapo_epochs,
            "sil_loss": total_sil_loss / self.config.vapo_epochs,
            "raw_reward_mean": raw_rewards.mean().item(),
            "raw_reward_std": raw_rewards.std().item(),
            "relative_reward_mean": relative_rewards.mean().item(),
            "advantage_mean": advantages_policy.mean().item(),
            "value_mean": values.mean().item(),
            "avg_response_length": np.mean(response_lengths_truncated)
        }
    
    def train(self, train_dataset: VAPODataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹VAPOè®­ç»ƒ...")
        
        dataloader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )
        
        global_step = 0
        
        for epoch in range(self.config.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs}")
            
            epoch_metrics = []
            
            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch + 1}")):
                batch_prompts = batch["prompt"]
                
                metrics = self.train_step(batch_prompts)
                epoch_metrics.append(metrics)
                
                if self.config.use_wandb:
                    wandb.log({
                        "step": global_step,
                        "epoch": epoch,
                        **metrics
                    })
                
                if global_step % self.config.save_steps == 0:
                    self.save_checkpoint(global_step)
                
                global_step += 1
                
                if batch_idx % 10 == 0:
                    logger.info(f"Step {global_step}: {metrics}")
            
            # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
            avg_metrics = {}
            for key in epoch_metrics[0].keys():
                avg_metrics[f"epoch_{key}"] = np.mean([m[key] for m in epoch_metrics])
            
            logger.info(f"Epoch {epoch + 1} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
            
            if self.config.use_wandb:
                wandb.log(avg_metrics)
        
        logger.info("VAPOè®­ç»ƒå®Œæˆ!")
        self.save_checkpoint("final")
    
    def save_checkpoint(self, step):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜ç­–ç•¥æ¨¡å‹
        self.policy_model.save_pretrained(os.path.join(checkpoint_dir, "policy"))
        
        # ä¿å­˜criticæ¨¡å‹å’Œvalue head
        self.critic_model.save_pretrained(os.path.join(checkpoint_dir, "critic"))
        torch.save(self.value_head.state_dict(), os.path.join(checkpoint_dir, "value_head.pt"))
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {checkpoint_dir}")

def load_training_data() -> List[str]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    try:
        dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1000]")
        
        prompts = []
        for item in dataset:
            conversation = item["chosen"]
            if conversation.startswith("Human:"):
                human_part = conversation.split("Assistant:")[0].replace("Human:", "").strip()
                if human_part:
                    prompts.append(human_part)
        
        logger.info(f"åŠ è½½äº† {len(prompts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return prompts
    
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½HHæ•°æ®é›†: {e}")
        logger.info("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        return [
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚",
            "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "è¯·è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚",
            "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"
        ] * 50

def main():
    """ä¸»å‡½æ•°"""
    config = VAPOConfig()
    
    os.makedirs(config.output_dir, exist_ok=True)
    
    prompts = load_training_data()
    
    tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset = VAPODataset(prompts, tokenizer, config.max_length)
    
    trainer = VAPOTrainer(config)
    
    trainer.train(train_dataset)

if __name__ == "__main__":
    main()
