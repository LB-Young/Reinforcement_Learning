# GRPO (Group Relative Policy Optimization) è®­ç»ƒåˆ†æ

## æ¦‚è¿°

GRPO (Group Relative Policy Optimization) æ˜¯PPOçš„ä¸€ä¸ªé‡è¦å˜ç§ï¼Œ**æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸éœ€è¦criticæ¨¡å‹**ï¼Œè€Œæ˜¯ä½¿ç”¨**ç»„å†…å¥–åŠ±å‡å€¼ä½œä¸ºåŸºçº¿**æ¥è®¡ç®—ä¼˜åŠ¿å‡½æ•°ã€‚è¿™å¤§å¤§ç®€åŒ–äº†è®­ç»ƒæµç¨‹å¹¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚

## æ ¸å¿ƒæ€æƒ³

GRPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ä½¿ç”¨ç»„å†…å¥–åŠ±çš„å¹³å‡å€¼ä½œä¸ºåŸºçº¿ï¼ˆbaselineï¼‰ï¼Œç›¸å¯¹å¥–åŠ±ï¼ˆreward - group_meanï¼‰ç›´æ¥ä½œä¸ºä¼˜åŠ¿å‡½æ•°ï¼Œæ— éœ€è®­ç»ƒé¢å¤–çš„criticæ¨¡å‹**ã€‚

### åŠ¨æœº
1. **ç®€åŒ–è®­ç»ƒ**: ä¸éœ€è¦è®­ç»ƒå’Œç»´æŠ¤criticæ¨¡å‹ï¼Œå‡å°‘è®¡ç®—å¼€é”€
2. **å‡å°‘å¥–åŠ±åå·®**: é€šè¿‡ç»„å†…æ¯”è¾ƒæ¶ˆé™¤å¥–åŠ±æ¨¡å‹çš„ç³»ç»Ÿæ€§åå·®
3. **è®­ç»ƒç¨³å®š**: ç»„å†…å‡å€¼ä½œä¸ºåŠ¨æ€åŸºçº¿ï¼Œæ¯”å›ºå®šåŸºçº¿æ›´ç¨³å®š
4. **æ ·æœ¬æ•ˆç‡**: é€šè¿‡ç»„å†…æ¯”è¾ƒå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ ·æœ¬ä¿¡æ¯

## å…³é”®å·®å¼‚åˆ†æ

### 1. æ¨¡å‹æ¶æ„å·®å¼‚ - **ğŸ”¥ æœ€é‡è¦çš„åŒºåˆ«**

#### PPOæ¶æ„
```python
class PPOTrainer:
    def _init_models(self):
        self.policy_model = ...        # ç­–ç•¥æ¨¡å‹
        self.critic_model = ...        # ğŸ”´ éœ€è¦criticæ¨¡å‹
        self.value_head = ...          # ğŸ”´ éœ€è¦value head
        self.reward_model = ...        # å¥–åŠ±æ¨¡å‹
        self.ref_policy_model = ...    # å‚è€ƒç­–ç•¥æ¨¡å‹
```

#### GRPOæ¶æ„ - **ğŸ”¥ æ— éœ€critic**
```python
class GRPOTrainer:
    def _init_models(self):
        self.policy_model = ...        # ç­–ç•¥æ¨¡å‹
        # ğŸ”¥ ä¸éœ€è¦criticæ¨¡å‹ï¼
        # ğŸ”¥ ä¸éœ€è¦value headï¼
        self.reward_model = ...        # å¥–åŠ±æ¨¡å‹
        self.ref_policy_model = ...    # å‚è€ƒç­–ç•¥æ¨¡å‹
```

**å…³é”®å·®å¼‚è¯´æ˜**:
- PPOéœ€è¦è®­ç»ƒcriticæ¨¡å‹æ¥ä¼°è®¡çŠ¶æ€ä»·å€¼V(s)
- GRPOä½¿ç”¨ç»„å†…å¥–åŠ±å‡å€¼ä½œä¸ºåŸºçº¿ï¼Œå®Œå…¨ä¸éœ€è¦critic
- è¿™å‡å°‘äº†çº¦50%çš„æ¨¡å‹å‚æ•°å’Œè®­ç»ƒå¼€é”€

### 2. é…ç½®å‚æ•°å·®å¼‚

#### PPOé…ç½®
```python
class PPOConfig:
    critic_model_name: str = "Qwen/Qwen2-0.5B"  # ğŸ”´ éœ€è¦criticæ¨¡å‹
    critic_learning_rate: float = 5e-6          # ğŸ”´ éœ€è¦criticå­¦ä¹ ç‡
    vf_coef: float = 0.1                        # ğŸ”´ éœ€è¦value functionç³»æ•°
    gamma: float = 0.99                         # ğŸ”´ éœ€è¦æŠ˜æ‰£å› å­
    lam: float = 0.95                           # ğŸ”´ éœ€è¦GAE lambda
```

#### GRPOé…ç½® - **ğŸ”¥ å¤§å¹…ç®€åŒ–**
```python
class GRPOConfig:
    # ğŸ”¥ ä¸éœ€è¦criticç›¸å…³å‚æ•°ï¼
    # ğŸ”¥ ä¸éœ€è¦vf_coefï¼
    # ğŸ”¥ ä¸éœ€è¦gammaå’Œlamï¼
    
    # GRPOç‰¹æœ‰å‚æ•°
    group_size: int = 4                    # ğŸ”¥ æ¯ç»„çš„æ ·æœ¬æ•°é‡
    use_group_normalization: bool = True   # ğŸ”¥ æ˜¯å¦ä½¿ç”¨ç»„å†…æ ‡å‡†åŒ–
```

**å…³é”®å·®å¼‚è¯´æ˜**:
- GRPOç§»é™¤äº†æ‰€æœ‰ä¸criticç›¸å…³çš„å‚æ•°
- ä¸éœ€è¦GAEç›¸å…³çš„gammaå’Œlambdaå‚æ•°
- é…ç½®æ›´ç®€å•ï¼Œè¶…å‚æ•°æ›´å°‘

### 3. ä¼˜åŠ¿å‡½æ•°è®¡ç®— - **ğŸ”¥ æ ¸å¿ƒå·®å¼‚**

#### PPOä¼˜åŠ¿å‡½æ•°
```python
# PPO: éœ€è¦criticæ¨¡å‹ä¼°è®¡value
def compute_advantages(self, rewards, values):
    # valuesæ¥è‡ªcriticæ¨¡å‹çš„ä¼°è®¡
    advantages = rewards - values  # A = R - V(s)
    returns = rewards
    
    # æ ‡å‡†åŒ–
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    return advantages, returns
```

#### GRPOä¼˜åŠ¿å‡½æ•° - **ğŸ”¥ ä½¿ç”¨ç»„å†…å‡å€¼**
```python
# GRPO: ä½¿ç”¨ç»„å†…å¥–åŠ±å‡å€¼ä½œä¸ºåŸºçº¿
def compute_relative_rewards(self, rewards, group_size):
    # å°†å¥–åŠ±åˆ†ç»„ [num_groups, group_size]
    rewards_grouped = rewards.view(-1, group_size)
    
    # ğŸ”¥ ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿ï¼ˆæ›¿ä»£criticçš„valueï¼‰
    group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
    
    # ğŸ”¥ ç›¸å¯¹å¥–åŠ± = å¥–åŠ± - ç»„å†…å‡å€¼ï¼Œè¿™å°±æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼
    # A = R - mean(R_group)
    relative_rewards = rewards_grouped - group_baselines
    
    return relative_rewards, group_baselines

def compute_advantages(self, advantages):
    # ä¼˜åŠ¿å·²ç»åœ¨compute_relative_rewardsä¸­è®¡ç®—å®Œæˆ
    # è¿™é‡Œåªéœ€è¦æ ‡å‡†åŒ–
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    return advantages
```

**æ•°å­¦å¯¹æ¯”**:
- **PPO**: `Advantage = R - V(s)`ï¼Œå…¶ä¸­V(s)ç”±criticæ¨¡å‹å­¦ä¹ 
- **GRPO**: `Advantage = R - mean(R_group)`ï¼Œå…¶ä¸­mean(R_group)æ˜¯ç»„å†…å‡å€¼

**GRPOçš„ä¼˜åŠ¿**:
1. ä¸éœ€è¦è®­ç»ƒcriticæ¨¡å‹
2. ç»„å†…å‡å€¼æ˜¯çœŸå®å¥–åŠ±çš„ç»Ÿè®¡é‡ï¼Œæ¯”å­¦ä¹ çš„valueæ›´å¯é 
3. åŠ¨æ€åŸºçº¿ï¼Œè‡ªåŠ¨é€‚åº”å¥–åŠ±åˆ†å¸ƒçš„å˜åŒ–

#### PPOå¥–åŠ±è®¡ç®—
```python
# PPO: ç›´æ¥ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¾“å‡º
def train_step(self, batch_prompts):
    responses, log_probs, values = self.generate_responses(batch_prompts)
    rewards = self.compute_rewards(batch_prompts, responses)  # ç›´æ¥ä½¿ç”¨
    advantages, returns = self.compute_advantages(rewards, values)
    # ... åç»­å¤„ç†
```

#### GRPOå¥–åŠ±è®¡ç®— - **ğŸ”¥ æ ¸å¿ƒåˆ›æ–°**
```python
# GRPO: å¼•å…¥ç›¸å¯¹å¥–åŠ±æœºåˆ¶
def train_step(self, batch_prompts):
    responses, log_probs, values = self.generate_responses(batch_prompts)
    
    # 1. è®¡ç®—åŸå§‹å¥–åŠ±
    raw_rewards = self.compute_rewards(batch_prompts, responses)
    
    # 2. ğŸ”¥ GRPOæ ¸å¿ƒï¼šè®¡ç®—ç›¸å¯¹å¥–åŠ±
    relative_rewards = self.compute_relative_rewards(raw_rewards)
    
    # 3. ä½¿ç”¨ç›¸å¯¹å¥–åŠ±è®¡ç®—ä¼˜åŠ¿
    advantages, returns = self.compute_advantages(relative_rewards, values)
    # ... åç»­å¤„ç†
```

### 3. ç›¸å¯¹å¥–åŠ±è®¡ç®—è¯¦è§£ - **GRPOç‹¬æœ‰**

```python
def compute_relative_rewards(self, rewards: torch.Tensor, group_size: int = None) -> torch.Tensor:
    """è®¡ç®—GRPOçš„ç›¸å¯¹å¥–åŠ± - GRPOçš„æ ¸å¿ƒåˆ›æ–°"""
    if group_size is None:
        group_size = self.config.group_size
    
    batch_size = rewards.shape[0]
    
    # ğŸ”¥ æ­¥éª¤1: å¤„ç†æ‰¹æ¬¡å¤§å°ä¸æ•´é™¤çš„æƒ…å†µ
    if batch_size % group_size != 0:
        num_complete_groups = batch_size // group_size
        rewards = rewards[:num_complete_groups * group_size]
        batch_size = rewards.shape[0]
    
    # ğŸ”¥ æ­¥éª¤2: å°†å¥–åŠ±é‡å¡‘ä¸ºç»„çš„å½¢çŠ¶ [num_groups, group_size]
    rewards_grouped = rewards.view(-1, group_size)
    
    # ğŸ”¥ æ­¥éª¤3: è®¡ç®—æ¯ç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿
    group_baselines = rewards_grouped.mean(dim=1, keepdim=True)  # [num_groups, 1]
    
    # ğŸ”¥ æ­¥éª¤4: è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼šæ¯ä¸ªæ ·æœ¬çš„å¥–åŠ±å‡å»ç»„å†…å¹³å‡å€¼
    relative_rewards = rewards_grouped - group_baselines  # [num_groups, group_size]
    
    # ğŸ”¥ æ­¥éª¤5: å¯é€‰çš„ç»„å†…æ ‡å‡†åŒ–
    if self.config.use_group_normalization:
        group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
        relative_rewards = relative_rewards / group_std
    
    # ğŸ”¥ æ­¥éª¤6: é‡æ–°å±•å¹³ä¸ºåŸå§‹å½¢çŠ¶
    relative_rewards = relative_rewards.view(-1)
    
    # ğŸ”¥ æ­¥éª¤7: ç»„åˆç›¸å¯¹å¥–åŠ±å’ŒåŸºçº¿å¥–åŠ±
    baseline_rewards = group_baselines.repeat(1, group_size).view(-1)
    combined_rewards = (self.config.relative_reward_weight * relative_rewards + 
                      self.config.baseline_reward_weight * baseline_rewards)
    
    return combined_rewards
```

**ç®—æ³•æ­¥éª¤è¯¦è§£**:

1. **åˆ†ç»„**: å°†æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æŒ‰`group_size`åˆ†ç»„
2. **åŸºçº¿è®¡ç®—**: æ¯ç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºè¯¥ç»„çš„åŸºçº¿
3. **ç›¸å¯¹å¥–åŠ±**: `ç›¸å¯¹å¥–åŠ± = ä¸ªä½“å¥–åŠ± - ç»„å†…å¹³å‡å¥–åŠ±`
4. **æ ‡å‡†åŒ–**: å¯é€‰çš„ç»„å†…æ ‡å‡†åŒ–ï¼Œå‡å°‘æ–¹å·®å½±å“
5. **ç»„åˆ**: å°†ç›¸å¯¹å¥–åŠ±å’ŒåŸºçº¿å¥–åŠ±æŒ‰æƒé‡ç»„åˆ

### 4. è®­ç»ƒå¾ªç¯å·®å¼‚ - **ğŸ”¥ æ— value loss**

#### PPOè®­ç»ƒå¾ªç¯
```python
def train_step(self, batch_prompts):
    # ç”Ÿæˆå›å¤å¹¶è®¡ç®—values
    responses, log_probs, values = self.generate_responses(batch_prompts)
    rewards = self.compute_rewards(batch_prompts, responses)
    
    # è®¡ç®—ä¼˜åŠ¿ï¼ˆä½¿ç”¨criticçš„valuesï¼‰
    advantages, returns = self.compute_advantages(rewards, values)
    
    for ppo_step in range(self.config.ppo_epochs):
        new_log_probs, new_values = self.compute_log_probs_and_values(...)
        
        # ğŸ”´ è®¡ç®—policy losså’Œvalue loss
        policy_loss = ...
        value_loss = F.mse_loss(new_values, returns)  # éœ€è¦è®­ç»ƒcritic
        
        # ğŸ”´ æ€»æŸå¤±åŒ…å«value loss
        total_loss = policy_loss + vf_coef * value_loss + entropy_loss + kl_loss
        
        # ğŸ”´ éœ€è¦æ›´æ–°ä¸¤ä¸ªæ¨¡å‹
        self.policy_optimizer.step()
        self.critic_optimizer.step()
```

#### GRPOè®­ç»ƒå¾ªç¯ - **ğŸ”¥ ç®€åŒ–**
```python
def train_step(self, batch_prompts):
    # ç”Ÿæˆå›å¤ï¼ˆä¸éœ€è¦è®¡ç®—valuesï¼‰
    responses, log_probs = self.generate_responses(batch_prompts)
    raw_rewards = self.compute_rewards(batch_prompts, responses)
    
    # ğŸ”¥ è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼ˆä¼˜åŠ¿ï¼‰
    relative_rewards, group_baselines = self.compute_relative_rewards(raw_rewards)
    advantages = self.compute_advantages(relative_rewards)
    
    for grpo_step in range(self.config.grpo_epochs):
        new_log_probs = self.compute_log_probs(...)
        
        # ğŸ”¥ åªè®¡ç®—policy lossï¼ˆæ— value lossï¼‰
        policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(...)
        
        # ğŸ”¥ æ€»æŸå¤±ä¸åŒ…å«value loss
        total_loss = policy_loss + entropy_loss + kl_loss
        
        # ğŸ”¥ åªéœ€è¦æ›´æ–°ç­–ç•¥æ¨¡å‹
        self.policy_optimizer.step()
```

**å…³é”®å·®å¼‚**:
1. GRPOä¸éœ€è¦è®¡ç®—å’Œæ›´æ–°value function
2. è®­ç»ƒå¾ªç¯æ›´ç®€å•ï¼Œåªæœ‰ä¸€ä¸ªä¼˜åŒ–å™¨
3. æ€»æŸå¤±ä¸åŒ…å«value lossé¡¹
4. è®¡ç®—æ•ˆç‡æ›´é«˜

### 5. è®­ç»ƒæŒ‡æ ‡å·®å¼‚

#### PPOè®­ç»ƒæŒ‡æ ‡
```python
return {
    "policy_loss": ...,
    "value_loss": ...,              # ğŸ”´ æœ‰value loss
    "reward_mean": ...,             # åªæœ‰åŸå§‹å¥–åŠ±
    "advantage_mean": ...,
}
```

#### GRPOè®­ç»ƒæŒ‡æ ‡ - **ğŸ”¥ å¢å¼ºçš„ç›‘æ§**
```python
return {
    "policy_loss": ...,
    # ğŸ”¥ æ— value loss
    
    # ğŸ”¥ åŸå§‹å¥–åŠ±ç»Ÿè®¡
    "raw_reward_mean": ...,
    "raw_reward_std": ...,
    
    # ğŸ”¥ ç›¸å¯¹å¥–åŠ±ç»Ÿè®¡
    "relative_reward_mean": ...,    # åº”æ¥è¿‘0
    "relative_reward_std": ...,
    
    # ğŸ”¥ ç»„å†…åŸºçº¿ç»Ÿè®¡
    "group_baseline_mean": ...,     # ç»„å†…å‡å€¼åŸºçº¿
    
    "advantage_mean": ...,          # æ ‡å‡†åŒ–ååº”æ¥è¿‘0
}
```

**ç›‘æ§æ„ä¹‰**:
- `raw_reward_*`: ç›‘æ§å¥–åŠ±æ¨¡å‹çš„åŸå§‹è¾“å‡ºåˆ†å¸ƒ
- `relative_reward_*`: ç›‘æ§ç›¸å¯¹å¥–åŠ±çš„åˆ†å¸ƒï¼Œç†è®ºä¸Šå‡å€¼åº”æ¥è¿‘0
- `group_baseline_mean`: ç›‘æ§ç»„å†…åŸºçº¿çš„å˜åŒ–è¶‹åŠ¿

#### PPOæ•°æ®å¤„ç†
```python
def train_step(self, batch_prompts):
    # æ‰€æœ‰æ ·æœ¬éƒ½å‚ä¸è®­ç»ƒï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
    responses, log_probs, values = self.generate_responses(batch_prompts)
    # ... ç›´æ¥ä½¿ç”¨æ‰€æœ‰æ•°æ®
```

#### GRPOæ•°æ®å¤„ç† - **éœ€è¦å¯¹é½**
```python
def train_step(self, batch_prompts):
    responses, log_probs, values = self.generate_responses(batch_prompts)
    raw_rewards = self.compute_rewards(batch_prompts, responses)
    
    # ğŸ”¥ ç›¸å¯¹å¥–åŠ±å¯èƒ½æ”¹å˜æ•°æ®é•¿åº¦ï¼ˆæˆªæ–­ä¸å®Œæ•´çš„ç»„ï¼‰
    relative_rewards = self.compute_relative_rewards(raw_rewards)
    
    # ğŸ”¥ éœ€è¦æˆªæ–­å…¶ä»–æ•°æ®ä»¥åŒ¹é…ç›¸å¯¹å¥–åŠ±çš„é•¿åº¦
    advantages, returns = self.compute_advantages(relative_rewards, values[:len(relative_rewards)])
    old_log_probs = log_probs[:len(relative_rewards)].detach()
    
    # ğŸ”¥ æˆªæ–­promptså’Œresponses
    batch_prompts_truncated = batch_prompts[:len(relative_rewards)]
    responses_truncated = responses[:len(relative_rewards)]
```

## ç®—æ³•ä¼˜åŠ¿åˆ†æ

### 1. ç®€åŒ–è®­ç»ƒæµç¨‹
**é—®é¢˜**: PPOéœ€è¦è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼ˆpolicyå’Œcriticï¼‰ï¼Œå¢åŠ äº†å¤æ‚åº¦
**GRPOè§£å†³æ–¹æ¡ˆ**: åªéœ€è¦è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œcriticè¢«ç»„å†…å‡å€¼åŸºçº¿æ›¿ä»£

### 2. å‡å°‘è®¡ç®—å¼€é”€
**é—®é¢˜**: Criticæ¨¡å‹éœ€è¦é¢å¤–çš„å‰å‘å’Œåå‘ä¼ æ’­
**GRPOè§£å†³æ–¹æ¡ˆ**: 
- å‡å°‘çº¦50%çš„æ¨¡å‹å‚æ•°
- å‡å°‘çº¦30-40%çš„è®­ç»ƒæ—¶é—´
- é™ä½GPUå†…å­˜å ç”¨

### 3. å‡å°‘å¥–åŠ±åå·®
**é—®é¢˜**: å¥–åŠ±æ¨¡å‹å¯èƒ½å¯¹æŸäº›ç±»å‹çš„å›å¤æœ‰ç³»ç»Ÿæ€§åå¥½
**GRPOè§£å†³æ–¹æ¡ˆ**: é€šè¿‡ç»„å†…æ¯”è¾ƒï¼Œæ¶ˆé™¤äº†ç»å¯¹å¥–åŠ±å€¼çš„å½±å“ï¼Œåªå…³æ³¨ç›¸å¯¹è´¨é‡

### 4. æé«˜è®­ç»ƒç¨³å®šæ€§
**é—®é¢˜**: Criticæ¨¡å‹çš„å­¦ä¹ å¯èƒ½ä¸ç¨³å®šï¼Œå½±å“ç­–ç•¥æ›´æ–°
**GRPOè§£å†³æ–¹æ¡ˆ**: ç»„å†…å‡å€¼æ˜¯çœŸå®å¥–åŠ±çš„ç»Ÿè®¡é‡ï¼Œæ¯”å­¦ä¹ çš„valueæ›´å¯é å’Œç¨³å®š

### 5. åŠ¨æ€è‡ªé€‚åº”åŸºçº¿
**é—®é¢˜**: å›ºå®šçš„å¥–åŠ±åŸºçº¿å¯èƒ½ä¸é€‚åº”åŠ¨æ€å˜åŒ–çš„å¥–åŠ±åˆ†å¸ƒ
**GRPOè§£å†³æ–¹æ¡ˆ**: æ¯ç»„çš„åŸºçº¿éƒ½æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”å¥–åŠ±åˆ†å¸ƒçš„å˜åŒ–

### 6. æ›´å¥½çš„æ ·æœ¬æ•ˆç‡
**é—®é¢˜**: PPOéœ€è¦å¤§é‡æ ·æœ¬æ‰èƒ½å­¦åˆ°æœ‰æ•ˆçš„ç­–ç•¥
**GRPOè§£å†³æ–¹æ¡ˆ**: ç»„å†…æ¯”è¾ƒæä¾›äº†æ›´ä¸°å¯Œçš„å­¦ä¹ ä¿¡å·ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½èƒ½ä»ç»„å†…å…¶ä»–æ ·æœ¬ä¸­å­¦ä¹ 

## æ ¸å¿ƒå…¬å¼å¯¹æ¯”

### PPO
```
ä¼˜åŠ¿å‡½æ•°: A(s,a) = R(s,a) - V(s)
å…¶ä¸­ V(s) ç”±criticæ¨¡å‹å­¦ä¹ 

æŸå¤±å‡½æ•°:
L = L_policy + c_vf * L_value + c_entropy * L_entropy + c_kl * L_kl
```

### GRPO
```
ä¼˜åŠ¿å‡½æ•°: A(s,a) = R(s,a) - mean(R_group)
å…¶ä¸­ mean(R_group) æ˜¯ç»„å†…å¥–åŠ±å‡å€¼

æŸå¤±å‡½æ•°:
L = L_policy + c_entropy * L_entropy + c_kl * L_kl
(æ—  L_value é¡¹)
```

## å®é™…åº”ç”¨è€ƒè™‘

### 1. ç»„å¤§å°é€‰æ‹©
- **å°ç»„ (2-4)**: æ›´ç²¾ç»†çš„æ¯”è¾ƒï¼Œä½†å¯èƒ½å¢åŠ å™ªå£°
- **å¤§ç»„ (8-16)**: æ›´ç¨³å®šçš„åŸºçº¿ï¼Œä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚ä¿¡æ¯
- **æ¨è**: ä»4å¼€å§‹ï¼Œæ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´

### 2. æ‰¹æ¬¡å¤§å°è¦æ±‚
GRPOè¦æ±‚æ‰¹æ¬¡å¤§å°æœ€å¥½æ˜¯`group_size`çš„å€æ•°ï¼Œå¦åˆ™ä¼šæˆªæ–­æ•°æ®ï¼š
```python
# æ¨èçš„æ‰¹æ¬¡å¤§å°è®¾ç½®
batch_size = 8   # group_size = 4 çš„å€æ•°
group_size = 4
```

### 3. ä½•æ—¶ä½¿ç”¨GRPO vs PPO

**ä½¿ç”¨GRPOçš„åœºæ™¯**:
- è®¡ç®—èµ„æºæœ‰é™ï¼Œå¸Œæœ›å‡å°‘è®­ç»ƒå¼€é”€
- å¥–åŠ±æ¨¡å‹å­˜åœ¨ç³»ç»Ÿæ€§åå·®
- å¸Œæœ›ç®€åŒ–è®­ç»ƒæµç¨‹
- æ ·æœ¬æ•°é‡æœ‰é™ï¼Œéœ€è¦æ›´å¥½çš„æ ·æœ¬æ•ˆç‡

**ä½¿ç”¨PPOçš„åœºæ™¯**:
- éœ€è¦ç²¾ç¡®çš„valueä¼°è®¡ç”¨äºå…¶ä»–ç›®çš„
- æœ‰å……è¶³çš„è®¡ç®—èµ„æº
- å¥–åŠ±ä¿¡å·éå¸¸ç¨€ç–æˆ–å»¶è¿Ÿ
- éœ€è¦æ›´å¤æ‚çš„ä¼˜åŠ¿ä¼°è®¡ï¼ˆå¦‚GAEï¼‰

## æ€»ç»“

GRPOé€šè¿‡**ç”¨ç»„å†…å¥–åŠ±å‡å€¼æ›¿ä»£criticæ¨¡å‹**ï¼Œåœ¨ä¿æŒPPOæ ¸å¿ƒç®—æ³•ä¸å˜çš„åŸºç¡€ä¸Šï¼Œæ˜¾è‘—ç®€åŒ–äº†è®­ç»ƒæµç¨‹ã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š

1. **ğŸ”¥ æ— éœ€criticæ¨¡å‹**: æœ€å¤§çš„åˆ›æ–°ï¼Œå‡å°‘50%æ¨¡å‹å‚æ•°
2. **ğŸ”¥ ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿**: `Advantage = R - mean(R_group)`
3. **ğŸ”¥ ç®€åŒ–çš„æŸå¤±å‡½æ•°**: æ— value lossé¡¹
4. **ğŸ”¥ å•ä¸€ä¼˜åŒ–å™¨**: åªéœ€è¦æ›´æ–°ç­–ç•¥æ¨¡å‹
5. **ğŸ”¥ åŠ¨æ€åŸºçº¿**: è‡ªåŠ¨é€‚åº”å¥–åŠ±åˆ†å¸ƒå˜åŒ–

è¿™äº›æ”¹è¿›ä½¿å¾—GRPOåœ¨ä¿æŒPPOä¼˜ç‚¹çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½äº†è®­ç»ƒå¤æ‚åº¦å’Œè®¡ç®—å¼€é”€ï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™æˆ–éœ€è¦å¿«é€Ÿè¿­ä»£çš„åœºæ™¯ã€‚