#!/usr/bin/env python3
"""
GSPO (Group Sequence Policy Optimization) è®­ç»ƒè„šæœ¬ - åŸºäºQwen2-0.5B
GSPOæ˜¯ä¸€ç§ç»“åˆäº†ç»„é‡‡æ ·å’Œåºåˆ—çº§ä¼˜åŒ–çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š
1. Group Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒ
2. Sequence-Level Rewards: åºåˆ—çº§åˆ«çš„å¥–åŠ±è®¡ç®—
3. Relative Advantage: ä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
4. æ— éœ€Criticæ¨¡å‹: ç±»ä¼¼GRPOï¼Œä½¿ç”¨ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿
5. Token-Level Optimization: æ”¯æŒtokençº§åˆ«çš„ç­–ç•¥ä¼˜åŒ–
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, pipeline
)
from datasets import load_dataset
import numpy as np
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
import wandb
from tqdm import tqdm
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GSPOConfig:
    """GSPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    policy_model_name: str = "Qwen/Qwen2-0.5B"
    reward_model_name: str = "OpenAssistant/reward-model-deberta-v3-large-v2"
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    mini_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    learning_rate: float = 1e-5
    num_epochs: int = 3
    max_length: int = 512
    
    # GSPOç‰¹æœ‰è¶…å‚æ•°
    gspo_epochs: int = 4  # æ¯ä¸ªæ‰¹æ¬¡æ•°æ®çš„GSPOæ›´æ–°æ¬¡æ•°
    clip_range: float = 0.2  # PPOè£å‰ªèŒƒå›´
    entropy_coef: float = 0.01  # ç†µæ­£åˆ™åŒ–ç³»æ•°
    kl_coef: float = 0.2  # KLæ•£åº¦æƒ©ç½šç³»æ•°
    target_kl: float = 0.01  # ç›®æ ‡KLæ•£åº¦
    adaptive_kl: bool = True  # æ˜¯å¦å¯ç”¨è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´
    
    # ğŸ”¥ GSPO Group Samplingå‚æ•°
    group_size: int = 4  # æ¯ç»„çš„æ ·æœ¬æ•°é‡
    use_group_normalization: bool = True  # æ˜¯å¦ä½¿ç”¨ç»„å†…æ ‡å‡†åŒ–
    
    # ğŸ”¥ GSPO Sequence-Levelä¼˜åŒ–å‚æ•°
    use_sequence_level_reward: bool = True  # æ˜¯å¦ä½¿ç”¨åºåˆ—çº§åˆ«å¥–åŠ±
    use_token_level_loss: bool = False  # æ˜¯å¦ä½¿ç”¨tokençº§åˆ«æŸå¤±ï¼ˆå¯é€‰ï¼‰
    
    # ğŸ”¥ GSPOä¼˜åŠ¿è®¡ç®—å‚æ•°
    advantage_type: str = "relative"  # ä¼˜åŠ¿ç±»å‹: "relative"(ç›¸å¯¹), "normalized"(æ ‡å‡†åŒ–)
    use_reward_shaping: bool = True  # æ˜¯å¦ä½¿ç”¨å¥–åŠ±å¡‘å½¢
    
    # å…¶ä»–é…ç½®
    save_steps: int = 500
    eval_steps: int = 100
    output_dir: str = "./gspo_output"
    use_wandb: bool = True
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class GSPODataset(Dataset):
    """GSPOè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, prompts: List[str], tokenizer, max_length: int = 512):
        self.prompts = prompts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        prompt = self.prompts[idx]
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "prompt": prompt
        }

class GSPOTrainer:
    """GSPOè®­ç»ƒå™¨"""
    
    def __init__(self, config: GSPOConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizers()
        
        # åˆå§‹åŒ–KLç³»æ•°
        self.kl_coef = config.kl_coef
        
        # åˆå§‹åŒ–wandb
        if config.use_wandb:
            wandb.init(project="gspo-qwen", config=config.__dict__)
    
    def _init_models(self):
        """åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼ˆGSPOä¸éœ€è¦criticæ¨¡å‹ï¼‰"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ç­–ç•¥æ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        
        # å¥–åŠ±æ¨¡å‹
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(
            self.config.reward_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.reward_tokenizer = AutoTokenizer.from_pretrained(self.config.reward_model_name)
        
        # å‚è€ƒç­–ç•¥æ¨¡å‹
        self.ref_policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.ref_policy_model.eval()
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼ˆGSPOæ— éœ€criticæ¨¡å‹ï¼‰")
    
    def _init_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.policy_optimizer = torch.optim.AdamW(
            self.policy_model.parameters(),
            lr=self.config.learning_rate
        )
    
    def generate_responses_with_group_sampling(self, prompts: List[str]) -> Tuple[List[str], List[str], List[int]]:
        """
        ğŸ”¥ GSPO Group Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        è¿”å›ï¼š(æ‰€æœ‰å›å¤, å¯¹åº”çš„prompt, å›å¤é•¿åº¦)
        """
        self.policy_model.eval()
        
        all_responses = []
        all_prompts_expanded = []
        all_response_lengths = []
        
        for prompt in prompts:
            for _ in range(self.config.group_size):
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,
                        temperature=0.7,
                        pad_token_id=self.tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                        output_scores=True
                    )
                
                generated_ids = outputs.sequences[0][inputs["input_ids"].shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                response_length = len(generated_ids)
                
                all_responses.append(response)
                all_prompts_expanded.append(prompt)
                all_response_lengths.append(response_length)
        
        return all_responses, all_prompts_expanded, all_response_lengths
    
    def compute_log_probs(self, prompts: List[str], responses: List[str], 
                         use_ref_model: bool = False,
                         return_per_token: bool = False) -> torch.Tensor:
        """
        è®¡ç®—logæ¦‚ç‡
        
        Args:
            return_per_token: æ˜¯å¦è¿”å›tokençº§åˆ«çš„logæ¦‚ç‡
        """
        all_log_probs = []
        all_token_log_probs = []
        
        model = self.ref_policy_model if use_ref_model else self.policy_model
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            with torch.no_grad():
                policy_outputs = model(**full_inputs)
                logits = policy_outputs.logits
                
                log_probs = F.log_softmax(logits, dim=-1)
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)
                
                response_log_probs = token_log_probs[0, response_start-1:-1]
                
                if return_per_token:
                    all_token_log_probs.append(response_log_probs)
                
                all_log_probs.append(response_log_probs.sum())
        
        if return_per_token:
            return all_token_log_probs
        else:
            return torch.stack(all_log_probs)
    
    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ğŸ”¥ GSPO Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±"""
        rewards = []
        
        for prompt, response in zip(prompts, responses):
            full_text = f"{prompt} {response}"
            
            inputs = self.reward_tokenizer(
                full_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                reward_outputs = self.reward_model(**inputs)
                reward = reward_outputs.logits[0, 0]
                rewards.append(reward)
        
        return torch.stack(rewards)
    
    def compute_relative_advantages(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ğŸ”¥ GSPO Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        è¿™æ˜¯GSPOçš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€
        """
        if group_size is None:
            group_size = self.config.group_size
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        # é‡å¡‘ä¸ºç»„çš„å½¢çŠ¶
        rewards_grouped = rewards.view(-1, group_size)
        
        # ğŸ”¥ è®¡ç®—ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
        
        # ğŸ”¥ è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
        if self.config.advantage_type == "relative":
            # ç›¸å¯¹ä¼˜åŠ¿ï¼šreward - baseline
            relative_advantages = rewards_grouped - group_baselines
        elif self.config.advantage_type == "normalized":
            # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼š(reward - baseline) / std
            relative_advantages = rewards_grouped - group_baselines
            if self.config.use_group_normalization:
                group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
                relative_advantages = relative_advantages / group_std
        else:
            raise ValueError(f"Unknown advantage type: {self.config.advantage_type}")
        
        # é‡æ–°å±•å¹³
        relative_advantages = relative_advantages.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_advantages, group_baselines
    
    def compute_kl_penalty(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦æƒ©ç½š"""
        current_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=False)
        ref_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=True)
        kl_divergence = current_log_probs - ref_log_probs
        return kl_divergence
    
    def compute_policy_loss_sequence_level(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                                          advantages: torch.Tensor) -> torch.Tensor:
        """
        ğŸ”¥ GSPO Sequence-Level Policy Loss: åºåˆ—çº§åˆ«çš„ç­–ç•¥æŸå¤±
        """
        ratio = torch.exp(log_probs - old_log_probs)
        
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        return policy_loss
    
    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor],
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """
        ğŸ”¥ GSPO Token-Level Policy Loss: tokençº§åˆ«çš„ç­–ç•¥æŸå¤±ï¼ˆå¯é€‰ï¼‰
        """
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            advantage = advantages[i]
            
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)
            
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios,
                               1 - self.config.clip_range,
                               1 + self.config.clip_range) * advantage
            
            token_loss = -torch.min(surr1, surr2).sum()
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        return total_token_loss / total_tokens
    
    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                          advantages: torch.Tensor, kl_penalty: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®¡ç®—GSPOç­–ç•¥æŸå¤±"""
        # é€‰æ‹©åºåˆ—çº§åˆ«æˆ–tokençº§åˆ«æŸå¤±
        if self.config.use_token_level_loss and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            policy_loss = self.compute_policy_loss_sequence_level(
                log_probs, old_log_probs, advantages
            )
        
        # ç†µæŸå¤±
        entropy = -log_probs.mean()
        entropy_loss = -self.config.entropy_coef * entropy
        
        # KLæŸå¤±
        kl_loss = self.kl_coef * kl_penalty.mean()
        
        return policy_loss, entropy_loss, kl_loss
    
    def update_kl_coef(self, kl_divergence: torch.Tensor):
        """è‡ªé€‚åº”è°ƒæ•´KLæ•£åº¦ç³»æ•°"""
        if not self.config.adaptive_kl:
            return
        
        mean_kl = kl_divergence.mean().item()
        
        if mean_kl > 2.0 * self.config.target_kl:
            self.kl_coef *= 1.5
        elif mean_kl < 0.5 * self.config.target_kl:
            self.kl_coef *= 0.5
        
        self.kl_coef = max(0.01, min(self.kl_coef, 1.0))
    
    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ­¥GSPOè®­ç»ƒ
        
        GSPOè®­ç»ƒæµç¨‹ï¼š
        1. Group Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        2. Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±
        3. Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        4. Policy Optimization: ä½¿ç”¨PPO-styleè£å‰ªæ›´æ–°ç­–ç•¥
        """
        # ğŸ”¥ 1. Group Sampling
        responses, prompts_expanded, response_lengths = self.generate_responses_with_group_sampling(batch_prompts)
        
        # ğŸ”¥ 2. Sequence-Level Rewards
        raw_rewards = self.compute_rewards(prompts_expanded, responses)
        
        # ğŸ”¥ 3. Relative Advantage
        relative_advantages, group_baselines = self.compute_relative_advantages(raw_rewards)
        
        # æˆªæ–­æ•°æ®
        prompts_truncated = prompts_expanded[:len(relative_advantages)]
        responses_truncated = responses[:len(relative_advantages)]
        
        # è®¡ç®—logæ¦‚ç‡
        log_probs = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=False)
        
        # å¦‚æœä½¿ç”¨token-level lossï¼Œè·å–tokençº§åˆ«çš„logæ¦‚ç‡
        old_token_log_probs_list = None
        if self.config.use_token_level_loss:
            old_token_log_probs_list = self.compute_log_probs(
                prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=True
            )
            old_token_log_probs_list = [t.detach() for t in old_token_log_probs_list]
        
        # è®¡ç®—KLæ•£åº¦æƒ©ç½š
        kl_penalty = self.compute_kl_penalty(prompts_truncated, responses_truncated)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (relative_advantages - relative_advantages.mean()) / (relative_advantages.std() + 1e-8)
        
        # ä¿å­˜æ—§çš„logæ¦‚ç‡
        old_log_probs = log_probs.detach()
        
        # ğŸ”¥ 4. GSPOæ›´æ–°å¾ªç¯
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        
        for gspo_step in range(self.config.gspo_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_log_probs = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=False)
            
            # å¦‚æœä½¿ç”¨token-level loss
            new_token_log_probs_list = None
            if self.config.use_token_level_loss:
                new_token_log_probs_list = self.compute_log_probs(
                    prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=True
                )
            
            # è®¡ç®—æŸå¤±
            policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
                new_log_probs, old_log_probs, advantages, kl_penalty,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            # æ€»æŸå¤±
            total_loss = policy_loss + entropy_loss + kl_loss
            
            # æ›´æ–°ç­–ç•¥æ¨¡å‹
            self.policy_optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.policy_optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_kl_loss += kl_loss.item()
        
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°
        self.update_kl_coef(kl_penalty)
        
        return {
            "policy_loss": total_policy_loss / self.config.gspo_epochs,
            "entropy_loss": total_entropy_loss / self.config.gspo_epochs,
            "kl_loss": total_kl_loss / self.config.gspo_epochs,
            "raw_reward_mean": raw_rewards.mean().item(),
            "raw_reward_std": raw_rewards.std().item(),
            "relative_advantage_mean": relative_advantages.mean().item(),
            "relative_advantage_std": relative_advantages.std().item(),
            "group_baseline_mean": group_baselines.mean().item(),
            "advantage_mean": advantages.mean().item(),
            "kl_divergence": kl_penalty.mean().item(),
            "kl_coef": self.kl_coef,
            "avg_response_length": np.mean(response_lengths[:len(relative_advantages)])
        }
    
    def train(self, train_dataset: GSPODataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹GSPOè®­ç»ƒ...")
        
        dataloader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )
        
        global_step = 0
        
        for epoch in range(self.config.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs}")
            
            epoch_metrics = []
            
            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch + 1}")):
                batch_prompts = batch["prompt"]
                
                metrics = self.train_step(batch_prompts)
                epoch_metrics.append(metrics)
                
                if self.config.use_wandb:
                    wandb.log({
                        "step": global_step,
                        "epoch": epoch,
                        **metrics
                    })
                
                if global_step % self.config.save_steps == 0:
                    self.save_checkpoint(global_step)
                
                global_step += 1
                
                if batch_idx % 10 == 0:
                    logger.info(f"Step {global_step}: {metrics}")
            
            # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
            avg_metrics = {}
            for key in epoch_metrics[0].keys():
                avg_metrics[f"epoch_{key}"] = np.mean([m[key] for m in epoch_metrics])
            
            logger.info(f"Epoch {epoch + 1} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
            
            if self.config.use_wandb:
                wandb.log(avg_metrics)
        
        logger.info("GSPOè®­ç»ƒå®Œæˆ!")
        self.save_checkpoint("final")
    
    def save_checkpoint(self, step):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜ç­–ç•¥æ¨¡å‹
        self.policy_model.save_pretrained(os.path.join(checkpoint_dir, "policy"))
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {checkpoint_dir}")

def load_training_data() -> List[str]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    try:
        dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1000]")
        
        prompts = []
        for item in dataset:
            conversation = item["chosen"]
            if conversation.startswith("Human:"):
                human_part = conversation.split("Assistant:")[0].replace("Human:", "").strip()
                if human_part:
                    prompts.append(human_part)
        
        logger.info(f"åŠ è½½äº† {len(prompts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return prompts
    
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½HHæ•°æ®é›†: {e}")
        logger.info("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        return [
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚",
            "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "è¯·è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚",
            "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"
        ] * 50

def main():
    """ä¸»å‡½æ•°"""
    config = GSPOConfig()
    
    os.makedirs(config.output_dir, exist_ok=True)
    
    prompts = load_training_data()
    
    tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset = GSPODataset(prompts, tokenizer, config.max_length)
    
    trainer = GSPOTrainer(config)
    
    trainer.train(train_dataset)

if __name__ == "__main__":
    main()
