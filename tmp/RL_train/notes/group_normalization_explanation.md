# Group Normalization 详解

## 你的理解完全正确！✅

```python
if self.config.use_group_normalization:
    group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
    relative_rewards = relative_rewards / group_std
```

**这段代码确实是对同一个问题的不同 answer 的奖励做归一化。**

## 完整流程分析

### 输入数据结构

假设：
- `batch_prompts = ['q1', 'q2']`  # 2个问题
- `group_size = 4`  # 每个问题生成4个答案

```python
# 原始奖励
rewards = Tensor([
    0.5, 0.8, 0.6, 0.9,  # q1的4个答案的奖励
    0.2, 0.3, 0.25, 0.28  # q2的4个答案的奖励
])
# shape: [8]
```

### Step 1: 重塑为分组结构

```python
rewards_grouped = rewards.view(-1, group_size)
# shape: [2, 4]
# [[0.5,  0.8,  0.6,  0.9 ],   # q1组
#  [0.2,  0.3,  0.25, 0.28]]   # q2组
```

### Step 2: 计算组内均值基线

```python
group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
# dim=1: 沿着每一行（每个组）计算均值
# keepdim=True: 保持维度 [2, 1]

# 计算结果：
# [[0.7 ],   # q1的4个答案的平均奖励: (0.5+0.8+0.6+0.9)/4 = 0.7
#  [0.2575]]  # q2的4个答案的平均奖励: (0.2+0.3+0.25+0.28)/4 = 0.2575
# shape: [2, 1]
```

### Step 3: 计算相对奖励（减去组内均值）

```python
relative_rewards = rewards_grouped - group_baselines
# 广播减法：[2, 4] - [2, 1] = [2, 4]

# 计算结果：
# [[ 0.5-0.7,   0.8-0.7,   0.6-0.7,   0.9-0.7  ],
#  [ 0.2-0.2575, 0.3-0.2575, 0.25-0.2575, 0.28-0.2575]]
# 
# = [[-0.2,    0.1,   -0.1,    0.2  ],   # q1组：相对于组内均值的偏差
#    [-0.0575, 0.0425, -0.0075, 0.0225]]  # q2组：相对于组内均值的偏差
# shape: [2, 4]
```

### Step 4: 🔥 组内标准化（Group Normalization）

```python
if self.config.use_group_normalization:
    # 计算每组的标准差
    group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
    # dim=1: 沿着每一行计算标准差
    
    # 计算结果：
    # [[0.1581],  # q1组的标准差: std([0.5, 0.8, 0.6, 0.9])
    #  [0.0416]]  # q2组的标准差: std([0.2, 0.3, 0.25, 0.28])
    # shape: [2, 1]
    
    # 除以标准差进行归一化
    relative_rewards = relative_rewards / group_std
    # [2, 4] / [2, 1] = [2, 4]
    
    # 计算结果：
    # [[-0.2/0.1581,   0.1/0.1581,  -0.1/0.1581,   0.2/0.1581 ],
    #  [-0.0575/0.0416, 0.0425/0.0416, -0.0075/0.0416, 0.0225/0.0416]]
    #
    # ≈ [[-1.265,  0.632, -0.632,  1.265],   # q1组：标准化后的相对奖励
    #    [-1.382,  1.022, -0.180,  0.541]]   # q2组：标准化后的相对奖励
```

### Step 5: 展平回原始形状

```python
relative_rewards = relative_rewards.view(-1)
# shape: [8]
# [-1.265, 0.632, -0.632, 1.265, -1.382, 1.022, -0.180, 0.541]
```

## 为什么要做 Group Normalization？

### 问题场景

不同问题的奖励分布可能差异很大：

```python
# 问题1：奖励范围大，方差大
q1_rewards = [0.1, 0.9, 0.3, 0.8]  # std ≈ 0.35

# 问题2：奖励范围小，方差小
q2_rewards = [0.45, 0.50, 0.48, 0.52]  # std ≈ 0.03
```

### 不使用 Group Normalization 的问题

```python
# 只减去均值，不除以标准差
q1_relative = [-0.425, 0.375, -0.225, 0.275]  # 范围大
q2_relative = [-0.0375, 0.0125, -0.0075, 0.0325]  # 范围小

# 问题：q1的梯度信号会远大于q2
# 导致模型更关注q1类型的问题，忽视q2类型的问题
```

### 使用 Group Normalization 的好处

```python
# 除以标准差后
q1_normalized = [-1.214, 1.071, -0.643, 0.786]  # 标准化范围
q2_normalized = [-1.250, 0.417, -0.250, 1.083]  # 标准化范围

# ✅ 好处：
# 1. 不同问题的相对奖励在相同的尺度上
# 2. 每个问题对训练的贡献更均衡
# 3. 训练更稳定，收敛更快
```

## 数学公式

对于第 i 个问题组：

```
原始奖励: r_i = [r_i1, r_i2, ..., r_iK]  (K = group_size)

组内均值: μ_i = (1/K) * Σ r_ij

相对奖励: Δr_ij = r_ij - μ_i

组内标准差: σ_i = sqrt((1/K) * Σ (r_ij - μ_i)²)

归一化相对奖励: normalized_Δr_ij = Δr_ij / σ_i
```

## 实际效果对比

### 示例数据

```python
# 两个问题，每个4个答案
rewards = Tensor([
    1.0, 5.0, 2.0, 4.0,   # q1: 高方差问题
    3.0, 3.1, 2.9, 3.05   # q2: 低方差问题
])
```

### 不使用 Group Normalization

```python
# 只减去均值
relative_rewards = [
    -2.0, 2.0, -1.0, 1.0,      # q1: 范围 [-2, 2]
    -0.0125, 0.0875, -0.1125, 0.0375  # q2: 范围 [-0.11, 0.09]
]

# 问题：q1的信号强度是q2的20倍！
```

### 使用 Group Normalization

```python
# 减去均值并除以标准差
relative_rewards = [
    -1.342, 1.342, -0.671, 0.671,   # q1: 标准化范围
    -0.134, 0.938, -1.206, 0.402    # q2: 标准化范围
]

# ✅ 两个问题的信号强度在同一尺度
```

## 代码中的关键点

```python
# 1. dim=1: 沿着每个组（每一行）计算
group_std = rewards_grouped.std(dim=1, keepdim=True)
#           ^^^^^^^^^^^^^^^^^ [2, 4]
#                             dim=1 →
#                             [[std of row 0],
#                              [std of row 1]]

# 2. keepdim=True: 保持维度以便广播
# shape: [2, 1] 而不是 [2]
# 这样可以直接做 [2, 4] / [2, 1] 的广播除法

# 3. + 1e-8: 防止除零
# 如果某组的所有奖励完全相同，std=0，加上小常数避免除零错误
```

## 总结

**你的理解完全正确！** 这段代码：

1. ✅ 对**同一个问题**的不同答案的奖励做归一化
2. ✅ 使每个问题组的相对奖励在**相同的尺度**上
3. ✅ 避免高方差问题主导训练过程
4. ✅ 提高训练的**稳定性和公平性**

这是 GRPO/DAPO 中非常重要的一个技巧，确保不同难度、不同奖励分布的问题都能得到合理的训练信号。
