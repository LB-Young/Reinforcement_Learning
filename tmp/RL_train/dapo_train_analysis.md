# DAPO vs GRPO ç®—æ³•å¯¹æ¯”åˆ†æ

## æ¦‚è¿°

DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization) æ˜¯ GRPO çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”±ç ”ç©¶äººå‘˜åœ¨å°è¯•å¤ç° DeepSeek-R1 æ—¶å¼€å‘ã€‚DAPO åœ¨ AIME 2024 æ•°å­¦ç«èµ›åŸºå‡†ä¸Šè¾¾åˆ° 50 åˆ†ï¼Œè¶…è¿‡äº† DeepSeek-R1 çš„ 47 åˆ†ï¼Œä¸”ä»…ä½¿ç”¨äº† 50% çš„è®­ç»ƒæ­¥æ•°ã€‚

## æ ¸å¿ƒå·®å¼‚å¯¹æ¯”

### 1. Clip-Higherï¼ˆéå¯¹ç§°è£å‰ªï¼‰

**GRPO å®ç°ï¼š**
```python
# å¯¹ç§°è£å‰ª
surr2 = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range) * advantages
# ä¾‹å¦‚ï¼šclip_range = 0.2ï¼ŒèŒƒå›´ä¸º [0.8, 1.2]
```

**DAPO å®ç°ï¼š**
```python
# éå¯¹ç§°è£å‰ª
surr2 = torch.clamp(ratio, 
                   1 - self.config.clip_range_low,   # 0.2 â†’ [0.8, ...]
                   1 + self.config.clip_range_high)  # 0.28 â†’ [..., 1.28]
                   * advantages
```

**æ”¹è¿›åŸå› ï¼š**
- GRPO çš„å¯¹ç§°è£å‰ªä¼šå¯¼è‡´**ç†µå´©æºƒ**ï¼ˆentropy collapseï¼‰
- æ¨¡å‹è¿‡æ—©å˜å¾—è¿‡äºç¡®å®šï¼Œé™åˆ¶æ¢ç´¢èƒ½åŠ›
- Clip-Higher å…è®¸æ¨¡å‹æ›´ç§¯æåœ°å¢åŠ å¥½å›å¤çš„æ¦‚ç‡
- ä¿æŒä¸‹ç•Œä¸å˜ï¼Œé¿å…å°†ä½æ¦‚ç‡ token å®Œå…¨å‹åˆ¶

**æ•ˆæœï¼š**
- é˜²æ­¢ç†µå´©æºƒï¼Œä¿æŒæ¢ç´¢èƒ½åŠ›
- æé«˜æ¨¡å‹åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°
- è®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«

---

### 2. Token-Level Lossï¼ˆtoken çº§åˆ«æŸå¤±ï¼‰

**GRPO å®ç°ï¼ˆSample-Levelï¼‰ï¼š**
```python
# grpo_train.py ä¸­çš„ç›®æ ‡å‡½æ•°
# å…ˆå¯¹æ¯ä¸ªå›å¤å†…çš„ token å¹³å‡ï¼Œå†å¯¹æ‰€æœ‰å›å¤å¹³å‡
loss = (1/G) * Î£[(1/|o_i|) * Î£ token_loss]

# åœ¨ä»£ç ä¸­ä½“ç°ä¸ºï¼š
# æ¯ä¸ªæ ·æœ¬çš„è´¡çŒ®è¢«å…¶é•¿åº¦å½’ä¸€åŒ–
policy_loss = -torch.min(surr1, surr2).mean()  # ç›´æ¥å¯¹æ‰€æœ‰æ ·æœ¬å¹³å‡
```

**é—®é¢˜ï¼š**
- çŸ­å›å¤çš„æ¯ä¸ª token æƒé‡æ›´å¤§
- æ¨¡å‹å€¾å‘äºç”ŸæˆçŸ­å›å¤æ¥"ä½œå¼Š"è·å¾—é«˜å¥–åŠ±
- ä¸åˆ©äºéœ€è¦è¯¦ç»†æ¨ç†çš„å¤æ‚ä»»åŠ¡

**DAPO å®ç°ï¼ˆToken-Levelï¼‰ï¼š**
```python
# dapo_train.py ä¸­çš„å®ç°
# å¯¹æ‰€æœ‰ token ä¸€èµ·å¹³å‡ï¼ŒæŒ‰å›å¤é•¿åº¦åŠ æƒ
if self.config.use_token_level_loss and response_lengths is not None:
    weights = torch.tensor(response_lengths, dtype=torch.float32, device=self.device)
    weights = weights / weights.sum()  # å½’ä¸€åŒ–
    policy_loss = -(torch.min(surr1, surr2) * weights).sum()
```

**æ”¹è¿›æ•ˆæœï¼š**
- é•¿å›å¤å’ŒçŸ­å›å¤çš„ token æƒé‡ç›¸åŒ
- é¼“åŠ±æ¨¡å‹ç”Ÿæˆè¯¦ç»†çš„æ¨ç†é“¾
- å‡å°‘"å¥–åŠ±é»‘å®¢"ï¼ˆreward hackingï¼‰è¡Œä¸º
- æ›´é€‚åˆéœ€è¦å¤šæ­¥æ¨ç†çš„ä»»åŠ¡

---

### 3. Dynamic Samplingï¼ˆåŠ¨æ€é‡‡æ ·ï¼‰

**GRPO å®ç°ï¼š**
```python
# å›ºå®šé‡‡æ · group_size ä¸ªå›å¤
responses, log_probs = self.generate_responses(batch_prompts)
raw_rewards = self.compute_rewards(batch_prompts, responses)
# å¦‚æœæ‰€æœ‰å¥–åŠ±ç›¸åŒï¼Œç›¸å¯¹å¥–åŠ±ä¸º 0ï¼Œæ²¡æœ‰è®­ç»ƒä¿¡å·
```

**é—®é¢˜ï¼š**
- å½“æ¨¡å‹å˜å¼ºåï¼ŒåŒä¸€é—®é¢˜çš„æ‰€æœ‰å›å¤å¯èƒ½éƒ½æ­£ç¡®
- å¯¼è‡´ç›¸å¯¹å¥–åŠ±å…¨ä¸º 0ï¼ˆå› ä¸ºéƒ½ä¸€æ ·ï¼‰
- æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒä¿¡å·
- å®é™… batch size å‡å°

**DAPO å®ç°ï¼š**
```python
def dynamic_sampling(self, prompt: str, initial_responses: List[str], 
                    initial_rewards: torch.Tensor) -> Tuple[List[str], torch.Tensor]:
    """
    å¦‚æœæ‰€æœ‰å›å¤çš„å¥–åŠ±ç›¸åŒï¼Œç»§ç»­é‡‡æ ·ç›´åˆ°æœ‰å·®å¼‚
    """
    responses = initial_responses.copy()
    rewards = initial_rewards.clone()
    
    reward_std = rewards.std().item()
    extra_samples = 0
    
    # å¦‚æœæ ‡å‡†å·®å¤ªå°ï¼ˆå¥–åŠ±éƒ½ç›¸åŒï¼‰ï¼Œç»§ç»­é‡‡æ ·
    while reward_std < 1e-6 and len(responses) < self.config.max_dynamic_samples:
        extra_response, _, _ = self.generate_responses([prompt], num_responses=1)
        extra_reward = self.compute_rewards([prompt], extra_response)
        
        responses.extend(extra_response)
        rewards = torch.cat([rewards, extra_reward])
        reward_std = rewards.std().item()
        extra_samples += 1
    
    return responses, rewards
```

**æ”¹è¿›æ•ˆæœï¼š**
- ç¡®ä¿æ¯ä¸ªé—®é¢˜éƒ½æœ‰æœ‰æ•ˆçš„è®­ç»ƒä¿¡å·
- é˜²æ­¢è®­ç»ƒåæœŸä¿¡å·æ¶ˆå¤±
- è™½ç„¶å¢åŠ è®¡ç®—æˆæœ¬ï¼Œä½†å¯ä»¥ç”¨æ›´å°‘çš„è®­ç»ƒæ­¥æ•°è¾¾åˆ°ç›¸åŒæ•ˆæœ
- å®éªŒæ˜¾ç¤ºï¼šåŠ¨æ€é‡‡æ ·åªéœ€ 1/3 çš„è®­ç»ƒæ­¥æ•°å°±èƒ½è¾¾åˆ°ç›¸åŒæ€§èƒ½

**æ³¨æ„äº‹é¡¹ï¼š**
- å¢åŠ çº¦ 25% çš„è®¡ç®—æ—¶é—´
- å¯èƒ½åœ¨æ¨¡å‹å·²ç»å¾ˆå¼ºæ—¶å¼•å…¥æ¬¡ä¼˜æ ·æœ¬
- éœ€è¦é…åˆç†µå¥–åŠ±ä½¿ç”¨ä»¥å‡å°‘å¼€é”€

---

### 4. KL Divergence Penaltyï¼ˆKL æ•£åº¦æƒ©ç½šï¼‰

**GRPO å®ç°ï¼š**
```python
# grpo_train.py ä¸­æ˜¾å¼åŒ…å« KL æƒ©ç½š
kl_penalty = self.compute_kl_penalty_simple(prompts, responses)
kl_loss = self.kl_coef * kl_penalty.mean()  # kl_coef = 0.2

# åœ¨ç›®æ ‡å‡½æ•°ä¸­
total_loss = policy_loss + entropy_loss + kl_loss
```

**DAPO å®ç°ï¼š**
```python
# dapo_train.py ä¸­é»˜è®¤ç§»é™¤ KL æƒ©ç½š
kl_coef: float = 0.0
use_kl_penalty: bool = False

# åœ¨æŸå¤±è®¡ç®—ä¸­
kl_loss = self.kl_coef * kl_penalty.mean() if self.config.use_kl_penalty else torch.tensor(0.0)
```

**ç§»é™¤åŸå› ï¼š**
- åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹åˆ†å¸ƒéœ€è¦æ˜¾è‘—åç¦»é¢„è®­ç»ƒæ¨¡å‹
- KL çº¦æŸä¼šé™åˆ¶è¿™ç§å¿…è¦çš„åç¦»
- Clip-Higher æœºåˆ¶å·²ç»æä¾›äº†è¶³å¤Ÿçš„ç¨³å®šæ€§
- ç§»é™¤ KL æƒ©ç½šå¯ä»¥ä¿ƒè¿›æ›´å¤šæ¢ç´¢

**æƒè¡¡ï¼š**
- ä¼˜ç‚¹ï¼šå…è®¸æ¨¡å‹æ›´è‡ªç”±åœ°æ¢ç´¢ï¼Œé€‚åˆéœ€è¦å¤§å¹…æ”¹å˜è¡Œä¸ºçš„ä»»åŠ¡
- ç¼ºç‚¹ï¼šå¯èƒ½å¯¼è‡´ç­–ç•¥åç¦»è¿‡è¿œï¼Œç”Ÿæˆä¸è¿è´¯çš„è¾“å‡º
- å»ºè®®ï¼šæ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©æ˜¯å¦ä½¿ç”¨

---

### 5. Overlong Response Handlingï¼ˆè¿‡é•¿å›å¤å¤„ç†ï¼‰

**GRPO å®ç°ï¼š**
```python
# æ²¡æœ‰ç‰¹æ®Šå¤„ç†
# å¦‚æœå›å¤è¢«æˆªæ–­ï¼Œç›´æ¥ç»™è´Ÿå¥–åŠ±
```

**é—®é¢˜ï¼š**
- è¢«æˆªæ–­çš„å›å¤å³ä½¿å‰é¢æ¨ç†æ­£ç¡®ä¹Ÿä¼šå¾—åˆ°è´Ÿå¥–åŠ±
- æ··æ·†æ¨¡å‹çš„å­¦ä¹ ä¿¡å·
- ä¸å…¬å¹³åœ°æƒ©ç½šäº†é«˜è´¨é‡çš„é•¿æ¨ç†é“¾

**DAPO å®ç°ï¼š**

**æ–¹æ¡ˆ 1ï¼šOverlong Filteringï¼ˆè¿‡æ»¤è¿‡é•¿å›å¤ï¼‰**
```python
if self.config.use_overlong_filtering:
    valid_indices = [i for i, length in enumerate(response_lengths) 
                   if length < self.config.max_response_length]
    if len(valid_indices) > 0:
        responses = [responses[i] for i in valid_indices]
        rewards = rewards[valid_indices]
```

**æ–¹æ¡ˆ 2ï¼šSoft Overlong Punishmentï¼ˆè½¯æƒ©ç½šï¼‰**
```python
def apply_soft_overlong_punishment(self, rewards: torch.Tensor, 
                                  response_lengths: List[int]) -> torch.Tensor:
    threshold_length = int(self.config.max_response_length * self.config.overlong_threshold)
    
    for reward, length in zip(rewards, response_lengths):
        if length > threshold_length:
            # æ¸è¿›å¼æƒ©ç½šï¼šè¶…å‡ºéƒ¨åˆ†è¶Šå¤šï¼Œæƒ©ç½šè¶Šå¤§
            excess_ratio = (length - threshold_length) / threshold_length
            punishment = -0.5 * excess_ratio
            punished_reward = reward + punishment
```

**æ”¹è¿›æ•ˆæœï¼š**
- é¿å…ä¸å…¬å¹³åœ°æƒ©ç½šé«˜è´¨é‡é•¿å›å¤
- æ¸è¿›å¼æƒ©ç½šæ›´åˆç†
- æé«˜è®­ç»ƒç¨³å®šæ€§

---

## é…ç½®å‚æ•°å¯¹æ¯”

| å‚æ•° | GRPO | DAPO | è¯´æ˜ |
|------|------|------|------|
| `clip_range` | 0.2 (å¯¹ç§°) | `clip_range_low=0.2`<br>`clip_range_high=0.28` | DAPO ä½¿ç”¨éå¯¹ç§°è£å‰ª |
| `kl_coef` | 0.2 | 0.0 | DAPO é»˜è®¤ç§»é™¤ KL æƒ©ç½š |
| `use_dynamic_sampling` | âŒ | âœ… | DAPO ç‰¹æœ‰åŠŸèƒ½ |
| `use_token_level_loss` | âŒ (Sample-level) | âœ… | DAPO ä½¿ç”¨ token çº§åˆ«æŸå¤± |
| `use_overlong_filtering` | âŒ | âœ… | DAPO ç‰¹æœ‰åŠŸèƒ½ |
| æŸå¤±èšåˆæ–¹å¼ | Sample-level | Token-level | æ ¸å¿ƒå·®å¼‚ |

---

## æ€§èƒ½å¯¹æ¯”

### å®éªŒç»“æœï¼ˆAIME 2024ï¼ŒQwen2.5-32Bï¼‰

| æ¨¡å‹ | åˆ†æ•° | è®­ç»ƒæ­¥æ•° |
|------|------|----------|
| DeepSeek-R1 (GRPO) | 47 | 100% |
| DAPO | 50 | 50% |

### å…³é”®æŒ‡æ ‡æ”¹è¿›

1. **å‡†ç¡®ç‡**ï¼šDAPO æé«˜ 6.4%ï¼ˆ47â†’50ï¼‰
2. **è®­ç»ƒæ•ˆç‡**ï¼šDAPO ä»…éœ€ 50% çš„è®­ç»ƒæ­¥æ•°
3. **ç†µç¨³å®šæ€§**ï¼šDAPO é¿å…äº†ç†µå´©æºƒ
4. **å›å¤é•¿åº¦**ï¼šDAPO ç”Ÿæˆæ›´é•¿ã€æ›´è¯¦ç»†çš„æ¨ç†é“¾

---

## ä»£ç ç»“æ„å¯¹æ¯”

### GRPO æ ¸å¿ƒæµç¨‹
```python
# 1. ç”Ÿæˆå›å¤
responses, log_probs = generate_responses(prompts)

# 2. è®¡ç®—å¥–åŠ±
rewards = compute_rewards(prompts, responses)

# 3. è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼ˆç»„å†…æ ‡å‡†åŒ–ï¼‰
relative_rewards, baselines = compute_relative_rewards(rewards)

# 4. æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆå…¨å±€æ ‡å‡†åŒ–ï¼‰
advantages = compute_advantages(relative_rewards)

# 5. è®¡ç®—æŸå¤±ï¼ˆå¯¹ç§°è£å‰ª + KL æƒ©ç½šï¼‰
policy_loss = compute_policy_loss(log_probs, old_log_probs, advantages, kl_penalty)
```

### DAPO æ ¸å¿ƒæµç¨‹
```python
# 1. ç”Ÿæˆå›å¤
responses, log_probs, lengths = generate_responses(prompts)

# 2. è®¡ç®—å¥–åŠ±
rewards = compute_rewards(prompts, responses)

# 3. ğŸ”¥ åº”ç”¨è½¯æƒ©ç½š
rewards = apply_soft_overlong_punishment(rewards, lengths)

# 4. ğŸ”¥ åŠ¨æ€é‡‡æ ·
responses, rewards = dynamic_sampling(prompt, responses, rewards)

# 5. ğŸ”¥ è¿‡æ»¤è¿‡é•¿å›å¤
responses, rewards = filter_overlong(responses, rewards, lengths)

# 6. è®¡ç®—ç›¸å¯¹å¥–åŠ±
relative_rewards, baselines = compute_relative_rewards(rewards)

# 7. æ ‡å‡†åŒ–ä¼˜åŠ¿
advantages = compute_advantages(relative_rewards)

# 8. ğŸ”¥ è®¡ç®—æŸå¤±ï¼ˆéå¯¹ç§°è£å‰ª + Token-Level + æ—  KLï¼‰
policy_loss = compute_policy_loss(log_probs, old_log_probs, advantages, 
                                 kl_penalty=None, response_lengths=lengths)
```

---

## é€‚ç”¨åœºæ™¯

### GRPO æ›´é€‚åˆï¼š
- çŸ­æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- è®¡ç®—èµ„æºæœ‰é™çš„åœºæ™¯
- éœ€è¦ä¸¥æ ¼æ§åˆ¶ç­–ç•¥åç¦»çš„ä»»åŠ¡
- ç®€å•çš„é—®ç­”ä»»åŠ¡

### DAPO æ›´é€‚åˆï¼š
- é•¿é“¾æ¨ç†ä»»åŠ¡ï¼ˆæ•°å­¦ã€ç¼–ç¨‹ã€å¤æ‚æ¨ç†ï¼‰
- éœ€è¦è¯¦ç»†è§£é‡Šçš„ä»»åŠ¡
- æ¨¡å‹å·²ç»è¾ƒå¼ºï¼Œéœ€è¦è¿›ä¸€æ­¥æå‡çš„åœºæ™¯
- æœ‰å……è¶³è®¡ç®—èµ„æºçš„åœºæ™¯
- éœ€è¦å¿«é€Ÿæ”¶æ•›çš„åœºæ™¯

---

## å®ç°å»ºè®®

### ä» GRPO è¿ç§»åˆ° DAPO

1. **æœ€å°æ”¹åŠ¨ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰**ï¼š
   - ä¿®æ”¹è£å‰ªèŒƒå›´ä¸ºéå¯¹ç§°
   - ç§»é™¤æˆ–å‡å° KL æƒ©ç½šç³»æ•°
   - å®ç° token-level loss

2. **å®Œæ•´åŠŸèƒ½**ï¼š
   - æ·»åŠ åŠ¨æ€é‡‡æ ·é€»è¾‘
   - å®ç°è¿‡é•¿å›å¤å¤„ç†
   - è°ƒæ•´è¶…å‚æ•°

3. **è¶…å‚æ•°è°ƒä¼˜**ï¼š
   ```python
   # DAPO æ¨èé…ç½®
   clip_range_low = 0.2
   clip_range_high = 0.28
   kl_coef = 0.0
   use_dynamic_sampling = True
   use_token_level_loss = True
   group_size = 4  # å¯ä»¥æ›´å°ï¼Œå› ä¸ºæœ‰åŠ¨æ€é‡‡æ ·
   ```

---

## æ€»ç»“

DAPO é€šè¿‡å››ä¸ªæ ¸å¿ƒæ”¹è¿›è§£å†³äº† GRPO åœ¨é•¿é“¾æ¨ç†ä»»åŠ¡ä¸­çš„ä¸»è¦é—®é¢˜ï¼š

1. **Clip-Higher** â†’ è§£å†³ç†µå´©æºƒ
2. **Token-Level Loss** â†’ è§£å†³å¥–åŠ±é»‘å®¢å’ŒçŸ­å›å¤åå¥½
3. **Dynamic Sampling** â†’ è§£å†³è®­ç»ƒä¿¡å·æ¶ˆå¤±
4. **ç§»é™¤ KL æƒ©ç½š** â†’ å…è®¸å¿…è¦çš„ç­–ç•¥åç¦»

è¿™äº›æ”¹è¿›ä½¿ DAPO åœ¨æ•°å­¦æ¨ç†ç­‰éœ€è¦é•¿é“¾æ€è€ƒçš„ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äº GRPOï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒç¨³å®šæ€§å¹¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚

---

## å‚è€ƒæ–‡çŒ®

1. DAPO è®ºæ–‡ï¼š[DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2512.07611)
2. DeepSeek-R1 è®ºæ–‡ï¼šDeepSeek-R1 Technical Report
3. GRPO åŸå§‹è®ºæ–‡ï¼šDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

---

## é™„å½•ï¼šå…³é”®ä»£ç ç‰‡æ®µå¯¹æ¯”

### A. è£å‰ªå‡½æ•°å¯¹æ¯”

**GRPO:**
```python
surr2 = torch.clamp(ratio, 1 - 0.2, 1 + 0.2) * advantages
# èŒƒå›´ï¼š[0.8, 1.2]
```

**DAPO:**
```python
surr2 = torch.clamp(ratio, 1 - 0.2, 1 + 0.28) * advantages
# èŒƒå›´ï¼š[0.8, 1.28]
```

### B. æŸå¤±èšåˆå¯¹æ¯”

**GRPO (Sample-Level):**
```python
policy_loss = -torch.min(surr1, surr2).mean()
```

**DAPO (Token-Level):**
```python
weights = torch.tensor(response_lengths, dtype=torch.float32)
weights = weights / weights.sum()
policy_loss = -(torch.min(surr1, surr2) * weights).sum()
```

### C. åŠ¨æ€é‡‡æ ·ä¼ªä»£ç 

```python
# DAPO ç‹¬æœ‰
while reward_std < threshold and samples < max_samples:
    new_sample = generate_one_more()
    samples.append(new_sample)
    reward_std = compute_std(samples)
```
