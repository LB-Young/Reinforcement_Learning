#!/usr/bin/env python3
"""
GRPO (Group Relative Policy Optimization) è®­ç»ƒè„šæœ¬ - åŸºäºQwen2-0.5B
GRPOæ˜¯PPOçš„å˜ç§ï¼Œä½¿ç”¨ç›¸å¯¹å¥–åŠ±å’Œç»„å†…æ¯”è¾ƒæ¥ä¼˜åŒ–ç­–ç•¥
"""

import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œ
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.nn.functional as F  # ç¥ç»ç½‘ç»œå‡½æ•°åº“
from torch.utils.data import DataLoader, Dataset  # æ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†åŸºç±»
from transformers import (  # Hugging Face transformersåº“
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,  # è‡ªåŠ¨æ¨¡å‹å’Œåˆ†è¯å™¨
    TrainingArguments, Trainer, pipeline  # è®­ç»ƒå‚æ•°ã€è®­ç»ƒå™¨ã€ç®¡é“
)
from datasets import load_dataset  # æ•°æ®é›†åŠ è½½å·¥å…·
import numpy as np  # æ•°å€¼è®¡ç®—åº“
from typing import Dict, List, Optional, Tuple  # ç±»å‹æç¤º
import logging  # æ—¥å¿—è®°å½•
from dataclasses import dataclass  # æ•°æ®ç±»è£…é¥°å™¨
import wandb  # å®éªŒè·Ÿè¸ªå·¥å…·
from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º
import json  # JSONæ•°æ®å¤„ç†
from accelerate import Accelerator  # å¤šGPUè®­ç»ƒåŠ é€Ÿå™¨

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)  # é…ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨

@dataclass
class GRPOConfig:
    """GRPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    policy_model_name: str = r"E:\models\Qwen\Qwen3-0___6B"  # ç­–ç•¥æ¨¡å‹åç§°ï¼Œç”¨äºç”Ÿæˆå›å¤
    reward_model_name: str = r"E:\models\reward-model-deberta-v3-large-v2"  # å¥–åŠ±æ¨¡å‹åç§°ï¼Œç”¨äºè¯„ä¼°å›å¤è´¨é‡
    # æ³¨æ„ï¼šGRPOä¸éœ€è¦criticæ¨¡å‹ï¼
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8  # æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
    mini_batch_size: int = 2  # GRPOæ›´æ–°æ—¶çš„å°æ‰¹æ¬¡å¤§å°ï¼Œç”¨äºå†…å­˜ä¼˜åŒ–
    gradient_accumulation_steps: int = 4  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
    learning_rate: float = 1e-5  # ç­–ç•¥æ¨¡å‹çš„å­¦ä¹ ç‡
    num_epochs: int = 3  # æ€»è®­ç»ƒè½®æ•°
    max_length: int = 512  # è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦
    
    # GRPOç‰¹æœ‰è¶…å‚æ•°
    grpo_epochs: int = 4  # æ¯ä¸ªæ‰¹æ¬¡æ•°æ®çš„GRPOæ›´æ–°æ¬¡æ•°
    clip_range: float = 0.2  # GRPOè£å‰ªèŒƒå›´ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
    entropy_coef: float = 0.01  # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
    kl_coef: float = 0.2  # KLæ•£åº¦æƒ©ç½šç³»æ•°ï¼Œé˜²æ­¢ç­–ç•¥åç¦»reference modelå¤ªè¿œ
    target_kl: float = 0.01  # ç›®æ ‡KLæ•£åº¦ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´kl_coef
    adaptive_kl: bool = True  # æ˜¯å¦å¯ç”¨è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´
    
    # GRPOç‰¹æœ‰å‚æ•°
    group_size: int = 4  # æ¯ç»„çš„æ ·æœ¬æ•°é‡ï¼Œç”¨äºç›¸å¯¹æ¯”è¾ƒ
    use_group_normalization: bool = True  # æ˜¯å¦ä½¿ç”¨ç»„å†…æ ‡å‡†åŒ–
    
    # å¤šGPUé…ç½®
    use_multi_gpu: bool = True  # æ˜¯å¦ä½¿ç”¨å¤šGPUè®­ç»ƒ
    mixed_precision: str = "fp16"  # æ··åˆç²¾åº¦è®­ç»ƒï¼šfp16, bf16, no
    
    # å…¶ä»–é…ç½®
    save_steps: int = 500  # æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹
    eval_steps: int = 100  # æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    output_dir: str = "./grpo_output"  # æ¨¡å‹è¾“å‡ºå’Œæ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
    use_wandb: bool = True  # æ˜¯å¦ä½¿ç”¨wandbè¿›è¡Œå®éªŒè·Ÿè¸ª
    device: str = "cuda" if torch.cuda.is_available() else "cpu"  # è®­ç»ƒè®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨GPU

class GRPODataset(Dataset):
    """GRPOè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, prompts: List[str], tokenizer, max_length: int = 512):
        self.prompts = prompts  # å­˜å‚¨æ‰€æœ‰çš„æç¤ºæ–‡æœ¬
        self.tokenizer = tokenizer  # åˆ†è¯å™¨ï¼Œç”¨äºæ–‡æœ¬ç¼–ç 
        self.max_length = max_length  # åºåˆ—æœ€å¤§é•¿åº¦ï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­
    
    def __len__(self):
        return len(self.prompts)  # è¿”å›æ•°æ®é›†å¤§å°
    
    def __getitem__(self, idx):
        prompt = self.prompts[idx]  # è·å–æŒ‡å®šç´¢å¼•çš„æç¤ºæ–‡æœ¬
        encoding = self.tokenizer(  # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
            prompt,
            truncation=True,  # å¯ç”¨æˆªæ–­ï¼Œè¶…å‡ºmax_lengthçš„éƒ¨åˆ†ä¼šè¢«åˆ é™¤
            padding="max_length",  # å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼ŒçŸ­åºåˆ—ç”¨pad_tokenå¡«å……
            max_length=self.max_length,  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
            return_tensors="pt"  # è¿”å›PyTorchå¼ é‡æ ¼å¼
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),  # è¾“å…¥tokençš„IDåºåˆ—ï¼Œå»é™¤æ‰¹æ¬¡ç»´åº¦
            "attention_mask": encoding["attention_mask"].squeeze(),  # æ³¨æ„åŠ›æ©ç ï¼Œæ ‡è¯†å“ªäº›ä½ç½®æ˜¯çœŸå®token
            "prompt": prompt  # åŸå§‹æç¤ºæ–‡æœ¬ï¼Œç”¨äºåç»­å¤„ç†
        }

class GRPOTrainer:
    """GRPOè®­ç»ƒå™¨"""
    
    def __init__(self, config: GRPOConfig):
        self.config = config  # ä¿å­˜è®­ç»ƒé…ç½®
        
        # ğŸ”¥ åˆå§‹åŒ–Acceleratorç”¨äºå¤šGPUè®­ç»ƒ
        if config.use_multi_gpu:
            self.accelerator = Accelerator(
                mixed_precision=config.mixed_precision,  # æ··åˆç²¾åº¦è®­ç»ƒ
                gradient_accumulation_steps=config.gradient_accumulation_steps  # æ¢¯åº¦ç´¯ç§¯
            )
            self.device = self.accelerator.device  # ä½¿ç”¨acceleratorç®¡ç†çš„è®¾å¤‡
            logger.info(f"ä½¿ç”¨å¤šGPUè®­ç»ƒï¼Œè®¾å¤‡æ•°é‡: {self.accelerator.num_processes}")
        else:
            self.accelerator = None
            self.device = torch.device(config.device)  # è®¾ç½®è®¡ç®—è®¾å¤‡(CPU/GPU)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)  # åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨
        if self.tokenizer.pad_token is None:  # å¦‚æœæ²¡æœ‰å¡«å……token
            self.tokenizer.pad_token = self.tokenizer.eos_token  # ä½¿ç”¨ç»“æŸtokenä½œä¸ºå¡«å……token
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()  # è°ƒç”¨æ¨¡å‹åˆå§‹åŒ–æ–¹æ³•
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizers()  # è°ƒç”¨ä¼˜åŒ–å™¨åˆå§‹åŒ–æ–¹æ³•
        
        # ğŸ”¥ ä½¿ç”¨Acceleratorå‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if self.accelerator:
            self.policy_model, self.policy_optimizer = self.accelerator.prepare(
                self.policy_model, self.policy_optimizer
            )
            # å¥–åŠ±æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ä¸éœ€è¦è®­ç»ƒï¼Œåªéœ€è¦ç§»åˆ°è®¾å¤‡ä¸Š
            self.reward_model = self.accelerator.prepare(self.reward_model)
            self.ref_policy_model = self.accelerator.prepare(self.ref_policy_model)
        
        # åˆå§‹åŒ–KLç³»æ•°ï¼ˆç”¨äºè‡ªé€‚åº”è°ƒæ•´ï¼‰
        self.kl_coef = config.kl_coef  # å½“å‰KLæ•£åº¦æƒ©ç½šç³»æ•°
        
        # åˆå§‹åŒ–wandbï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if config.use_wandb and (not self.accelerator or self.accelerator.is_main_process):
            wandb.init(project="grpo-qwen", config=config.__dict__)  # åˆå§‹åŒ–wandbé¡¹ç›®
    
    def _init_models(self):
        """åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼ˆGRPOä¸éœ€è¦criticæ¨¡å‹ï¼‰"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ğŸ”¥ å¤šGPUè®­ç»ƒæ—¶ä¸ä½¿ç”¨device_map="auto"ï¼Œè®©Acceleratorç®¡ç†è®¾å¤‡åˆ†é…
        if self.config.use_multi_gpu:
            # ç­–ç•¥æ¨¡å‹ (Qwen2-0.5B)
            self.policy_model = AutoModelForCausalLM.from_pretrained(
                self.config.policy_model_name,
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
            )
            
            # å¥–åŠ±æ¨¡å‹
            self.reward_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.reward_model_name,
                torch_dtype=torch.float16,
            )
            
            # å‚è€ƒç­–ç•¥æ¨¡å‹
            self.ref_policy_model = AutoModelForCausalLM.from_pretrained(
                self.config.policy_model_name,
                torch_dtype=torch.float16,
            )
        else:
            # å•GPUæˆ–CPUè®­ç»ƒ
            self.policy_model = AutoModelForCausalLM.from_pretrained(
                self.config.policy_model_name,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None
            )
            
            self.reward_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.reward_model_name,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None
            )
            
            self.ref_policy_model = AutoModelForCausalLM.from_pretrained(
                self.config.policy_model_name,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                device_map="auto" if self.device.type == "cuda" else None
            )
        
        self.reward_tokenizer = AutoTokenizer.from_pretrained(self.config.reward_model_name)
        # ä¸ºreward tokenizerè®¾ç½®pad_token
        if self.reward_tokenizer.pad_token is None:
            self.reward_tokenizer.pad_token = self.reward_tokenizer.eos_token
        
        self.ref_policy_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä¸æ›´æ–°å‚æ•°
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆï¼ˆGRPOæ— éœ€criticæ¨¡å‹ï¼‰")
    
    def _init_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆGRPOåªéœ€è¦ç­–ç•¥ä¼˜åŒ–å™¨ï¼‰"""
        self.policy_optimizer = torch.optim.AdamW(  # ç­–ç•¥æ¨¡å‹ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨AdamWç®—æ³•
            self.policy_model.parameters(),  # ç­–ç•¥æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
            lr=self.config.learning_rate  # è®¾ç½®å­¦ä¹ ç‡
        )
    
    def generate_responses(self, prompts: List[str]) -> Tuple[List[str], torch.Tensor, List[str]]:
        """
        ğŸ”¥ GRPOæ ¸å¿ƒï¼šä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        è¿”å›ï¼š(å›å¤åˆ—è¡¨, logæ¦‚ç‡, å¯¹åº”çš„promptåˆ—è¡¨)
        """
        self.policy_model.eval()
        
        all_responses = []
        all_prompts_expanded = []
        
        # ğŸ”¥ å…³é”®ï¼šä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤
        for prompt in prompts:
            for _ in range(self.config.group_size):
                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ç”Ÿæˆå›å¤
                with torch.no_grad():
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,  # å¿…é¡»å¯ç”¨é‡‡æ ·æ‰èƒ½ç”Ÿæˆä¸åŒçš„å›å¤
                        temperature=0.7,
                        pad_token_id=self.tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                        output_scores=True
                    )
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_ids = outputs.sequences[0][inputs["input_ids"].shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                all_responses.append(response)
                all_prompts_expanded.append(prompt)
        
        # æ‰¹é‡è®¡ç®—logæ¦‚ç‡
        log_probs = self.compute_log_probs(all_prompts_expanded, all_responses)
        
        return all_responses, log_probs, all_prompts_expanded
    def compute_log_probs(self, prompts: List[str], responses: List[str], 
                         use_ref_model: bool = False) -> torch.Tensor:
        """æ‰¹é‡è®¡ç®—logæ¦‚ç‡ï¼ˆGRPOä¸éœ€è¦valuesï¼‰"""
        all_log_probs = []  # å­˜å‚¨æ‰€æœ‰logæ¦‚ç‡
        
        # é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹
        model = self.ref_policy_model if use_ref_model else self.policy_model  # æ ¹æ®å‚æ•°é€‰æ‹©å‚è€ƒæ¨¡å‹æˆ–å½“å‰ç­–ç•¥æ¨¡å‹
        
        for prompt, response in zip(prompts, responses):  # éå†æç¤ºå’Œå›å¤å¯¹
            # æ‹¼æ¥å®Œæ•´å¯¹è¯
            full_text = prompt + response  # ç»„åˆå®Œæ•´æ–‡æœ¬
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)  # ç¼–ç å®Œæ•´æ–‡æœ¬
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}  # ç§»åˆ°æŒ‡å®šè®¾å¤‡
            
            # ç¼–ç promptä»¥ç¡®å®šå›å¤å¼€å§‹ä½ç½®
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)  # ç¼–ç æç¤ºéƒ¨åˆ†
            response_start = prompt_inputs["input_ids"].shape[1]  # è®¡ç®—å›å¤å¼€å§‹çš„tokenä½ç½®
            
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
                # è®¡ç®—logæ¦‚ç‡
                policy_outputs = model(**full_inputs)  # è·å–æ¨¡å‹è¾“å‡º
                logits = policy_outputs.logits  # æå–logits
                
                # è®¡ç®—tokençº§åˆ«çš„logæ¦‚ç‡
                log_probs = F.log_softmax(logits, dim=-1)  # åº”ç”¨log softmax
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)  # æ”¶é›†å®é™…tokençš„logæ¦‚ç‡
                
                # åªè€ƒè™‘ç”Ÿæˆéƒ¨åˆ†çš„logæ¦‚ç‡
                response_log_probs = token_log_probs[0, response_start-1:-1]  # æå–å›å¤éƒ¨åˆ†ï¼Œæ’é™¤æœ€åä¸€ä¸ªtoken
                all_log_probs.append(response_log_probs.sum())  # ä½¿ç”¨sumè€Œä¸æ˜¯meanï¼Œä¿æŒä¸tokenæ•°é‡çš„å…³ç³»
        
        return torch.stack(all_log_probs)  # è¿”å›å †å çš„å¼ é‡

    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å¥–åŠ±"""
        rewards = []  # å­˜å‚¨è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
        
        for prompt, response in zip(prompts, responses):  # éå†æç¤ºå’Œå›å¤å¯¹
            # ç»„åˆpromptå’Œresponse
            full_text = f"{prompt} {response}"  # æ‹¼æ¥å®Œæ•´å¯¹è¯æ–‡æœ¬
            
            # ä½¿ç”¨å¥–åŠ±æ¨¡å‹tokenizerç¼–ç 
            inputs = self.reward_tokenizer(  # ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä¸“ç”¨åˆ†è¯å™¨
                full_text,
                return_tensors="pt",  # è¿”å›PyTorchå¼ é‡
                padding=True,  # å¯ç”¨å¡«å……
                truncation=True,  # å¯ç”¨æˆªæ–­
                max_length=512  # è®¾ç½®æœ€å¤§é•¿åº¦ä¸º512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}  # å°†è¾“å…¥ç§»åˆ°æŒ‡å®šè®¾å¤‡
            
            # è®¡ç®—å¥–åŠ±
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
                reward_outputs = self.reward_model(**inputs)  # é€šè¿‡å¥–åŠ±æ¨¡å‹è·å–è¾“å‡º
                reward = reward_outputs.logits[0, 0]  # å‡è®¾æ˜¯äºŒåˆ†ç±»ï¼Œå–ç¬¬ä¸€ä¸ªç±»åˆ«çš„logitä½œä¸ºå¥–åŠ±
                rewards.append(reward)  # æ·»åŠ åˆ°å¥–åŠ±åˆ—è¡¨
        
        return torch.stack(rewards)  # å°†å¥–åŠ±åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡å¹¶è¿”å›
    
    def compute_relative_rewards(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—GRPOçš„ç›¸å¯¹å¥–åŠ± - GRPOçš„æ ¸å¿ƒåˆ›æ–°
        è¿”å›ï¼š(ç›¸å¯¹å¥–åŠ±, ç»„å†…å‡å€¼åŸºçº¿)
        """
        if group_size is None:
            group_size = self.config.group_size
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            # å¦‚æœæ‰¹æ¬¡å¤§å°ä¸èƒ½è¢«ç»„å¤§å°æ•´é™¤ï¼Œæˆªæ–­åˆ°æœ€å¤§çš„å®Œæ•´ç»„æ•°
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        # å°†å¥–åŠ±é‡å¡‘ä¸ºç»„çš„å½¢çŠ¶ [num_groups, group_size]
        rewards_grouped = rewards.view(-1, group_size)
        
        # ğŸ”¥ GRPOæ ¸å¿ƒï¼šè®¡ç®—æ¯ç»„çš„å¹³å‡å¥–åŠ±ä½œä¸ºåŸºçº¿ï¼ˆæ›¿ä»£criticçš„valueï¼‰
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)  # [num_groups, 1]
        
        # ğŸ”¥ è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼šæ¯ä¸ªæ ·æœ¬çš„å¥–åŠ±å‡å»ç»„å†…å¹³å‡å€¼
        # è¿™å°±æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼šadvantage = reward - baseline
        relative_rewards = rewards_grouped - group_baselines  # [num_groups, group_size]
        
        # å¯é€‰ï¼šç»„å†…æ ‡å‡†åŒ–
        if self.config.use_group_normalization:
            group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
            relative_rewards = relative_rewards / group_std
        
        # é‡æ–°å±•å¹³ä¸ºåŸå§‹å½¢çŠ¶
        relative_rewards = relative_rewards.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_rewards, group_baselines
    
    def compute_kl_penalty_simple(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦æƒ©ç½šçš„ç®€åŒ–ç‰ˆæœ¬ï¼ˆå¸¸ç”¨äºå®é™…å®ç°ï¼‰"""
        # è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
        current_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=False)  # å½“å‰ç­–ç•¥æ¨¡å‹çš„logæ¦‚ç‡
        
        # è®¡ç®—å‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡
        ref_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=True)  # å‚è€ƒæ¨¡å‹çš„logæ¦‚ç‡
        
        # ç®€åŒ–çš„KLæ•£åº¦ä¼°è®¡ï¼šå¯¹äºå·²ç”Ÿæˆçš„åºåˆ—ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆç†çš„è¿‘ä¼¼
        # å› ä¸ºæˆ‘ä»¬å·²ç»ä»å½“å‰ç­–ç•¥é‡‡æ ·äº†åŠ¨ä½œï¼Œæ‰€ä»¥ E_{a~Ï€_Î¸}[log Ï€_Î¸ - log Ï€_ref] â‰ˆ log Ï€_Î¸(a) - log Ï€_ref(a)
        kl_divergence = current_log_probs - ref_log_probs  # ç®€åŒ–çš„KLæ•£åº¦ä¼°è®¡
        
        return kl_divergence  # è¿”å›KLæ•£åº¦ä¼°è®¡
    
    def compute_advantages(self, advantages: torch.Tensor) -> torch.Tensor:
        """
        GRPOçš„ä¼˜åŠ¿å‡½æ•°è®¡ç®—ï¼ˆå·²ç»åœ¨compute_relative_rewardsä¸­å®Œæˆï¼‰
        è¿™é‡Œåªéœ€è¦æ ‡å‡†åŒ–
        """
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼Œå‡å‡å€¼é™¤æ ‡å‡†å·®ï¼ŒåŠ å°å¸¸æ•°é˜²æ­¢é™¤é›¶
        
        return advantages  # è¿”å›æ ‡å‡†åŒ–çš„ä¼˜åŠ¿
    
    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor, 
                          advantages: torch.Tensor, kl_penalty: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®¡ç®—GRPOç­–ç•¥æŸå¤±ã€ç†µæŸå¤±å’ŒKLæŸå¤±ï¼ˆæ— value lossï¼‰"""
        # è®¡ç®—æ¦‚ç‡æ¯”ç‡
        ratio = torch.exp(log_probs - old_log_probs)  # æ–°ç­–ç•¥æ¦‚ç‡ / æ—§ç­–ç•¥æ¦‚ç‡
        
        # GRPO clipæŸå¤± (ä¸PPOç›¸åŒçš„è£å‰ªæœºåˆ¶)
        surr1 = ratio * advantages  # æœªè£å‰ªçš„ç­–ç•¥æ¢¯åº¦ç›®æ ‡ï¼Ÿ
        surr2 = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range) * advantages  # è£å‰ªåçš„ç›®æ ‡ï¼Œé™åˆ¶æ¯”ç‡åœ¨[1-Îµ, 1+Îµ]èŒƒå›´å†…
        policy_loss = -torch.min(surr1, surr2).mean()  # å–ä¸¤è€…æœ€å°å€¼çš„è´Ÿæ•°ä½œä¸ºæŸå¤±ï¼ˆå› ä¸ºè¦æœ€å¤§åŒ–ç›®æ ‡ï¼‰
        
        # è®¡ç®—ç†µæŸå¤±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        entropy = -log_probs.mean()  # ç®€åŒ–çš„ç†µè®¡ç®—
        entropy_loss = -self.config.entropy_coef * entropy  # ç†µæŸå¤±ï¼Œè´Ÿå·å› ä¸ºè¦æœ€å¤§åŒ–ç†µ
        
        # è®¡ç®—KLæŸå¤±
        kl_loss = self.kl_coef * kl_penalty.mean()  # KLæ•£åº¦æƒ©ç½šæŸå¤±
        
        return policy_loss, entropy_loss, kl_loss  # è¿”å›ç­–ç•¥æŸå¤±ã€ç†µæŸå¤±å’ŒKLæŸå¤±ï¼ˆæ— value lossï¼‰
    
    def update_kl_coef(self, kl_divergence: torch.Tensor):
        """è‡ªé€‚åº”è°ƒæ•´KLæ•£åº¦ç³»æ•°"""
        if not self.config.adaptive_kl:  # å¦‚æœæœªå¯ç”¨è‡ªé€‚åº”è°ƒæ•´
            return
        
        mean_kl = kl_divergence.mean().item()  # è®¡ç®—å¹³å‡KLæ•£åº¦
        
        if mean_kl > 2.0 * self.config.target_kl:  # å¦‚æœKLæ•£åº¦è¿‡å¤§
            self.kl_coef *= 1.5  # å¢åŠ KLæƒ©ç½šç³»æ•°
        elif mean_kl < 0.5 * self.config.target_kl:  # å¦‚æœKLæ•£åº¦è¿‡å°
            self.kl_coef *= 0.5  # å‡å°‘KLæƒ©ç½šç³»æ•°
        
        # é™åˆ¶KLç³»æ•°çš„èŒƒå›´
        self.kl_coef = max(0.01, min(self.kl_coef, 1.0))  # å°†KLç³»æ•°é™åˆ¶åœ¨[0.01, 1.0]èŒƒå›´å†…
    
    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ­¥GRPOè®­ç»ƒ
        ğŸ”¥ å…³é”®ï¼šæ¯ä¸ªpromptä¼šç”Ÿæˆgroup_sizeä¸ªå›å¤ï¼Œç„¶åè®¡ç®—ç»„å†…ç›¸å¯¹å¥–åŠ±
        """
        # ğŸ”¥ ç”Ÿæˆå›å¤ï¼šæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤
        # ä¾‹å¦‚ï¼šbatch_prompts=['q1', 'q2'], group_size=4
        # è¿”å›ï¼šresponses=['a1_1', 'a1_2', 'a1_3', 'a1_4', 'a2_1', 'a2_2', 'a2_3', 'a2_4']
        responses, log_probs, prompts_expanded = self.generate_responses(batch_prompts)
        
        # è®¡ç®—åŸå§‹å¥–åŠ±
        raw_rewards = self.compute_rewards(prompts_expanded, responses)
        
        # ğŸ”¥ GRPOæ ¸å¿ƒï¼šè®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼ˆä¼˜åŠ¿ï¼‰å’ŒåŸºçº¿
        # å°†rewardsæŒ‰group_sizeåˆ†ç»„ï¼Œè®¡ç®—ç»„å†…ç›¸å¯¹å¥–åŠ±
        # ä¾‹å¦‚ï¼š[r1_1, r1_2, r1_3, r1_4] -> å‡å»ç»„å†…å‡å€¼ -> [adv1_1, adv1_2, adv1_3, adv1_4]
        relative_rewards, group_baselines = self.compute_relative_rewards(raw_rewards)
        
        # æˆªæ–­æ•°æ®ä»¥åŒ¹é…ç›¸å¯¹å¥–åŠ±çš„é•¿åº¦
        prompts_truncated = prompts_expanded[:len(relative_rewards)]
        responses_truncated = responses[:len(relative_rewards)]
        log_probs_truncated = log_probs[:len(relative_rewards)]
        
        # è®¡ç®—KLæ•£åº¦æƒ©ç½š
        kl_penalty = self.compute_kl_penalty_simple(prompts_truncated, responses_truncated)
        
        # ğŸ”¥ GRPOçš„ä¼˜åŠ¿å‡½æ•°å°±æ˜¯ç›¸å¯¹å¥–åŠ±ï¼ˆå·²ç»å‡å»äº†ç»„å†…å‡å€¼åŸºçº¿ï¼‰
        advantages = self.compute_advantages(relative_rewards)
        
        # ä¿å­˜æ—§çš„logæ¦‚ç‡ç”¨äºGRPO
        old_log_probs = log_probs_truncated.detach()
        
        # GRPOæ›´æ–°å¾ªç¯
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        
        for grpo_step in range(self.config.grpo_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_log_probs = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=False)
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            ratio = torch.exp(new_log_probs - old_log_probs)  # Ï€_new / Ï€_old
            ratio_mean = ratio.mean().item()  # å¹³å‡æ¯”ç‡
            
            # è®¡ç®—æŸå¤±ï¼ˆGRPOæ— value lossï¼‰
            policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(new_log_probs, old_log_probs, advantages, kl_penalty)
            
            # ğŸ”¥ æ€»æŸå¤±ï¼šGRPOæŸå¤± + ç†µæŸå¤± + KLæŸå¤±ï¼ˆæ— value lossï¼‰
            total_loss = policy_loss + entropy_loss + kl_loss
            
            # ç­–ç•¥æ¨¡å‹æ›´æ–°
            self.policy_optimizer.zero_grad()  # æ¸…é›¶ç­–ç•¥æ¨¡å‹æ¢¯åº¦
            
            # ğŸ”¥ ä½¿ç”¨Acceleratorçš„backwardæˆ–æ™®é€šbackward
            if self.accelerator:
                self.accelerator.backward(total_loss)  # Acceleratorç®¡ç†çš„åå‘ä¼ æ’­
            else:
                total_loss.backward()  # æ™®é€šåå‘ä¼ æ’­
            
            # æ¢¯åº¦è£å‰ª
            if self.accelerator:
                self.accelerator.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            else:
                torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            
            self.policy_optimizer.step()  # æ›´æ–°ç­–ç•¥æ¨¡å‹å‚æ•°
            
            total_policy_loss += policy_loss.item()  # ç´¯åŠ ç­–ç•¥æŸå¤±å€¼
            total_entropy_loss += entropy_loss.item()  # ç´¯åŠ ç†µæŸå¤±å€¼
            total_kl_loss += kl_loss.item()  # ç´¯åŠ KLæŸå¤±å€¼
            
            # è®°å½•æ¯æ­¥çš„æ¯”ç‡å˜åŒ–ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
            if grpo_step == 0:
                first_ratio = ratio_mean  # ç¬¬ä¸€æ­¥çš„æ¯”ç‡åº”è¯¥æ¥è¿‘1.0
        
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°
        self.update_kl_coef(kl_penalty)  # æ ¹æ®å½“å‰KLæ•£åº¦è°ƒæ•´æƒ©ç½šç³»æ•°
        
        return {  # è¿”å›è®­ç»ƒæŒ‡æ ‡å­—å…¸
            "policy_loss": total_policy_loss / self.config.grpo_epochs,  # å¹³å‡ç­–ç•¥æŸå¤±
            "entropy_loss": total_entropy_loss / self.config.grpo_epochs,  # å¹³å‡ç†µæŸå¤±
            "kl_loss": total_kl_loss / self.config.grpo_epochs,  # å¹³å‡KLæŸå¤±
            "raw_reward_mean": raw_rewards.mean().item(),  # åŸå§‹å¥–åŠ±å‡å€¼
            "raw_reward_std": raw_rewards.std().item(),  # åŸå§‹å¥–åŠ±æ ‡å‡†å·®
            "relative_reward_mean": relative_rewards.mean().item(),  # ç›¸å¯¹å¥–åŠ±å‡å€¼ï¼ˆåº”æ¥è¿‘0ï¼‰
            "relative_reward_std": relative_rewards.std().item(),  # ç›¸å¯¹å¥–åŠ±æ ‡å‡†å·®
            "group_baseline_mean": group_baselines.mean().item(),  # ç»„å†…åŸºçº¿å‡å€¼
            "advantage_mean": advantages.mean().item(),  # ä¼˜åŠ¿å‡å€¼ï¼ˆæ ‡å‡†åŒ–ååº”æ¥è¿‘0ï¼‰
            "kl_divergence": kl_penalty.mean().item(),  # å¹³å‡KLæ•£åº¦
            "kl_coef": self.kl_coef,  # å½“å‰KLç³»æ•°
            "first_step_ratio": first_ratio if 'first_ratio' in locals() else 1.0  # ç¬¬ä¸€æ­¥çš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        }
    
    def train(self, train_dataset: GRPODataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹GRPOè®­ç»ƒ...")
        
        dataloader = DataLoader(  # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataset,  # è®­ç»ƒæ•°æ®é›†
            batch_size=self.config.batch_size,  # æ‰¹æ¬¡å¤§å°
            shuffle=True  # æ¯ä¸ªepochéšæœºæ‰“ä¹±æ•°æ®é¡ºåº
        )
        
        # ğŸ”¥ ä½¿ç”¨Acceleratorå‡†å¤‡dataloader
        if self.accelerator:
            dataloader = self.accelerator.prepare(dataloader)
        
        global_step = 0  # å…¨å±€è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        
        for epoch in range(self.config.num_epochs):  # éå†æ¯ä¸ªè®­ç»ƒè½®æ¬¡
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs}")
            
            epoch_metrics = []  # å­˜å‚¨å½“å‰epochçš„æ‰€æœ‰æŒ‡æ ‡
            
            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch + 1}")):  # éå†æ¯ä¸ªæ‰¹æ¬¡ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
                # æå–prompts
                batch_prompts = batch["prompt"]  # ä»æ‰¹æ¬¡ä¸­æå–æç¤ºæ–‡æœ¬åˆ—è¡¨
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                metrics = self.train_step(batch_prompts)  # æ‰§è¡Œä¸€æ­¥GRPOè®­ç»ƒå¹¶è·å–æŒ‡æ ‡
                epoch_metrics.append(metrics)  # å°†æŒ‡æ ‡æ·»åŠ åˆ°epochæŒ‡æ ‡åˆ—è¡¨
                
                # è®°å½•æŒ‡æ ‡ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
                if self.config.use_wandb and (not self.accelerator or self.accelerator.is_main_process):
                    wandb.log({  # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°wandb
                        "step": global_step,  # å½“å‰æ­¥æ•°
                        "epoch": epoch,  # å½“å‰epoch
                        **metrics  # å±•å¼€æ‰€æœ‰è®­ç»ƒæŒ‡æ ‡
                    })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
                if global_step % self.config.save_steps == 0:
                    if not self.accelerator or self.accelerator.is_main_process:
                        self.save_checkpoint(global_step)
                
                global_step += 1  # å¢åŠ å…¨å±€æ­¥æ•°è®¡æ•°
                
                # æ‰“å°è¿›åº¦ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
                if batch_idx % 10 == 0:
                    if not self.accelerator or self.accelerator.is_main_process:
                        logger.info(f"Step {global_step}: {metrics}")
            
            # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
            avg_metrics = {}  # å­˜å‚¨å¹³å‡æŒ‡æ ‡çš„å­—å…¸
            for key in epoch_metrics[0].keys():  # éå†æŒ‡æ ‡çš„æ‰€æœ‰é”®
                avg_metrics[f"epoch_{key}"] = np.mean([m[key] for m in epoch_metrics])  # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡åœ¨æ•´ä¸ªepochçš„å¹³å‡å€¼
            
            if not self.accelerator or self.accelerator.is_main_process:
                logger.info(f"Epoch {epoch + 1} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
            
            if self.config.use_wandb and (not self.accelerator or self.accelerator.is_main_process):
                wandb.log(avg_metrics)  # è®°å½•epochå¹³å‡æŒ‡æ ‡
        
        if not self.accelerator or self.accelerator.is_main_process:
            logger.info("GRPOè®­ç»ƒå®Œæˆ!")
            self.save_checkpoint("final")  # ä¿å­˜æœ€ç»ˆæ¨¡å‹æ£€æŸ¥ç‚¹
    
    def save_checkpoint(self, step):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆGRPOåªéœ€ä¿å­˜ç­–ç•¥æ¨¡å‹ï¼‰"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ğŸ”¥ ä½¿ç”¨Acceleratorçš„unwrap_modelè·å–åŸå§‹æ¨¡å‹
        if self.accelerator:
            unwrapped_model = self.accelerator.unwrap_model(self.policy_model)
            unwrapped_model.save_pretrained(
                os.path.join(checkpoint_dir, "policy"),
                save_function=self.accelerator.save  # ä½¿ç”¨acceleratorçš„ä¿å­˜å‡½æ•°
            )
        else:
            self.policy_model.save_pretrained(os.path.join(checkpoint_dir, "policy"))
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {checkpoint_dir}")

def load_training_data() -> List[str]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    try:
        # ä½¿ç”¨Anthropic HHæ•°æ®é›†ä½œä¸ºç¤ºä¾‹
        dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1000]")  # å–å‰1000æ¡ç”¨äºæ¼”ç¤º
        
        prompts = []
        for item in dataset:
            # æå–humançš„é—®é¢˜ä½œä¸ºprompt
            conversation = item["chosen"]
            if conversation.startswith("Human:"):
                # æå–Humançš„éƒ¨åˆ†ä½œä¸ºprompt
                human_part = conversation.split("Assistant:")[0].replace("Human:", "").strip()
                if human_part:
                    prompts.append(human_part)
        
        logger.info(f"åŠ è½½äº† {len(prompts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return prompts
    
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½HHæ•°æ®é›†: {e}")
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        logger.info("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        return [
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚",
            "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "è¯·è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚",
            "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"
        ] * 50  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = GRPOConfig()
    
    # éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config.policy_model_name):
        raise FileNotFoundError(f"ç­–ç•¥æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {config.policy_model_name}")
    if not os.path.exists(config.reward_model_name):
        raise FileNotFoundError(f"å¥–åŠ±æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {config.reward_model_name}")
    
    logger.info(f"ç­–ç•¥æ¨¡å‹è·¯å¾„: {config.policy_model_name}")
    logger.info(f"å¥–åŠ±æ¨¡å‹è·¯å¾„: {config.reward_model_name}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.output_dir, exist_ok=True)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    prompts = load_training_data()
    
    # åˆ›å»ºæ•°æ®é›†
    tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset = GRPODataset(prompts, tokenizer, config.max_length)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GRPOTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_dataset)

if __name__ == "__main__":
    main()