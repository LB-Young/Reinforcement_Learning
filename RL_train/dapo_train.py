#!/usr/bin/env python3
"""
DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization) è®­ç»ƒè„šæœ¬ - åŸºäºQwen2-0.5B
DAPOæ˜¯GRPOçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡Clip-Higherã€Token-Level Lossã€Dynamic Samplingç­‰æŠ€æœ¯æå‡æ€§èƒ½
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, pipeline
)
from datasets import load_dataset
import numpy as np
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
import wandb
from tqdm import tqdm
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DAPOConfig:
    """DAPOè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    policy_model_name: str = "Qwen/Qwen2-0.5B"
    reward_model_name: str = "OpenAssistant/reward-model-deberta-v3-large-v2"
    
    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    mini_batch_size: int = 2
    gradient_accumulation_steps: int = 4
    learning_rate: float = 1e-5
    num_epochs: int = 3
    max_length: int = 512
    
    # DAPOç‰¹æœ‰è¶…å‚æ•°
    dapo_epochs: int = 4
    clip_range_low: float = 0.2  # ä¸‹ç•Œè£å‰ªèŒƒå›´
    clip_range_high: float = 0.28  # ğŸ”¥ Clip-Higher: ä¸Šç•Œè£å‰ªèŒƒå›´æ›´å¤§
    entropy_coef: float = 0.01
    kl_coef: float = 0.0  # ğŸ”¥ DAPOç§»é™¤KLæƒ©ç½š
    use_kl_penalty: bool = False  # ğŸ”¥ æ˜¯å¦ä½¿ç”¨KLæƒ©ç½š
    
    # DAPOç‰¹æœ‰å‚æ•°
    group_size: int = 4
    use_group_normalization: bool = True
    use_dynamic_sampling: bool = True  # ğŸ”¥ æ˜¯å¦å¯ç”¨åŠ¨æ€é‡‡æ ·
    max_dynamic_samples: int = 8  # åŠ¨æ€é‡‡æ ·æœ€å¤§æ ·æœ¬æ•°
    use_token_level_loss: bool = True  # ğŸ”¥ æ˜¯å¦ä½¿ç”¨tokençº§åˆ«æŸå¤±
    
    # Overlong responseå¤„ç†
    max_response_length: int = 256
    use_overlong_filtering: bool = True  # ğŸ”¥ è¿‡æ»¤è¿‡é•¿å›å¤
    use_soft_overlong_punishment: bool = False  # è½¯æƒ©ç½šè¿‡é•¿å›å¤
    overlong_threshold: float = 0.8  # è¶…è¿‡max_lengthçš„æ¯”ä¾‹å¼€å§‹æƒ©ç½š
    
    # å…¶ä»–é…ç½®
    save_steps: int = 500
    eval_steps: int = 100
    output_dir: str = "./dapo_output"
    use_wandb: bool = True
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class DAPODataset(Dataset):
    """DAPOè®­ç»ƒæ•°æ®é›†"""
    
    def __init__(self, prompts: List[str], tokenizer, max_length: int = 512):
        self.prompts = prompts
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        prompt = self.prompts[idx]
        encoding = self.tokenizer(
            prompt,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "prompt": prompt
        }

class DAPOTrainer:
    """DAPOè®­ç»ƒå™¨"""
    
    def __init__(self, config: DAPOConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self._init_optimizers()
        
        # åˆå§‹åŒ–KLç³»æ•°
        self.kl_coef = config.kl_coef
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.dynamic_sampling_stats = {
            "total_questions": 0,
            "resampled_questions": 0,
            "avg_extra_samples": 0.0
        }
        
        # åˆå§‹åŒ–wandb
        if config.use_wandb:
            wandb.init(project="dapo-qwen", config=config.__dict__)
    
    def _init_models(self):
        """åˆå§‹åŒ–ç­–ç•¥æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹"""
        logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ç­–ç•¥æ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        
        # å¥–åŠ±æ¨¡å‹
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(
            self.config.reward_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.reward_tokenizer = AutoTokenizer.from_pretrained(self.config.reward_model_name)
        
        # å‚è€ƒç­–ç•¥æ¨¡å‹
        self.ref_policy_model = AutoModelForCausalLM.from_pretrained(
            self.config.policy_model_name,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            device_map="auto" if self.device.type == "cuda" else None
        )
        self.ref_policy_model.eval()
        
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _init_optimizers(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        self.policy_optimizer = torch.optim.AdamW(
            self.policy_model.parameters(),
            lr=self.config.learning_rate
        )
    
    def apply_soft_overlong_punishment(self, rewards: torch.Tensor, response_lengths: List[int]) -> torch.Tensor:
        """
        ğŸ”¥ DAPOç‰¹æ€§ï¼šè½¯æƒ©ç½šè¿‡é•¿å›å¤
        å¯¹è¶…è¿‡é˜ˆå€¼çš„å›å¤è¿›è¡Œæ¸è¿›å¼æƒ©ç½š
        """
        if not self.config.use_soft_overlong_punishment:
            return rewards
        
        threshold_length = int(self.config.max_response_length * self.config.overlong_threshold)
        punished_rewards = []
        
        for reward, length in zip(rewards, response_lengths):
            if length > threshold_length:
                # æ¸è¿›å¼æƒ©ç½šï¼šè¶…å‡ºéƒ¨åˆ†è¶Šå¤šï¼Œæƒ©ç½šè¶Šå¤§
                excess_ratio = (length - threshold_length) / threshold_length
                punishment = -0.5 * excess_ratio  # æƒ©ç½šç³»æ•°å¯è°ƒ
                punished_reward = reward + punishment
            else:
                punished_reward = reward
            punished_rewards.append(punished_reward)
        
        return torch.stack(punished_rewards)
    
    def compute_log_probs(self, prompts: List[str], responses: List[str], 
                         use_ref_model: bool = False, return_per_token: bool = False) -> torch.Tensor:
        """
        æ‰¹é‡è®¡ç®—logæ¦‚ç‡
        
        Args:
            prompts: æç¤ºåˆ—è¡¨
            responses: å›å¤åˆ—è¡¨
            use_ref_model: æ˜¯å¦ä½¿ç”¨å‚è€ƒæ¨¡å‹
            return_per_token: ğŸ”¥ æ˜¯å¦è¿”å›æ¯ä¸ªtokençš„logæ¦‚ç‡ï¼ˆç”¨äºtoken-level lossï¼‰
        
        Returns:
            å¦‚æœreturn_per_token=False: è¿”å›æ¯ä¸ªæ ·æœ¬çš„æ€»logæ¦‚ç‡ [batch_size]
            å¦‚æœreturn_per_token=True: è¿”å›æ¯ä¸ªtokençš„logæ¦‚ç‡åˆ—è¡¨ List[Tensor]
        """
        all_log_probs = []
        all_token_log_probs = []  # ğŸ”¥ å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„tokençº§åˆ«logæ¦‚ç‡
        
        model = self.ref_policy_model if use_ref_model else self.policy_model
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True)
            full_inputs = {k: v.to(self.device) for k, v in full_inputs.items()}
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            with torch.no_grad() if use_ref_model else torch.enable_grad():
                policy_outputs = model(**full_inputs)
                logits = policy_outputs.logits
                
                log_probs = F.log_softmax(logits, dim=-1)
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)
                
                # åªè€ƒè™‘ç”Ÿæˆéƒ¨åˆ†çš„logæ¦‚ç‡
                response_log_probs = token_log_probs[0, response_start-1:-1]
                
                if return_per_token:
                    all_token_log_probs.append(response_log_probs)  # ğŸ”¥ ä¿å­˜æ¯ä¸ªtokençš„logæ¦‚ç‡
                
                all_log_probs.append(response_log_probs.sum())
        
        if return_per_token:
            return all_token_log_probs  # ğŸ”¥ è¿”å›tokençº§åˆ«çš„logæ¦‚ç‡åˆ—è¡¨
        else:
            return torch.stack(all_log_probs)  # è¿”å›æ ·æœ¬çº§åˆ«çš„æ€»logæ¦‚ç‡
    
    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å¥–åŠ±"""
        rewards = []
        
        for prompt, response in zip(prompts, responses):
            full_text = f"{prompt} {response}"
            
            inputs = self.reward_tokenizer(
                full_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                reward_outputs = self.reward_model(**inputs)
                reward = reward_outputs.logits[0, 0]
                rewards.append(reward)
        
        return torch.stack(rewards)
    
    def compute_relative_rewards(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—DAPOçš„ç›¸å¯¹å¥–åŠ±ï¼ˆä¸GRPOç›¸åŒï¼‰
        è¿”å›ï¼š(ç›¸å¯¹å¥–åŠ±, ç»„å†…å‡å€¼åŸºçº¿)
        """
        if group_size is None:
            group_size = self.config.group_size
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        rewards_grouped = rewards.view(-1, group_size)
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
        relative_rewards = rewards_grouped - group_baselines
        
        if self.config.use_group_normalization:     # åŒä¸€ä¸ªé—®é¢˜çš„ä¸åŒç­”æ¡ˆä¹‹é—´åšå½’ä¸€åŒ–
            group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
            relative_rewards = relative_rewards / group_std
        
        relative_rewards = relative_rewards.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_rewards, group_baselines
    
    def compute_kl_penalty_simple(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦æƒ©ç½šï¼ˆDAPOé»˜è®¤ä¸ä½¿ç”¨ï¼‰"""
        if not self.config.use_kl_penalty:
            return torch.zeros(len(prompts), device=self.device)
        
        current_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=False)
        ref_log_probs = self.compute_log_probs(prompts, responses, use_ref_model=True)
        kl_divergence = current_log_probs - ref_log_probs
        
        return kl_divergence
    
    def compute_advantages(self, advantages: torch.Tensor) -> torch.Tensor:
        """æ ‡å‡†åŒ–ä¼˜åŠ¿"""
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        return advantages
    
    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor], 
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """
        ğŸ”¥ DAPO Token-Level Loss: å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—PPOæŸå¤±
        
        Args:
            token_log_probs_list: å½“å‰ç­–ç•¥æ¯ä¸ªæ ·æœ¬çš„tokençº§åˆ«logæ¦‚ç‡åˆ—è¡¨
            old_token_log_probs_list: æ—§ç­–ç•¥æ¯ä¸ªæ ·æœ¬çš„tokençº§åˆ«logæ¦‚ç‡åˆ—è¡¨
            advantages: æ¯ä¸ªæ ·æœ¬çš„ä¼˜åŠ¿å€¼ [batch_size]
        
        Returns:
            tokençº§åˆ«çš„ç­–ç•¥æŸå¤±
        """
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            # å¯¹æ¯ä¸ªtokenè®¡ç®—ratioå’Œclipped surrogate loss
            advantage = advantages[i]  # è¯¥æ ·æœ¬çš„ä¼˜åŠ¿å€¼
            
            # è®¡ç®—æ¯ä¸ªtokençš„æ¦‚ç‡æ¯”ç‡
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)  # [num_tokens]
            
            # ğŸ”¥ DAPO Clip-Higher: éå¯¹ç§°è£å‰ªï¼Œå¯¹æ¯ä¸ªtokenåº”ç”¨
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios, 
                               1 - self.config.clip_range_low, 
                               1 + self.config.clip_range_high) * advantage
            
            # å¯¹æ¯ä¸ªtokenå–æœ€å°å€¼ï¼Œç„¶åæ±‚å’Œ
            token_loss = -torch.min(surr1, surr2).sum()  # å¯¹è¯¥æ ·æœ¬çš„æ‰€æœ‰tokenæ±‚å’Œ
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        # è¿”å›å¹³å‡tokenæŸå¤±
        return total_token_loss / total_tokens
    
    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor, 
                          advantages: torch.Tensor, kl_penalty: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ğŸ”¥ è®¡ç®—DAPOç­–ç•¥æŸå¤±ï¼ˆä½¿ç”¨Clip-Higherå’ŒToken-Level Lossï¼‰
        
        Args:
            log_probs: æ ·æœ¬çº§åˆ«çš„logæ¦‚ç‡ï¼ˆç”¨äºç†µè®¡ç®—ï¼‰
            old_log_probs: æ—§ç­–ç•¥çš„æ ·æœ¬çº§åˆ«logæ¦‚ç‡
            advantages: ä¼˜åŠ¿å€¼
            kl_penalty: KLæƒ©ç½š
            token_log_probs_list: ğŸ”¥ tokençº§åˆ«çš„logæ¦‚ç‡åˆ—è¡¨ï¼ˆç”¨äºtoken-level lossï¼‰
            old_token_log_probs_list: ğŸ”¥ æ—§ç­–ç•¥çš„tokençº§åˆ«logæ¦‚ç‡åˆ—è¡¨
        """
        # ğŸ”¥ Token-Level Loss: å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—æŸå¤±
        if self.config.use_token_level_loss and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            # Sample-level loss (GRPOæ–¹å¼)
            ratio = torch.exp(log_probs - old_log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 
                               1 - self.config.clip_range_low, 
                               1 + self.config.clip_range_high) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
        
        # ç†µæŸå¤±ï¼ˆä½¿ç”¨æ ·æœ¬çº§åˆ«çš„logæ¦‚ç‡ï¼‰
        entropy = -log_probs.mean()
        entropy_loss = -self.config.entropy_coef * entropy
        
        # KLæŸå¤±ï¼ˆDAPOé»˜è®¤ä¸ä½¿ç”¨ï¼‰
        kl_loss = self.kl_coef * kl_penalty.mean() if self.config.use_kl_penalty else torch.tensor(0.0, device=self.device)
        
        return policy_loss, entropy_loss, kl_loss
    
    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """
        æ‰§è¡Œä¸€æ­¥DAPOè®­ç»ƒ
        ğŸ”¥ å…³é”®ï¼šæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤ï¼ˆå¯èƒ½é€šè¿‡åŠ¨æ€é‡‡æ ·å¢åŠ ï¼‰
        """
        self.dynamic_sampling_stats["total_questions"] += len(batch_prompts)
        
        all_prompts = []
        all_responses = []
        all_response_lengths = []
        all_raw_rewards = []
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªpromptç”Ÿæˆgroup_sizeä¸ªå›å¤å¹¶åº”ç”¨åŠ¨æ€é‡‡æ ·
        for prompt in batch_prompts:
            # ç”Ÿæˆåˆå§‹å›å¤ç»„ï¼ˆåŒä¸€ä¸ªpromptç”Ÿæˆgroup_sizeæ¬¡ï¼‰
            responses = []
            response_lengths = []
            
            for _ in range(self.config.group_size):
                inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=self.config.max_response_length,
                        do_sample=True,
                        temperature=0.7,
                        pad_token_id=self.tokenizer.pad_token_id,
                        return_dict_in_generate=True,
                        output_scores=True
                    )
                
                generated_ids = outputs.sequences[0][inputs["input_ids"].shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                responses.append(response)
                response_lengths.append(len(generated_ids))
            
            # ä¸ºè¿™ç»„å›å¤åˆ›å»ºå¯¹åº”çš„promptåˆ—è¡¨
            prompts_repeated = [prompt] * len(responses)
            
            # è®¡ç®—å¥–åŠ±ï¼ŒåŒä¸€ä¸ªpromptçš„ä¸åŒanswerè®¡ç®—å¥–åŠ±
            raw_rewards = self.compute_rewards(prompts_repeated, responses)
            
            # ğŸ”¥ åº”ç”¨è½¯æƒ©ç½šï¼ˆå¦‚æœå¯ç”¨ï¼‰
            raw_rewards = self.apply_soft_overlong_punishment(raw_rewards, response_lengths)
            
            # ğŸ”¥ åŠ¨æ€é‡‡æ ·ï¼šå¦‚æœæ‰€æœ‰å¥–åŠ±ç›¸åŒï¼Œç»§ç»­é‡‡æ ·
            if self.config.use_dynamic_sampling:
                reward_std = raw_rewards.std().item()
                extra_samples = 0
                
                while reward_std < 1e-6 and len(responses) < self.config.max_dynamic_samples:
                    # é‡‡æ ·é¢å¤–çš„å›å¤
                    inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.policy_model.generate(
                            **inputs,
                            max_new_tokens=self.config.max_response_length,
                            do_sample=True,
                            temperature=0.7,
                            pad_token_id=self.tokenizer.pad_token_id,
                            return_dict_in_generate=True,
                            output_scores=True
                        )
                    
                    generated_ids = outputs.sequences[0][inputs["input_ids"].shape[1]:]
                    extra_response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                    extra_length = len(generated_ids)
                    
                    responses.append(extra_response)
                    response_lengths.append(extra_length)
                    prompts_repeated.append(prompt)
                    
                    # é‡æ–°è®¡ç®—å¥–åŠ±
                    extra_reward = self.compute_rewards([prompt], [extra_response])
                    extra_reward = self.apply_soft_overlong_punishment(extra_reward, [extra_length])
                    raw_rewards = torch.cat([raw_rewards, extra_reward])
                    
                    reward_std = raw_rewards.std().item()
                    extra_samples += 1
                
                if extra_samples > 0:
                    self.dynamic_sampling_stats["resampled_questions"] += 1
                    self.dynamic_sampling_stats["avg_extra_samples"] += extra_samples
            
            # ğŸ”¥ è¿‡æ»¤è¿‡é•¿å›å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.use_overlong_filtering:
                valid_indices = [i for i, length in enumerate(response_lengths) 
                               if length < self.config.max_response_length]
                if len(valid_indices) > 0:
                    responses = [responses[i] for i in valid_indices]
                    prompts_repeated = [prompts_repeated[i] for i in valid_indices]
                    raw_rewards = raw_rewards[valid_indices]
                    response_lengths = [response_lengths[i] for i in valid_indices]
            
            all_prompts.extend(prompts_repeated)
            all_responses.extend(responses)
            all_response_lengths.extend(response_lengths)
            all_raw_rewards.append(raw_rewards)
            """
            all_prompts:          List[str],    é•¿åº¦=8  ['q1','q1','q1','q1','q2','q2','q2','q2']
            all_responses:        List[str],    é•¿åº¦=8  ['a1','a2','a3','a4','b1','b2','b3','b4']
            all_response_lengths: List[int],    é•¿åº¦=8  [10, 15, 12, 20, 8, 18, 14, 11]
            all_raw_rewards:      List[Tensor], é•¿åº¦=2  [Tensor([...]), Tensor([...])]æ¯ä¸ªTensor shape=[4]
            """
        
        if len(all_responses) == 0:
            logger.warning("æ‰€æœ‰å›å¤éƒ½è¢«è¿‡æ»¤ï¼Œè·³è¿‡æ­¤æ­¥")
            return {}
        
        # åˆå¹¶æ‰€æœ‰å¥–åŠ± - å°†æ¯ä¸ªpromptç»„çš„å¥–åŠ±åˆå¹¶æˆä¸€ä¸ªå¼ é‡ï¼Œç”¨äºåç»­è®¡ç®—ç›¸å¯¹å¥–åŠ±å’Œä¼˜åŠ¿å‡½æ•°
        all_raw_rewards = torch.cat(all_raw_rewards)
        """
        # è¾“å‡º: Tensor, shape=[8]
        """
        
        # è®¡ç®—ç›¸å¯¹å¥–åŠ±
        relative_rewards, group_baselines = self.compute_relative_rewards(
            all_raw_rewards, 
            group_size=len(all_responses) // len(batch_prompts)
        )
        
        # æˆªæ–­æ•°æ®
        all_prompts = all_prompts[:len(relative_rewards)]
        all_responses = all_responses[:len(relative_rewards)]
        all_response_lengths = all_response_lengths[:len(relative_rewards)]
        
        # ğŸ”¥ è®¡ç®—logæ¦‚ç‡ï¼ˆåŒæ—¶è·å–æ ·æœ¬çº§åˆ«å’Œtokençº§åˆ«ï¼‰
        log_probs = self.compute_log_probs(all_prompts, all_responses, return_per_token=False)
        old_token_log_probs_list = None
        
        # ğŸ”¥ å¦‚æœä½¿ç”¨token-level lossï¼Œè·å–tokençº§åˆ«çš„logæ¦‚ç‡
        if self.config.use_token_level_loss:
            old_token_log_probs_list = self.compute_log_probs(
                all_prompts, all_responses, use_ref_model=False, return_per_token=True
            )
            # detachä»¥é¿å…æ¢¯åº¦ä¼ æ’­
            old_token_log_probs_list = [t.detach() for t in old_token_log_probs_list]
        
        # è®¡ç®—KLæ•£åº¦
        kl_penalty = self.compute_kl_penalty_simple(all_prompts, all_responses)
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages = self.compute_advantages(relative_rewards)
        
        # ä¿å­˜æ—§çš„logæ¦‚ç‡
        old_log_probs = log_probs.detach()
        
        # DAPOæ›´æ–°å¾ªç¯
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        
        for dapo_step in range(self.config.dapo_epochs):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_log_probs = self.compute_log_probs(all_prompts, all_responses, use_ref_model=False, return_per_token=False)
            
            # ğŸ”¥ å¦‚æœä½¿ç”¨token-level lossï¼Œè·å–å½“å‰ç­–ç•¥çš„tokençº§åˆ«logæ¦‚ç‡
            new_token_log_probs_list = None
            if self.config.use_token_level_loss:
                new_token_log_probs_list = self.compute_log_probs(
                    all_prompts, all_responses, use_ref_model=False, return_per_token=True
                )
            
            # è®¡ç®—æŸå¤±
            policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
                new_log_probs, old_log_probs, advantages, kl_penalty,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            # æ€»æŸå¤±
            total_loss = policy_loss + entropy_loss + kl_loss
            
            # æ›´æ–°ç­–ç•¥æ¨¡å‹
            self.policy_optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.policy_optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_kl_loss += kl_loss.item()
        
        # è®¡ç®—åŠ¨æ€é‡‡æ ·ç»Ÿè®¡
        resample_rate = (self.dynamic_sampling_stats["resampled_questions"] / 
                        max(1, self.dynamic_sampling_stats["total_questions"]))
        avg_extra = (self.dynamic_sampling_stats["avg_extra_samples"] / 
                    max(1, self.dynamic_sampling_stats["resampled_questions"]))
        
        return {
            "policy_loss": total_policy_loss / self.config.dapo_epochs,
            "entropy_loss": total_entropy_loss / self.config.dapo_epochs,
            "kl_loss": total_kl_loss / self.config.dapo_epochs,
            "raw_reward_mean": all_raw_rewards.mean().item(),
            "raw_reward_std": all_raw_rewards.std().item(),
            "relative_reward_mean": relative_rewards.mean().item(),
            "relative_reward_std": relative_rewards.std().item(),
            "advantage_mean": advantages.mean().item(),
            "avg_response_length": np.mean(all_response_lengths),
            "kl_divergence": kl_penalty.mean().item() if self.config.use_kl_penalty else 0.0,
            "dynamic_resample_rate": resample_rate,
            "avg_extra_samples": avg_extra
        }
    
    def train(self, train_dataset: DAPODataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹DAPOè®­ç»ƒ...")
        
        dataloader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True
        )
        
        global_step = 0
        
        for epoch in range(self.config.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{self.config.num_epochs}")
            
            epoch_metrics = []
            
            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch + 1}")):
                batch_prompts = batch["prompt"]
                
                metrics = self.train_step(batch_prompts)
                
                if metrics:  # å¦‚æœä¸ä¸ºç©º
                    epoch_metrics.append(metrics)
                    
                    if self.config.use_wandb:
                        wandb.log({
                            "step": global_step,
                            "epoch": epoch,
                            **metrics
                        })
                    
                    if global_step % self.config.save_steps == 0:
                        self.save_checkpoint(global_step)
                    
                    if batch_idx % 10 == 0:
                        logger.info(f"Step {global_step}: {metrics}")
                
                global_step += 1
            
            # è®¡ç®—epochå¹³å‡æŒ‡æ ‡
            if epoch_metrics:
                avg_metrics = {}
                for key in epoch_metrics[0].keys():
                    avg_metrics[f"epoch_{key}"] = np.mean([m[key] for m in epoch_metrics])
                
                logger.info(f"Epoch {epoch + 1} å¹³å‡æŒ‡æ ‡: {avg_metrics}")
                
                if self.config.use_wandb:
                    wandb.log(avg_metrics)
        
        logger.info("DAPOè®­ç»ƒå®Œæˆ!")
        self.save_checkpoint("final")
    
    def save_checkpoint(self, step):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        self.policy_model.save_pretrained(os.path.join(checkpoint_dir, "policy"))
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {checkpoint_dir}")

def load_training_data() -> List[str]:
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    logger.info("æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
    
    try:
        dataset = load_dataset("Anthropic/hh-rlhf", split="train[:1000]")
        
        prompts = []
        for item in dataset:
            conversation = item["chosen"]
            if conversation.startswith("Human:"):
                human_part = conversation.split("Assistant:")[0].replace("Human:", "").strip()
                if human_part:
                    prompts.append(human_part)
        
        logger.info(f"åŠ è½½äº† {len(prompts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return prompts
    
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½HHæ•°æ®é›†: {e}")
        logger.info("ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ")
        return [
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚",
            "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "è¯·è§£é‡Šç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ã€‚",
            "å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ"
        ] * 50

def main():
    """ä¸»å‡½æ•°"""
    config = DAPOConfig()
    
    os.makedirs(config.output_dir, exist_ok=True)
    
    prompts = load_training_data()
    
    tokenizer = AutoTokenizer.from_pretrained(config.policy_model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_dataset = DAPODataset(prompts, tokenizer, config.max_length)
    
    trainer = DAPOTrainer(config)
    
    trainer.train(train_dataset)

if __name__ == "__main__":
    main()
