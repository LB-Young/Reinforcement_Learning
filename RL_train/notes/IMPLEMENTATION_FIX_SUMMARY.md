# GRPO/DAPO å®ç°ä¿®å¤æ€»ç»“

## ğŸ”¥ å…³é”®é—®é¢˜

åŸå§‹å®ç°ä¸­å­˜åœ¨ä¸€ä¸ª**ä¸¥é‡ç¼ºé™·**ï¼šæ²¡æœ‰æ­£ç¡®å®ç° GRPO çš„æ ¸å¿ƒæœºåˆ¶â€”â€”**åŒä¸€ä¸ª query ç”Ÿæˆå¤šä¸ª answer**ã€‚

## é—®é¢˜è¯¦æƒ…

### é”™è¯¯å®ç°
```python
# grpo_train.py (ä¿®å¤å‰)
def generate_responses(self, prompts: List[str]):
    for prompt in prompts:  # æ¯ä¸ª prompt åªç”Ÿæˆä¸€æ¬¡
        response = self.model.generate(prompt)
        responses.append(response)
```

**ç»“æœ**ï¼š
- `batch_prompts = ['q1', 'q2']`
- `responses = ['a1', 'a2']`  âŒ åªæœ‰ 2 ä¸ªå›å¤
- æ— æ³•è®¡ç®—ç»„å†…ç›¸å¯¹å¥–åŠ±

### æ­£ç¡®å®ç°
```python
# grpo_train.py (ä¿®å¤å)
def generate_responses(self, prompts: List[str]):
    for prompt in prompts:
        for _ in range(self.config.group_size):  # ğŸ”¥ æ¯ä¸ª prompt ç”Ÿæˆå¤šæ¬¡
            response = self.model.generate(prompt, do_sample=True)
            all_responses.append(response)
            all_prompts_expanded.append(prompt)
```

**ç»“æœ**ï¼š
- `batch_prompts = ['q1', 'q2']`
- `responses = ['a1_1', 'a1_2', 'a1_3', 'a1_4', 'a2_1', 'a2_2', 'a2_3', 'a2_4']` âœ… 8 ä¸ªå›å¤
- `prompts_expanded = ['q1', 'q1', 'q1', 'q1', 'q2', 'q2', 'q2', 'q2']`
- å¯ä»¥æŒ‰ group_size=4 åˆ†ç»„è®¡ç®—ç›¸å¯¹å¥–åŠ±

## ä¿®å¤å†…å®¹

### 1. grpo_train.py

#### ä¿®æ”¹ `generate_responses` å‡½æ•°
```python
# ä¿®å¤å‰
def generate_responses(self, prompts: List[str]) -> Tuple[List[str], torch.Tensor]:
    for prompt in prompts:
        # åªç”Ÿæˆä¸€æ¬¡
        response = generate_one(prompt)
        responses.append(response)
    return responses, log_probs

# ä¿®å¤å
def generate_responses(self, prompts: List[str]) -> Tuple[List[str], torch.Tensor, List[str]]:
    for prompt in prompts:
        for _ in range(self.config.group_size):  # ğŸ”¥ ç”Ÿæˆ group_size æ¬¡
            response = generate_one(prompt)
            all_responses.append(response)
            all_prompts_expanded.append(prompt)
    return all_responses, log_probs, all_prompts_expanded
```

#### ä¿®æ”¹ `train_step` å‡½æ•°
```python
# ä¿®å¤å‰
responses, log_probs = self.generate_responses(batch_prompts)
raw_rewards = self.compute_rewards(batch_prompts, responses)

# ä¿®å¤å
responses, log_probs, prompts_expanded = self.generate_responses(batch_prompts)
raw_rewards = self.compute_rewards(prompts_expanded, responses)  # ğŸ”¥ ä½¿ç”¨æ‰©å±•çš„ prompts
```

### 2. dapo_train.py

#### é‡æ„ `train_step` å‡½æ•°
å°†ç”Ÿæˆé€»è¾‘ç›´æ¥æ•´åˆåˆ° `train_step` ä¸­ï¼Œå¹¶æ·»åŠ åŠ¨æ€é‡‡æ ·ï¼š

```python
def train_step(self, batch_prompts: List[str]):
    for prompt in batch_prompts:
        # ğŸ”¥ ä¸ºæ¯ä¸ª prompt ç”Ÿæˆ group_size ä¸ªå›å¤
        responses = []
        for _ in range(self.config.group_size):
            response = self.model.generate(prompt, do_sample=True)
            responses.append(response)
        
        # è®¡ç®—å¥–åŠ±
        rewards = self.compute_rewards([prompt] * len(responses), responses)
        
        # ğŸ”¥ åŠ¨æ€é‡‡æ ·ï¼šå¦‚æœæ‰€æœ‰å¥–åŠ±ç›¸åŒï¼Œç»§ç»­é‡‡æ ·
        if self.config.use_dynamic_sampling:
            while rewards.std() < 1e-6 and len(responses) < max_samples:
                extra_response = self.model.generate(prompt, do_sample=True)
                responses.append(extra_response)
                # é‡æ–°è®¡ç®—å¥–åŠ±
```

#### åˆ é™¤å†—ä½™å‡½æ•°
- åˆ é™¤äº†ç‹¬ç«‹çš„ `generate_responses` å‡½æ•°
- åˆ é™¤äº†ç‹¬ç«‹çš„ `dynamic_sampling` å‡½æ•°
- é€»è¾‘æ•´åˆåˆ° `train_step` ä¸­ï¼Œæ›´æ¸…æ™°

## ä¸ºä»€ä¹ˆè¿™ä¸ªä¿®å¤å¾ˆé‡è¦ï¼Ÿ

### 1. GRPO ç®—æ³•çš„æ ¸å¿ƒ
GRPO çš„"Group"æŒ‡çš„æ˜¯**åŒä¸€ä¸ªé—®é¢˜çš„å¤šä¸ªå›å¤**ï¼š
```
Group 1 (q1): [a1_1, a1_2, a1_3, a1_4]
  â†“
è®¡ç®—ç»„å†…å‡å€¼ä½œä¸º baseline
  â†“
ç›¸å¯¹å¥–åŠ± = reward - group_mean
```

### 2. æ›¿ä»£ Critic
```python
# PPO: éœ€è¦è®­ç»ƒ critic ç½‘ç»œ
advantage = reward - critic(state)

# GRPO: ç”¨ç»„å†…å‡å€¼æ›¿ä»£ critic
advantage = reward - mean(group_rewards)
```

### 3. å¯¹æ¯”å­¦ä¹ 
åŒä¸€ä¸ªé—®é¢˜çš„ä¸åŒå›å¤è´¨é‡ä¸åŒï¼š
- å¥½çš„å›å¤ï¼šadvantage > 0 â†’ å¢åŠ æ¦‚ç‡
- å·®çš„å›å¤ï¼šadvantage < 0 â†’ å‡å°‘æ¦‚ç‡

## æ€§èƒ½å½±å“

### è®¡ç®—æˆæœ¬
```
ä¿®å¤å‰ï¼šbatch_size=8 â†’ ç”Ÿæˆ 8 ä¸ªå›å¤
ä¿®å¤åï¼šbatch_size=8, group_size=4 â†’ ç”Ÿæˆ 32 ä¸ªå›å¤

è®¡ç®—æˆæœ¬å¢åŠ  4 å€ï¼Œä½†è¿™æ˜¯ç®—æ³•å¿…éœ€çš„
```

### è®­ç»ƒæ•ˆæœ
- âœ… æ­£ç¡®å®ç° GRPO ç®—æ³•
- âœ… æ›´ç¨³å®šçš„è®­ç»ƒä¿¡å·
- âœ… ç¬¦åˆè®ºæ–‡æè¿°
- âœ… å¯ä»¥æ­£ç¡®è®¡ç®—ç»„å†…ç›¸å¯¹å¥–åŠ±

## éªŒè¯æ–¹æ³•

### æ£€æŸ¥æ•°æ®å½¢çŠ¶
```python
batch_prompts = ['q1', 'q2']  # 2 ä¸ªé—®é¢˜
group_size = 4

responses, log_probs, prompts_expanded = generate_responses(batch_prompts)

assert len(responses) == 2 * 4  # åº”è¯¥æ˜¯ 8 ä¸ªå›å¤
assert len(prompts_expanded) == 8  # åº”è¯¥æ˜¯ 8 ä¸ª prompt
assert prompts_expanded[:4] == ['q1', 'q1', 'q1', 'q1']  # å‰ 4 ä¸ªæ˜¯ q1
assert prompts_expanded[4:] == ['q2', 'q2', 'q2', 'q2']  # å 4 ä¸ªæ˜¯ q2
```

### æ£€æŸ¥ç›¸å¯¹å¥–åŠ±
```python
rewards = [0.8, 0.6, 0.9, 0.7, 0.85, 0.75, 0.80, 0.90]
relative_rewards, baselines = compute_relative_rewards(rewards, group_size=4)

# Group 1 baseline: (0.8 + 0.6 + 0.9 + 0.7) / 4 = 0.75
assert baselines[0] == 0.75

# Group 2 baseline: (0.85 + 0.75 + 0.80 + 0.90) / 4 = 0.825
assert baselines[4] == 0.825
```

## æ–‡ä»¶æ¸…å•

### ä¿®æ”¹çš„æ–‡ä»¶
1. âœ… `grpo_train.py` - ä¿®å¤ GRPO å®ç°
2. âœ… `dapo_train.py` - ä¿®å¤ DAPO å®ç°

### æ–°å¢çš„æ–‡æ¡£
1. âœ… `grpo_rollout_explanation.md` - è¯¦ç»†è§£é‡Š rollout æœºåˆ¶
2. âœ… `IMPLEMENTATION_FIX_SUMMARY.md` - æœ¬æ–‡æ¡£

### å·²æœ‰çš„æ–‡æ¡£
1. `grpo_train_analysis.md` - GRPO ç®—æ³•åˆ†æ
2. `dapo_train_analysis.md` - DAPO vs GRPO å¯¹æ¯”
3. `ppo_train_analysis.md` - PPO ç®—æ³•åˆ†æ
4. `kl_divergence_explanation.md` - KL æ•£åº¦è§£é‡Š

## å…³é”®è¦ç‚¹

1. **GRPO/DAPO å¿…é¡»å¯¹åŒä¸€ä¸ª query ç”Ÿæˆå¤šä¸ª answer**
2. **group_size æ˜¯æŒ‡åŒä¸€ä¸ªé—®é¢˜ç”Ÿæˆå‡ ä¸ªå›å¤**
3. **å¿…é¡»å¯ç”¨ `do_sample=True` æ‰èƒ½ç”Ÿæˆä¸åŒçš„å›å¤**
4. **prompts éœ€è¦æ‰©å±•ä»¥åŒ¹é… responses çš„æ•°é‡**
5. **ç»„å†…ç›¸å¯¹å¥–åŠ±æ˜¯ GRPO çš„æ ¸å¿ƒåˆ›æ–°**

## ä¸‹ä¸€æ­¥

ç°åœ¨å®ç°å·²ç»æ­£ç¡®ï¼Œå¯ä»¥ï¼š
1. è¿è¡Œè®­ç»ƒè„šæœ¬éªŒè¯
2. è°ƒæ•´è¶…å‚æ•°ï¼ˆgroup_size, learning_rate ç­‰ï¼‰
3. åœ¨å®é™…ä»»åŠ¡ä¸Šæµ‹è¯•æ€§èƒ½
4. å¯¹æ¯” PPOã€GRPOã€DAPO çš„æ•ˆæœ

## å‚è€ƒèµ„æ–™

- DeepSeekMath è®ºæ–‡ï¼šGRPO ç®—æ³•åŸå§‹è®ºæ–‡
- DAPO è®ºæ–‡ï¼šåŠ¨æ€é‡‡æ ·æ”¹è¿›
- æœ¬æ¬¡ä¿®å¤ï¼šç¡®ä¿æ­£ç¡®å®ç°ç»„å†…é‡‡æ ·æœºåˆ¶
