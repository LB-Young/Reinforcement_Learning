# DAPO Token-Level Loss ä¿®å¤è¯´æ˜

## é—®é¢˜æè¿°

åŸå§‹å®ç°ä¸­çš„ token-level loss å®é™…ä¸Šæ˜¯ **sample-level çš„åŠ æƒ**ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„ token-level è®¡ç®—ï¼š

```python
# âŒ åŸå§‹å®ç°ï¼ˆé”™è¯¯ï¼‰
weights = torch.tensor(response_lengths, dtype=torch.float32, device=self.device)
weights = weights / weights.sum()
policy_loss = -(torch.min(surr1, surr2) * weights).sum()
```

è¿™ç§æ–¹å¼ï¼š
- åªæ˜¯ç»™é•¿å›å¤æ›´é«˜çš„æƒé‡
- ä»ç„¶æ˜¯åœ¨æ ·æœ¬çº§åˆ«è®¡ç®—æŸå¤±
- ä¼šå¯¼è‡´æ¨¡å‹å€¾å‘äºç”Ÿæˆæ›´é•¿çš„å›å¤

## ä¿®å¤æ–¹æ¡ˆ

### 1. ä¿®æ”¹ `compute_log_probs` æ–¹æ³•

æ·»åŠ  `return_per_token` å‚æ•°ï¼Œæ”¯æŒè¿”å›æ¯ä¸ª token çš„ log æ¦‚ç‡ï¼š

```python
def compute_log_probs(self, prompts, responses, use_ref_model=False, return_per_token=False):
    if return_per_token:
        return all_token_log_probs  # List[Tensor], æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªæ ·æœ¬çš„tokençº§åˆ«logæ¦‚ç‡
    else:
        return torch.stack(all_log_probs)  # Tensor [batch_size], æ ·æœ¬çº§åˆ«çš„æ€»logæ¦‚ç‡
```

### 2. æ–°å¢ `compute_policy_loss_token_level` æ–¹æ³•

çœŸæ­£å®ç° token-level çš„ PPO æŸå¤±è®¡ç®—ï¼š

```python
def compute_policy_loss_token_level(self, token_log_probs_list, old_token_log_probs_list, advantages):
    """å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—PPOæŸå¤±"""
    for i, (token_log_probs, old_token_log_probs) in enumerate(...):
        advantage = advantages[i]
        
        # å¯¹æ¯ä¸ªtokenè®¡ç®—ratio
        token_ratios = torch.exp(token_log_probs - old_token_log_probs)
        
        # å¯¹æ¯ä¸ªtokenåº”ç”¨Clip-Higher
        surr1 = token_ratios * advantage
        surr2 = torch.clamp(token_ratios, 1 - clip_low, 1 + clip_high) * advantage
        
        # å¯¹è¯¥æ ·æœ¬çš„æ‰€æœ‰tokenæ±‚å’Œ
        token_loss = -torch.min(surr1, surr2).sum()
        total_token_loss += token_loss
    
    return total_token_loss / total_tokens
```

### 3. æ›´æ–° `compute_policy_loss` æ–¹æ³•

æ”¯æŒ token-level å’Œ sample-level ä¸¤ç§æ¨¡å¼ï¼š

```python
def compute_policy_loss(self, log_probs, old_log_probs, advantages, kl_penalty,
                       token_log_probs_list=None, old_token_log_probs_list=None):
    if self.config.use_token_level_loss and token_log_probs_list is not None:
        # ğŸ”¥ Token-Level Loss
        policy_loss = self.compute_policy_loss_token_level(...)
    else:
        # Sample-Level Loss (GRPOæ–¹å¼)
        ratio = torch.exp(log_probs - old_log_probs)
        ...
```

### 4. æ›´æ–° `train_step` æ–¹æ³•

åœ¨è®­ç»ƒå¾ªç¯ä¸­è·å– token çº§åˆ«çš„ log æ¦‚ç‡ï¼š

```python
# è·å–æ—§ç­–ç•¥çš„tokençº§åˆ«logæ¦‚ç‡ï¼ˆç”¨äºè®¡ç®—ratioï¼‰
if self.config.use_token_level_loss:
    old_token_log_probs_list = self.compute_log_probs(
        all_prompts, all_responses, use_ref_model=False, return_per_token=True
    )
    old_token_log_probs_list = [t.detach() for t in old_token_log_probs_list]

# åœ¨DAPOæ›´æ–°å¾ªç¯ä¸­
for dapo_step in range(self.config.dapo_epochs):
    # è·å–å½“å‰ç­–ç•¥çš„tokençº§åˆ«logæ¦‚ç‡
    if self.config.use_token_level_loss:
        new_token_log_probs_list = self.compute_log_probs(
            all_prompts, all_responses, use_ref_model=False, return_per_token=True
        )
    
    # è®¡ç®—æŸå¤±
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        ..., 
        token_log_probs_list=new_token_log_probs_list,
        old_token_log_probs_list=old_token_log_probs_list
    )
```

## å…³é”®å·®å¼‚å¯¹æ¯”

| ç»´åº¦ | åŸå§‹å®ç°ï¼ˆé”™è¯¯ï¼‰ | ä¿®å¤åå®ç°ï¼ˆæ­£ç¡®ï¼‰ |
|------|-----------------|-------------------|
| æŸå¤±è®¡ç®—ç²’åº¦ | Sample-level | Token-level |
| ratio è®¡ç®— | æ•´ä¸ªåºåˆ—ä¸€ä¸ª ratio | æ¯ä¸ª token ä¸€ä¸ª ratio |
| clip åº”ç”¨ | å¯¹æ•´ä¸ªåºåˆ—åº”ç”¨ä¸€æ¬¡ | å¯¹æ¯ä¸ª token åˆ†åˆ«åº”ç”¨ |
| ä¼˜åŠ¿ä¼ æ’­ | æ ·æœ¬çº§åˆ«çš„ä¼˜åŠ¿ | æ¯ä¸ª token ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬ä¼˜åŠ¿ |
| é•¿åº¦å½±å“ | é•¿å›å¤æƒé‡æ›´é«˜ | é•¿å›å¤æœ‰æ›´å¤š token å‚ä¸ä¼˜åŒ– |

## DAPO çš„æ ¸å¿ƒä¼˜åŠ¿

é€šè¿‡ token-level lossï¼ŒDAPO èƒ½å¤Ÿï¼š

1. **æ›´ç²¾ç»†çš„ä¼˜åŒ–**ï¼šæ¯ä¸ª token éƒ½æœ‰è‡ªå·±çš„ ratio å’Œ clip æ“ä½œ
2. **æ›´å¥½çš„ä¿¡ç”¨åˆ†é…**ï¼šé•¿åºåˆ—ä¸­çš„æ¯ä¸ª token éƒ½èƒ½å¾—åˆ°ç‹¬ç«‹çš„æ¢¯åº¦ä¿¡å·
3. **é¿å…é•¿åº¦åå·®**ï¼šä¸ä¼šå› ä¸ºç®€å•çš„åŠ æƒè€Œåå‘é•¿å›å¤
4. **æ›´ç¨³å®šçš„è®­ç»ƒ**ï¼štoken çº§åˆ«çš„ clip æä¾›æ›´ç»†ç²’åº¦çš„çº¦æŸ

## ä½¿ç”¨æ–¹å¼

é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```python
config = DAPOConfig(
    use_token_level_loss=True,  # å¯ç”¨token-level loss
    clip_range_low=0.2,         # ä¸‹ç•Œè£å‰ª
    clip_range_high=0.28,       # ä¸Šç•Œè£å‰ªï¼ˆClip-Higherï¼‰
)
```

å¦‚æœè®¾ç½® `use_token_level_loss=False`ï¼Œåˆ™å›é€€åˆ° GRPO çš„ sample-level loss è®¡ç®—æ–¹å¼ã€‚
