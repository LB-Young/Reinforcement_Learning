#!/usr/bin/env python3
# author: YoungL
# date: 2026/01/18
# email: lby15356@gmail.com

"""
è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–å·¥å…·
ç”¨äºç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ã€å¥–åŠ±ã€ç†µç­‰æŒ‡æ ‡
"""

import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Optional
import os


def plot_training_metrics(
    metrics_history: Dict[str, List[float]],
    save_path: Optional[str] = None,
    figsize: tuple = (15, 10),
    title: str = "Training Metrics"
):
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„é¡¹æŒ‡æ ‡
    
    Args:
        metrics_history: æŒ‡æ ‡å†å²å­—å…¸ï¼Œä¾‹å¦‚:
            {
                'policy_loss': [0.5, 0.4, ...],
                'value_loss': [0.3, 0.2, ...],
                'reward': [1.2, 1.5, ...],
                'entropy': [0.8, 0.7, ...],
                ...
            }
        save_path: ä¿å­˜å›¾ç‰‡çš„è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™æ˜¾ç¤ºå›¾ç‰‡
        figsize: å›¾ç‰‡å¤§å°
        title: å›¾ç‰‡æ ‡é¢˜
    """
    num_metrics = len(metrics_history)
    if num_metrics == 0:
        print("æ²¡æœ‰æŒ‡æ ‡æ•°æ®å¯ç»˜åˆ¶")
        return
    
    # è®¡ç®—å­å›¾å¸ƒå±€
    cols = 2
    rows = (num_metrics + 1) // 2
    
    fig, axes = plt.subplots(rows, cols, figsize=figsize)
    fig.suptitle(title, fontsize=16, fontweight='bold')
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªå­å›¾ï¼Œaxes ä¸æ˜¯æ•°ç»„
    if num_metrics == 1:
        axes = np.array([axes])
    axes = axes.flatten()
    
    # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡
    for idx, (metric_name, values) in enumerate(metrics_history.items()):
        ax = axes[idx]
        steps = range(1, len(values) + 1)
        
        ax.plot(steps, values, linewidth=2, marker='o', markersize=4)
        ax.set_xlabel('Step', fontsize=10)
        ax.set_ylabel(metric_name.replace('_', ' ').title(), fontsize=10)
        ax.set_title(metric_name.replace('_', ' ').title(), fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(values) > 1:
            z = np.polyfit(steps, values, 1)
            p = np.poly1d(z)
            ax.plot(steps, p(steps), "--", alpha=0.5, linewidth=1.5, label='Trend')
            ax.legend()
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(num_metrics, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_loss_and_entropy(
    losses: List[float],
    entropies: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (12, 5)
):
    """
    ä¸“é—¨ç»˜åˆ¶æŸå¤±å’Œç†µçš„å˜åŒ–
    
    Args:
        losses: æŸå¤±å€¼åˆ—è¡¨
        entropies: ç†µå€¼åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    steps = range(1, len(losses) + 1)
    
    # ç»˜åˆ¶æŸå¤±
    ax1.plot(steps, losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    ax1.set_xlabel('Step', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶ç†µ
    ax2.plot(steps, entropies, linewidth=2, marker='s', markersize=4, color='#3498db')
    ax2.set_xlabel('Step', fontsize=12)
    ax2.set_ylabel('Entropy', fontsize=12)
    ax2.set_title('Policy Entropy', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_ppo_metrics_with_entropy(
    policy_losses: List[float],
    value_losses: List[float],
    rewards: List[float],
    advantages: List[float],
    entropies: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (15, 12)
):
    """
    ä¸“é—¨ä¸º PPO ç®—æ³•ç»˜åˆ¶æŒ‡æ ‡ï¼ˆåŒ…å«ç†µï¼‰
    
    Args:
        policy_losses: ç­–ç•¥æŸå¤±åˆ—è¡¨
        value_losses: ä»·å€¼æŸå¤±åˆ—è¡¨
        rewards: å¥–åŠ±åˆ—è¡¨
        advantages: ä¼˜åŠ¿å€¼åˆ—è¡¨
        entropies: ç†µåˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(3, 2, figsize=figsize)
    fig.suptitle('PPO Training Metrics (with Entropy)', fontsize=16, fontweight='bold')
    
    steps = range(1, len(policy_losses) + 1)
    
    # ç­–ç•¥æŸå¤±
    axes[0, 0].plot(steps, policy_losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Policy Loss')
    axes[0, 0].set_title('Policy Loss', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    
    # ä»·å€¼æŸå¤±
    axes[0, 1].plot(steps, value_losses, linewidth=2, marker='s', markersize=4, color='#9b59b6')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Value Loss')
    axes[0, 1].set_title('Value Loss', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # å¥–åŠ±
    axes[1, 0].plot(steps, rewards, linewidth=2, marker='^', markersize=4, color='#2ecc71')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Reward')
    axes[1, 0].set_title('Average Reward', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ä¼˜åŠ¿å€¼
    axes[1, 1].plot(steps, advantages, linewidth=2, marker='d', markersize=4, color='#3498db')
    axes[1, 1].set_xlabel('Step')
    axes[1, 1].set_ylabel('Advantage')
    axes[1, 1].set_title('Average Advantage', fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    # ç†µ
    axes[2, 0].plot(steps, entropies, linewidth=2, marker='*', markersize=6, color='#f39c12')
    axes[2, 0].set_xlabel('Step')
    axes[2, 0].set_ylabel('Entropy')
    axes[2, 0].set_title('Policy Entropy', fontweight='bold')
    axes[2, 0].grid(True, alpha=0.3)
    
    # éšè—æœ€åä¸€ä¸ªå­å›¾
    axes[2, 1].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_ppo_metrics(
    policy_losses: List[float],
    value_losses: List[float],
    rewards: List[float],
    advantages: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (15, 10)
):
    """
    ä¸“é—¨ä¸º PPO ç®—æ³•ç»˜åˆ¶æŒ‡æ ‡
    
    Args:
        policy_losses: ç­–ç•¥æŸå¤±åˆ—è¡¨
        value_losses: ä»·å€¼æŸå¤±åˆ—è¡¨
        rewards: å¥–åŠ±åˆ—è¡¨
        advantages: ä¼˜åŠ¿å€¼åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    fig.suptitle('PPO Training Metrics', fontsize=16, fontweight='bold')
    
    steps = range(1, len(policy_losses) + 1)
    
    # ç­–ç•¥æŸå¤±
    axes[0, 0].plot(steps, policy_losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Policy Loss')
    axes[0, 0].set_title('Policy Loss', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    
    # ä»·å€¼æŸå¤±
    axes[0, 1].plot(steps, value_losses, linewidth=2, marker='s', markersize=4, color='#9b59b6')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Value Loss')
    axes[0, 1].set_title('Value Loss', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # å¥–åŠ±
    axes[1, 0].plot(steps, rewards, linewidth=2, marker='^', markersize=4, color='#2ecc71')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Reward')
    axes[1, 0].set_title('Average Reward', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ä¼˜åŠ¿å€¼
    axes[1, 1].plot(steps, advantages, linewidth=2, marker='d', markersize=4, color='#3498db')
    axes[1, 1].set_xlabel('Step')
    axes[1, 1].set_ylabel('Advantage')
    axes[1, 1].set_title('Average Advantage', fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_grpo_metrics(losses, rewards, entropies, save_path):
    """ç»˜åˆ¶GRPOè®­ç»ƒæŒ‡æ ‡"""
    try:
        import matplotlib.pyplot as plt
        
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))
        
        # Lossæ›²çº¿
        ax1.plot(losses, 'b-', linewidth=2)
        ax1.set_title('Training Loss', fontsize=14)
        ax1.set_xlabel('Step')
        ax1.set_ylabel('Loss')
        ax1.grid(True, alpha=0.3)
        
        # Rewardæ›²çº¿
        ax2.plot(rewards, 'g-', linewidth=2)
        ax2.set_title('Average Reward', fontsize=14)
        ax2.set_xlabel('Step')
        ax2.set_ylabel('Reward')
        ax2.grid(True, alpha=0.3)
        
        # Entropyæ›²çº¿
        ax3.plot(entropies, 'r-', linewidth=2)
        ax3.set_title('Policy Entropy', fontsize=14)
        ax3.set_xlabel('Step')
        ax3.set_ylabel('Entropy')
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
    except ImportError:
        print("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡ç»˜å›¾")


def plot_grpo_metrics_advanced(
    losses: List[float],
    rewards: List[float],
    kl_divs: Optional[List[float]] = None,
    save_path: Optional[str] = None,
    figsize: tuple = (15, 5)
):
    """
    ä¸“é—¨ä¸º GRPO ç®—æ³•ç»˜åˆ¶æŒ‡æ ‡ï¼ˆé«˜çº§ç‰ˆæœ¬ï¼‰
    
    Args:
        losses: æŸå¤±åˆ—è¡¨
        rewards: å¥–åŠ±åˆ—è¡¨
        kl_divs: KL æ•£åº¦åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    num_plots = 3 if kl_divs else 2
    fig, axes = plt.subplots(1, num_plots, figsize=figsize)
    fig.suptitle('GRPO Training Metrics', fontsize=16, fontweight='bold')
    
    steps = range(1, len(losses) + 1)
    
    # æŸå¤±
    axes[0].plot(steps, losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    axes[0].set_xlabel('Step')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training Loss', fontweight='bold')
    axes[0].grid(True, alpha=0.3)
    
    # å¥–åŠ±
    axes[1].plot(steps, rewards, linewidth=2, marker='s', markersize=4, color='#2ecc71')
    axes[1].set_xlabel('Step')
    axes[1].set_ylabel('Reward')
    axes[1].set_title('Average Reward', fontweight='bold')
    axes[1].grid(True, alpha=0.3)
    
    # KL æ•£åº¦
    if kl_divs:
        axes[2].plot(steps, kl_divs, linewidth=2, marker='^', markersize=4, color='#f39c12')
        axes[2].set_xlabel('Step')
        axes[2].set_ylabel('KL Divergence')
        axes[2].set_title('KL Divergence', fontweight='bold')
        axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    example_metrics = {
        'policy_loss': [0.5, 0.45, 0.4, 0.38, 0.35, 0.33, 0.31, 0.30],
        'value_loss': [0.3, 0.28, 0.25, 0.23, 0.21, 0.20, 0.19, 0.18],
        'reward': [1.0, 1.2, 1.5, 1.7, 1.9, 2.1, 2.3, 2.5],
        'entropy': [0.8, 0.75, 0.7, 0.68, 0.65, 0.63, 0.61, 0.60]
    }
    
    # ç»˜åˆ¶æ‰€æœ‰æŒ‡æ ‡
    plot_training_metrics(example_metrics, save_path="example_metrics.png")
    
    # ç»˜åˆ¶ PPO æŒ‡æ ‡
    plot_ppo_metrics(
        policy_losses=example_metrics['policy_loss'],
        value_losses=example_metrics['value_loss'],
        rewards=example_metrics['reward'],
        advantages=[0.1, 0.15, 0.2, 0.18, 0.16, 0.14, 0.12, 0.10],
        save_path="ppo_metrics.png"
    )

def plot_dapo_metrics(
    policy_losses: List[float],
    entropy_losses: List[float],
    rewards: List[float],
    entropies: List[float],
    dynamic_resample_rates: List[float],
    avg_response_lengths: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (18, 12)
):
    """
    ä¸“é—¨ä¸º DAPO ç®—æ³•ç»˜åˆ¶æŒ‡æ ‡
    
    Args:
        policy_losses: ç­–ç•¥æŸå¤±åˆ—è¡¨
        entropy_losses: ç†µæŸå¤±åˆ—è¡¨
        rewards: å¥–åŠ±åˆ—è¡¨
        entropies: ç†µåˆ—è¡¨
        dynamic_resample_rates: åŠ¨æ€é‡é‡‡æ ·ç‡åˆ—è¡¨
        avg_response_lengths: å¹³å‡å›å¤é•¿åº¦åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(3, 2, figsize=figsize)
    fig.suptitle('DAPO Training Metrics', fontsize=16, fontweight='bold')
    
    steps = range(1, len(policy_losses) + 1)
    
    # ç­–ç•¥æŸå¤±
    axes[0, 0].plot(steps, policy_losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Policy Loss')
    axes[0, 0].set_title('Policy Loss (Token-Level)', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    
    # ç†µæŸå¤±
    axes[0, 1].plot(steps, entropy_losses, linewidth=2, marker='s', markersize=4, color='#9b59b6')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Entropy Loss')
    axes[0, 1].set_title('Entropy Loss', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # å¥–åŠ±
    axes[1, 0].plot(steps, rewards, linewidth=2, marker='^', markersize=4, color='#2ecc71')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Reward')
    axes[1, 0].set_title('Average Reward', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ç†µ
    axes[1, 1].plot(steps, entropies, linewidth=2, marker='d', markersize=4, color='#f39c12')
    axes[1, 1].set_xlabel('Step')
    axes[1, 1].set_ylabel('Entropy')
    axes[1, 1].set_title('Policy Entropy', fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    
    # ğŸ”¥ åŠ¨æ€é‡é‡‡æ ·ç‡ (DAPOç‰¹æœ‰)
    axes[2, 0].plot(steps, dynamic_resample_rates, linewidth=2, marker='*', markersize=6, color='#3498db')
    axes[2, 0].set_xlabel('Step')
    axes[2, 0].set_ylabel('Dynamic Resample Rate')
    axes[2, 0].set_title('Dynamic Resample Rate', fontweight='bold')
    axes[2, 0].grid(True, alpha=0.3)
    axes[2, 0].set_ylim(0, 1)
    
    # ğŸ”¥ å¹³å‡å›å¤é•¿åº¦ (DAPOç‰¹æœ‰)
    axes[2, 1].plot(steps, avg_response_lengths, linewidth=2, marker='h', markersize=4, color='#e67e22')
    axes[2, 1].set_xlabel('Step')
    axes[2, 1].set_ylabel('Avg Response Length')
    axes[2, 1].set_title('Average Response Length', fontweight='bold')
    axes[2, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_dapo_vs_grpo_comparison(
    dapo_losses: List[float],
    grpo_losses: List[float],
    dapo_rewards: List[float],
    grpo_rewards: List[float],
    dapo_entropies: List[float],
    grpo_entropies: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (15, 10)
):
    """
    ç»˜åˆ¶ DAPO ä¸ GRPO çš„å¯¹æ¯”å›¾è¡¨
    
    Args:
        dapo_losses: DAPOæŸå¤±åˆ—è¡¨
        grpo_losses: GRPOæŸå¤±åˆ—è¡¨
        dapo_rewards: DAPOå¥–åŠ±åˆ—è¡¨
        grpo_rewards: GRPOå¥–åŠ±åˆ—è¡¨
        dapo_entropies: DAPOç†µåˆ—è¡¨
        grpo_entropies: GRPOç†µåˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    fig.suptitle('DAPO vs GRPO Comparison', fontsize=16, fontweight='bold')
    
    dapo_steps = range(1, len(dapo_losses) + 1)
    grpo_steps = range(1, len(grpo_losses) + 1)
    
    # æŸå¤±å¯¹æ¯”
    axes[0, 0].plot(dapo_steps, dapo_losses, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='DAPO (Token-Level)')
    axes[0, 0].plot(grpo_steps, grpo_losses, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO (Sample-Level)')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Loss Comparison', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].legend()
    
    # å¥–åŠ±å¯¹æ¯”
    axes[0, 1].plot(dapo_steps, dapo_rewards, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='DAPO')
    axes[0, 1].plot(grpo_steps, grpo_rewards, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].set_title('Average Reward Comparison', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].legend()
    
    # ç†µå¯¹æ¯”
    axes[1, 0].plot(dapo_steps, dapo_entropies, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='DAPO (Clip-Higher)')
    axes[1, 0].plot(grpo_steps, grpo_entropies, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO (Symmetric Clip)')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Entropy')
    axes[1, 0].set_title('Policy Entropy Comparison', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    
    # ç®—æ³•ç‰¹æ€§å¯¹æ¯”ï¼ˆæ–‡æœ¬è¯´æ˜ï¼‰
    axes[1, 1].axis('off')
    comparison_text = """
DAPO vs GRPO Key Differences:

ğŸ”¥ DAPO Improvements:
â€¢ Clip-Higher: [0.8, 1.28] vs [0.8, 1.2]
â€¢ Token-Level Loss vs Sample-Level
â€¢ Dynamic Sampling for training signal
â€¢ No KL Penalty (KL_COEF = 0.0)
â€¢ Overlong Response Filtering

ğŸ“Š Expected Benefits:
â€¢ Prevents entropy collapse
â€¢ Better long-chain reasoning
â€¢ Faster convergence (50% steps)
â€¢ Higher final performance
    """
    axes[1, 1].text(0.05, 0.95, comparison_text, transform=axes[1, 1].transAxes,
                    fontsize=11, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()
def plot_gspo_metrics(
    policy_losses: List[float],
    entropy_losses: List[float],
    kl_losses: List[float],
    rewards: List[float],
    relative_advantages: List[float],
    kl_divergences: List[float],
    kl_coefs: List[float],
    avg_response_lengths: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (20, 15)
):
    """
    ä¸“é—¨ä¸º GSPO ç®—æ³•ç»˜åˆ¶æŒ‡æ ‡
    
    Args:
        policy_losses: ç­–ç•¥æŸå¤±åˆ—è¡¨
        entropy_losses: ç†µæŸå¤±åˆ—è¡¨
        kl_losses: KLæŸå¤±åˆ—è¡¨
        rewards: å¥–åŠ±åˆ—è¡¨
        relative_advantages: ç›¸å¯¹ä¼˜åŠ¿åˆ—è¡¨
        kl_divergences: KLæ•£åº¦åˆ—è¡¨
        kl_coefs: KLç³»æ•°åˆ—è¡¨
        avg_response_lengths: å¹³å‡å›å¤é•¿åº¦åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(4, 2, figsize=figsize)
    fig.suptitle('GSPO Training Metrics', fontsize=16, fontweight='bold')
    
    steps = range(1, len(policy_losses) + 1)
    
    # ç­–ç•¥æŸå¤±
    axes[0, 0].plot(steps, policy_losses, linewidth=2, marker='o', markersize=4, color='#e74c3c')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Policy Loss')
    axes[0, 0].set_title('Policy Loss (Sequence-Level)', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    
    # ç†µæŸå¤±
    axes[0, 1].plot(steps, entropy_losses, linewidth=2, marker='s', markersize=4, color='#9b59b6')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Entropy Loss')
    axes[0, 1].set_title('Entropy Loss', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    
    # KLæŸå¤±
    axes[1, 0].plot(steps, kl_losses, linewidth=2, marker='^', markersize=4, color='#f39c12')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('KL Loss')
    axes[1, 0].set_title('KL Divergence Loss', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # å¥–åŠ±
    axes[1, 1].plot(steps, rewards, linewidth=2, marker='d', markersize=4, color='#2ecc71')
    axes[1, 1].set_xlabel('Step')
    axes[1, 1].set_ylabel('Reward')
    axes[1, 1].set_title('Average Reward', fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    
    # ğŸ”¥ ç›¸å¯¹ä¼˜åŠ¿ (GSPOç‰¹æœ‰)
    axes[2, 0].plot(steps, relative_advantages, linewidth=2, marker='*', markersize=6, color='#3498db')
    axes[2, 0].set_xlabel('Step')
    axes[2, 0].set_ylabel('Relative Advantage')
    axes[2, 0].set_title('Relative Advantage (Group-based)', fontweight='bold')
    axes[2, 0].grid(True, alpha=0.3)
    axes[2, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    # ğŸ”¥ KLæ•£åº¦ (GSPOç‰¹æœ‰)
    axes[2, 1].plot(steps, kl_divergences, linewidth=2, marker='h', markersize=4, color='#e67e22')
    axes[2, 1].set_xlabel('Step')
    axes[2, 1].set_ylabel('KL Divergence')
    axes[2, 1].set_title('KL Divergence from Reference', fontweight='bold')
    axes[2, 1].grid(True, alpha=0.3)
    
    # ğŸ”¥ è‡ªé€‚åº”KLç³»æ•° (GSPOç‰¹æœ‰)
    axes[3, 0].plot(steps, kl_coefs, linewidth=2, marker='v', markersize=4, color='#8e44ad')
    axes[3, 0].set_xlabel('Step')
    axes[3, 0].set_ylabel('KL Coefficient')
    axes[3, 0].set_title('Adaptive KL Coefficient', fontweight='bold')
    axes[3, 0].grid(True, alpha=0.3)
    
    # å¹³å‡å›å¤é•¿åº¦
    axes[3, 1].plot(steps, avg_response_lengths, linewidth=2, marker='p', markersize=4, color='#16a085')
    axes[3, 1].set_xlabel('Step')
    axes[3, 1].set_ylabel('Avg Response Length')
    axes[3, 1].set_title('Average Response Length', fontweight='bold')
    axes[3, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()


def plot_gspo_vs_grpo_comparison(
    gspo_losses: List[float],
    grpo_losses: List[float],
    gspo_rewards: List[float],
    grpo_rewards: List[float],
    gspo_relative_advantages: List[float],
    grpo_relative_rewards: List[float],
    save_path: Optional[str] = None,
    figsize: tuple = (15, 10)
):
    """
    ç»˜åˆ¶ GSPO ä¸ GRPO çš„å¯¹æ¯”å›¾è¡¨
    
    Args:
        gspo_losses: GSPOæŸå¤±åˆ—è¡¨
        grpo_losses: GRPOæŸå¤±åˆ—è¡¨
        gspo_rewards: GSPOå¥–åŠ±åˆ—è¡¨
        grpo_rewards: GRPOå¥–åŠ±åˆ—è¡¨
        gspo_relative_advantages: GSPOç›¸å¯¹ä¼˜åŠ¿åˆ—è¡¨
        grpo_relative_rewards: GRPOç›¸å¯¹å¥–åŠ±åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        figsize: å›¾ç‰‡å¤§å°
    """
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    fig.suptitle('GSPO vs GRPO Comparison', fontsize=16, fontweight='bold')
    
    gspo_steps = range(1, len(gspo_losses) + 1)
    grpo_steps = range(1, len(grpo_losses) + 1)
    
    # æŸå¤±å¯¹æ¯”
    axes[0, 0].plot(gspo_steps, gspo_losses, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='GSPO (Group Sequence)')
    axes[0, 0].plot(grpo_steps, grpo_losses, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO (Group Relative)')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training Loss Comparison', fontweight='bold')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].legend()
    
    # å¥–åŠ±å¯¹æ¯”
    axes[0, 1].plot(gspo_steps, gspo_rewards, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='GSPO')
    axes[0, 1].plot(grpo_steps, grpo_rewards, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('Reward')
    axes[0, 1].set_title('Average Reward Comparison', fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].legend()
    
    # ç›¸å¯¹ä¼˜åŠ¿/å¥–åŠ±å¯¹æ¯”
    axes[1, 0].plot(gspo_steps, gspo_relative_advantages, linewidth=2, marker='o', markersize=4, 
                    color='#e74c3c', label='GSPO (Relative Advantage)')
    axes[1, 0].plot(grpo_steps, grpo_relative_rewards, linewidth=2, marker='s', markersize=4, 
                    color='#3498db', label='GRPO (Relative Reward)')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Relative Value')
    axes[1, 0].set_title('Relative Advantage/Reward Comparison', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].legend()
    axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)
    
    # ç®—æ³•ç‰¹æ€§å¯¹æ¯”ï¼ˆæ–‡æœ¬è¯´æ˜ï¼‰
    axes[1, 1].axis('off')
    comparison_text = """
GSPO vs GRPO Key Differences:

ğŸ”¥ GSPO Features:
â€¢ Group Sampling: Multi-response per prompt
â€¢ Sequence-Level Rewards: Full sequence evaluation
â€¢ Relative Advantage: Group-based baseline
â€¢ Adaptive KL: Dynamic KL coefficient adjustment
â€¢ Flexible Optimization: Sequence/Token level

ğŸ“Š GRPO Features:
â€¢ Group Relative Policy: Relative rewards
â€¢ Token-Level Loss: Fine-grained optimization
â€¢ Fixed KL: Static KL coefficient
â€¢ Simpler Architecture: Fewer hyperparameters

ğŸ¯ Use Cases:
â€¢ GSPO: Complex reasoning, diverse generation
â€¢ GRPO: Long text generation, efficiency focus
    """
    axes[1, 1].text(0.05, 0.95, comparison_text, transform=axes[1, 1].transAxes,
                    fontsize=10, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()
    
    plt.close()