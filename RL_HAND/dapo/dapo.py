#!/usr/bin/env python3
# author: YoungL
# date: 2026/01/19
# email: lby15356@gmail.com

"""
æ ‡å‡†ç‰ˆ DAPO è®­ç»ƒè„šæœ¬
DAPO (Decoupled Clip and Dynamic sAmpling Policy Optimization) æ˜¯GRPOçš„æ”¹è¿›ç‰ˆæœ¬
ä¸»è¦æ”¹è¿›ï¼š
1. Clip-Higher: éå¯¹ç§°è£å‰ªï¼Œé˜²æ­¢ç†µå´©æºƒ
2. Token-Level Loss: æŒ‰tokençº§åˆ«è®¡ç®—æŸå¤±ï¼Œé¿å…çŸ­å›å¤åå¥½
3. Dynamic Sampling: åŠ¨æ€é‡‡æ ·ç¡®ä¿è®­ç»ƒä¿¡å·
4. ç§»é™¤KLæƒ©ç½š: å…è®¸ç­–ç•¥æ›´è‡ªç”±æ¢ç´¢

æ³¨ï¼š
1. ç”±äºé¡¹ç›®ç€é‡äºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®ç°å’Œå„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•å¼‚åŒçš„å¯¹æ¯”ï¼Œæ•…åªæ¶‰åŠæ•´ä½“æµç¨‹ï¼Œä¸åŒ…å«å·¥ç¨‹å±‚é¢çš„è°ƒåº¦ä¼˜åŒ–ï¼›
2. ç”±äºæœ¬é¡¹ç›®å®éªŒç¯å¢ƒä»…åŒ…å«5060ti-16G * 2ï¼Œæ‰€ä»¥å°†policyæ¨¡å‹æ”¾åœ¨0å·gpuï¼Œreferenceã€rewardä¸¤ä¸ªæ¨¡å‹æ”¾åœ¨1å·gpuï¼›
"""

import os
import shutil
import sys
import gc
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from typing import List, Tuple, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥ç»˜å›¾å‡½æ•°
from utils.plot_metrics import plot_dapo_metrics

# ==================== é…ç½® ====================
POLICY_MODEL = r"E:\models\Qwen\Qwen3-0___6B"
REWARD_MODEL = r"E:\models\reward-model-deberta-v3-large-v2"
train_datasets = [
    {
        "path":r"E:\datasets\gsm8k\main\train-00000-of-00001.parquet",
        "type":"parquet",
        "input":"question",
        "output":"answer"
    }
]

BATCH_SIZE = 2  # DAPOéœ€è¦æ›´å¤šæ˜¾å­˜ï¼Œé€‚å½“è°ƒå°
LEARNING_RATE = 1e-6
DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
NUM_EPOCHS = 1
GROUP_SIZE = 4
DAPO_EPOCHS = 4

# DAPOç‰¹æœ‰å‚æ•°
CLIP_RANGE_LOW = 0.2    # ä¸‹ç•Œè£å‰ªèŒƒå›´
CLIP_RANGE_HIGH = 0.28  # ğŸ”¥ Clip-Higher: ä¸Šç•Œè£å‰ªèŒƒå›´æ›´å¤§
KL_COEF = 0.0           # ğŸ”¥ DAPOç§»é™¤KLæƒ©ç½š
ENTROPY_COEF = 0.01
USE_DYNAMIC_SAMPLING = True     # ğŸ”¥ åŠ¨æ€é‡‡æ ·
MAX_DYNAMIC_SAMPLES = 8         # åŠ¨æ€é‡‡æ ·æœ€å¤§æ ·æœ¬æ•°
USE_TOKEN_LEVEL_LOSS = True     # ğŸ”¥ Tokençº§åˆ«æŸå¤±
USE_OVERLONG_FILTERING = True   # ğŸ”¥ è¿‡æ»¤è¿‡é•¿å›å¤
MAX_RESPONSE_LENGTH = 256       # æœ€å¤§å›å¤é•¿åº¦

OUTPUT_DIR = r"E:\projects\train_related\trained_model\rl_exprement\grpo_output\dapo_gsm8k_v1"

# ==================== æ•°æ®é›† ====================
class DAPODataset(Dataset):
    def __init__(self, prompts: List[str]):
        self.prompts = prompts
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        return {"prompt": self.prompts[idx]}

# ==================== DAPOè®­ç»ƒå™¨ ====================
class DAPOTrainer:
    def __init__(self):
        num_gpus = torch.cuda.device_count()
        self.device_policy = torch.device("cuda:0")
        self.device_ref = torch.device("cuda:1") if num_gpus > 1 else torch.device("cuda:0")
        self.device_reward = torch.device("cuda:1") if num_gpus > 1 else torch.device("cuda:0")
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®°å½•
        self.metrics_history = {
            'policy_loss': [],
            'entropy_loss': [],
            'kl_loss': [],
            'reward': [],
            'entropy': [],
            'dynamic_resample_rate': [],
            'avg_response_length': []
        }
        
        # åŠ¨æ€é‡‡æ ·ç»Ÿè®¡
        self.dynamic_sampling_stats = {
            "total_questions": 0,
            "resampled_questions": 0,
            "avg_extra_samples": 0.0
        }
        
        self.tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL, torch_dtype=DTYPE, device_map={"": self.device_policy})
        self.ref_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL, torch_dtype=DTYPE, device_map={"": self.device_ref})
        self.ref_model.eval()
        
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL, torch_dtype=DTYPE, device_map={"": self.device_reward})
        self.reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)
        
        self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=LEARNING_RATE)

    def generate_responses(self, prompts: List[str]) -> Tuple[List[str], List[str], List[int]]:
        """ç”Ÿæˆå›å¤ï¼Œè¿”å›å›å¤ã€å¯¹åº”çš„promptå’Œå›å¤é•¿åº¦"""
        self.policy_model.eval()
        all_responses, all_prompts, all_lengths = [], [], []
        
        for prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device_policy)
            with torch.no_grad():
                outputs = self.policy_model.generate(
                    **inputs,
                    max_new_tokens=MAX_RESPONSE_LENGTH,
                    do_sample=True,
                    temperature=0.7,
                    num_return_sequences=GROUP_SIZE,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # ä»…æå–ç”Ÿæˆçš„ Response éƒ¨åˆ†
            gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
            responses = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
            lengths = [len(gen_id) for gen_id in gen_ids]
            
            all_responses.extend(responses)
            all_prompts.extend([prompt] * GROUP_SIZE)
            all_lengths.extend(lengths)
            
            # é‡Šæ”¾å½“å‰å¾ªç¯çš„å¼ é‡
            del inputs, outputs, gen_ids
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        return all_responses, all_prompts, all_lengths

    def apply_dynamic_sampling(self, prompt: str, initial_responses: List[str], 
                             initial_rewards: torch.Tensor, initial_lengths: List[int]) -> Tuple[List[str], torch.Tensor, List[int]]:
        """ğŸ”¥ DAPOç‰¹æ€§ï¼šåŠ¨æ€é‡‡æ ·ï¼Œå¦‚æœæ‰€æœ‰å¥–åŠ±ç›¸åŒåˆ™ç»§ç»­é‡‡æ ·"""
        if not USE_DYNAMIC_SAMPLING:
            return initial_responses, initial_rewards, initial_lengths
        
        responses = initial_responses.copy()
        rewards = initial_rewards.clone()
        lengths = initial_lengths.copy()
        
        reward_std = rewards.std().item()
        extra_samples = 0
        
        # å¦‚æœæ ‡å‡†å·®å¤ªå°ï¼ˆå¥–åŠ±éƒ½ç›¸åŒï¼‰ï¼Œç»§ç»­é‡‡æ ·
        while reward_std < 1e-6 and len(responses) < MAX_DYNAMIC_SAMPLES:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device_policy)
            with torch.no_grad():
                outputs = self.policy_model.generate(
                    **inputs,
                    max_new_tokens=MAX_RESPONSE_LENGTH,
                    do_sample=True,
                    temperature=0.7,
                    num_return_sequences=1,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
            extra_response = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
            extra_length = len(gen_ids[0])
            
            # è®¡ç®—é¢å¤–æ ·æœ¬çš„å¥–åŠ±
            extra_reward = self.compute_rewards([prompt], [extra_response])
            
            responses.append(extra_response)
            rewards = torch.cat([rewards, extra_reward])
            lengths.append(extra_length)
            
            reward_std = rewards.std().item()
            extra_samples += 1
            
            # é‡Šæ”¾å¼ é‡
            del inputs, outputs, gen_ids
        
        if extra_samples > 0:
            self.dynamic_sampling_stats["resampled_questions"] += 1
            self.dynamic_sampling_stats["avg_extra_samples"] += extra_samples
        
        torch.cuda.empty_cache()
        return responses, rewards, lengths

    def apply_overlong_filtering(self, prompts: List[str], responses: List[str], 
                               rewards: torch.Tensor, lengths: List[int]) -> Tuple[List[str], List[str], torch.Tensor, List[int]]:
        """ğŸ”¥ DAPOç‰¹æ€§ï¼šè¿‡æ»¤è¿‡é•¿å›å¤"""
        if not USE_OVERLONG_FILTERING:
            return prompts, responses, rewards, lengths
        
        valid_indices = [i for i, length in enumerate(lengths) if length < MAX_RESPONSE_LENGTH]
        
        if len(valid_indices) == 0:
            return prompts, responses, rewards, lengths
        
        filtered_prompts = [prompts[i] for i in valid_indices]
        filtered_responses = [responses[i] for i in valid_indices]
        filtered_rewards = rewards[valid_indices]
        filtered_lengths = [lengths[i] for i in valid_indices]
        
        return filtered_prompts, filtered_responses, filtered_rewards, filtered_lengths

    def get_token_log_probs(self, model, prompts: List[str], responses: List[str], device, 
                          return_per_token: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[torch.Tensor]]:
        """è·å– Token çº§åˆ«çš„ log_probs å¹¶è¿”å› Maskã€Entropy å’Œæ¯ä¸ªæ ·æœ¬çš„tokençº§åˆ«log_probs"""
        full_texts = [p + r for p, r in zip(prompts, responses)]
        inputs = self.tokenizer(full_texts, return_tensors="pt", padding=True, truncation=True).to(device)
        
        # è®¡ç®— Prompt çš„é•¿åº¦
        prompt_lens = [len(self.tokenizer.encode(p, add_special_tokens=False)) for p in prompts]
        
        outputs = model(**inputs)
        logits = outputs.logits[:, :-1, :] # Shift å¯¹é½
        labels = inputs["input_ids"][:, 1:]
        
        log_probs = F.log_softmax(logits, dim=-1)
        token_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)
        
        # è®¡ç®—ç†µå€¼ (ä»…åœ¨responseåŒºåŸŸ)
        probs = F.softmax(logits, dim=-1)
        entropy = -(probs * log_probs).sum(dim=-1)  # [batch_size, seq_len]
        
        # åˆ¶ä½œ Mask: 1 ä»…åœ¨ Response åŒºåŸŸä¸”é Padding å¤„
        mask = torch.zeros_like(labels, dtype=torch.bool)
        for i, p_len in enumerate(prompt_lens):
            mask[i, p_len:] = (labels[i, p_len:] != self.tokenizer.pad_token_id)
        
        # ğŸ”¥ å¦‚æœéœ€è¦è¿”å›æ¯ä¸ªæ ·æœ¬çš„tokençº§åˆ«log_probsï¼ˆç”¨äºtoken-level lossï¼‰
        per_token_log_probs = []
        if return_per_token:
            for i, p_len in enumerate(prompt_lens):
                # æå–è¯¥æ ·æœ¬responseéƒ¨åˆ†çš„token log_probs
                response_mask = mask[i, p_len:]
                if response_mask.sum() > 0:
                    sample_token_log_probs = token_log_probs[i, p_len:][response_mask]
                    per_token_log_probs.append(sample_token_log_probs)
                else:
                    per_token_log_probs.append(torch.tensor([], device=device))
        
        # é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
        del inputs, outputs, logits, probs, labels
        
        # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜
        if device.type == 'cuda':
            torch.cuda.empty_cache()

        return token_log_probs, mask, entropy, per_token_log_probs

    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """è®¡ç®—å¥–åŠ±"""
        rewards = []
        for p, r in zip(prompts, responses):
            inputs = self.reward_tokenizer(p + r, return_tensors="pt", truncation=True).to(self.device_reward)
            with torch.no_grad():
                reward = self.reward_model(**inputs).logits[0, 0]
                rewards.append(reward)
            # é‡Šæ”¾å½“å‰å¾ªç¯çš„å¼ é‡
            del inputs
        
        result = torch.stack(rewards).to(self.device_policy)
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        return result

    def compute_relative_rewards(self, rewards: torch.Tensor, group_sizes: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—ç›¸å¯¹å¥–åŠ±ï¼ˆæ”¯æŒä¸åŒç»„å¤§å°ï¼‰"""
        relative_rewards = []
        group_baselines = []
        
        start_idx = 0
        for group_size in group_sizes:
            end_idx = start_idx + group_size
            group_rewards = rewards[start_idx:end_idx]
            
            group_mean = group_rewards.mean()
            group_std = group_rewards.std() + 1e-8
            
            # ç»„å†…æ ‡å‡†åŒ–
            group_relative = (group_rewards - group_mean) / group_std
            
            relative_rewards.append(group_relative)
            group_baselines.extend([group_mean] * group_size)
            
            start_idx = end_idx
        
        relative_rewards = torch.cat(relative_rewards)
        group_baselines = torch.tensor(group_baselines, device=rewards.device)
        
        return relative_rewards, group_baselines

    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor], 
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ DAPO Token-Level Loss: å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—PPOæŸå¤±"""
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            if len(token_log_probs) == 0:  # è·³è¿‡ç©ºçš„tokenåºåˆ—
                continue
                
            advantage = advantages[i]  # è¯¥æ ·æœ¬çš„ä¼˜åŠ¿å€¼
            
            # è®¡ç®—æ¯ä¸ªtokençš„æ¦‚ç‡æ¯”ç‡
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)  # [num_tokens]
            
            # ğŸ”¥ DAPO Clip-Higher: éå¯¹ç§°è£å‰ª
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios, 
                               1 - CLIP_RANGE_LOW, 
                               1 + CLIP_RANGE_HIGH) * advantage
            
            # å¯¹æ¯ä¸ªtokenå–æœ€å°å€¼ï¼Œç„¶åæ±‚å’Œ
            token_loss = -torch.min(surr1, surr2).sum()  # å¯¹è¯¥æ ·æœ¬çš„æ‰€æœ‰tokenæ±‚å’Œ
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        # è¿”å›å¹³å‡tokenæŸå¤±
        return total_token_loss / max(total_tokens, 1)

    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor, 
                          advantages: torch.Tensor, mask: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> torch.Tensor:
        """ğŸ”¥ è®¡ç®—DAPOç­–ç•¥æŸå¤±ï¼ˆä½¿ç”¨Clip-Higherå’ŒToken-Level Lossï¼‰"""
        
        # ğŸ”¥ Token-Level Loss: å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—æŸå¤±
        if USE_TOKEN_LEVEL_LOSS and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            # Sample-level loss (ç±»ä¼¼GRPO)
            log_ratio = (log_probs - old_log_probs) * mask
            ratio = torch.exp(log_ratio)
            
            # ğŸ”¥ DAPO Clip-Higher: éå¯¹ç§°è£å‰ª
            adv_t = advantages.unsqueeze(1) # å¹¿æ’­ä¼˜åŠ¿åˆ°æ¯ä¸ª Token
            surr1 = ratio * adv_t
            surr2 = torch.clamp(ratio, 1 - CLIP_RANGE_LOW, 1 + CLIP_RANGE_HIGH) * adv_t
            policy_loss = -torch.min(surr1, surr2)
            
            # å¯¹ Mask æ±‚å‡å€¼
            loss_map = policy_loss * mask
            policy_loss = loss_map.sum() / mask.sum()
        
        return policy_loss

    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€æ­¥DAPOè®­ç»ƒ"""
        self.dynamic_sampling_stats["total_questions"] += len(batch_prompts)
        
        all_prompts = []
        all_responses = []
        all_response_lengths = []
        all_raw_rewards = []
        group_sizes = []
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªpromptç”Ÿæˆå›å¤å¹¶åº”ç”¨åŠ¨æ€é‡‡æ ·
        for prompt in batch_prompts:
            # ç”Ÿæˆåˆå§‹å›å¤ç»„
            responses, prompts_repeated, lengths = self.generate_responses([prompt])
            
            # è®¡ç®—å¥–åŠ±
            raw_rewards = self.compute_rewards(prompts_repeated, responses)
            
            # ğŸ”¥ åº”ç”¨åŠ¨æ€é‡‡æ ·
            responses, raw_rewards, lengths = self.apply_dynamic_sampling(
                prompt, responses, raw_rewards, lengths
            )
            
            # æ›´æ–°å¯¹åº”çš„promptåˆ—è¡¨
            prompts_repeated = [prompt] * len(responses)
            
            # ğŸ”¥ åº”ç”¨è¿‡é•¿å›å¤è¿‡æ»¤
            prompts_repeated, responses, raw_rewards, lengths = self.apply_overlong_filtering(
                prompts_repeated, responses, raw_rewards, lengths
            )
            
            if len(responses) > 0:  # ç¡®ä¿è¿‡æ»¤åè¿˜æœ‰æœ‰æ•ˆå›å¤
                all_prompts.extend(prompts_repeated)
                all_responses.extend(responses)
                all_response_lengths.extend(lengths)
                all_raw_rewards.append(raw_rewards)
                group_sizes.append(len(responses))
        
        if len(all_responses) == 0:
            print("æ‰€æœ‰å›å¤éƒ½è¢«è¿‡æ»¤ï¼Œè·³è¿‡æ­¤æ­¥")
            return {}
        
        # åˆå¹¶æ‰€æœ‰å¥–åŠ±
        all_raw_rewards = torch.cat(all_raw_rewards)
        
        # è®¡ç®—ç›¸å¯¹å¥–åŠ±
        relative_rewards, group_baselines = self.compute_relative_rewards(all_raw_rewards, group_sizes)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (relative_rewards - relative_rewards.mean()) / (relative_rewards.std() + 1e-8)
        
        # ğŸ”¥ è·å–tokençº§åˆ«çš„logæ¦‚ç‡ï¼ˆç”¨äºtoken-level lossï¼‰
        old_token_log_probs, mask, _, old_token_log_probs_list = self.get_token_log_probs(
            self.policy_model, all_prompts, all_responses, self.device_policy, return_per_token=USE_TOKEN_LEVEL_LOSS
        )
        
        # detachä»¥é¿å…æ¢¯åº¦ä¼ æ’­
        old_token_log_probs = old_token_log_probs.detach()
        mask = mask.detach()
        if old_token_log_probs_list:
            old_token_log_probs_list = [t.detach() for t in old_token_log_probs_list]
        
        # DAPOæ›´æ–°å¾ªç¯
        self.policy_model.train()
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        total_entropy = 0
        
        for _ in range(DAPO_EPOCHS):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_token_log_probs, _, entropy, new_token_log_probs_list = self.get_token_log_probs(
                self.policy_model, all_prompts, all_responses, self.device_policy, return_per_token=USE_TOKEN_LEVEL_LOSS
            )
            
            # è®¡ç®—å¹³å‡ç†µå€¼ (ä»…åœ¨responseåŒºåŸŸ)
            masked_entropy = entropy * mask.float()
            avg_entropy = masked_entropy.sum() / mask.sum()
            total_entropy += avg_entropy.item()
            
            # è®¡ç®—ç­–ç•¥æŸå¤±
            policy_loss = self.compute_policy_loss(
                new_token_log_probs, old_token_log_probs, advantages, mask,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            # ç†µæŸå¤±
            entropy_loss = -ENTROPY_COEF * avg_entropy
            
            # KLæŸå¤±ï¼ˆDAPOé»˜è®¤ä¸º0ï¼‰
            kl_loss = torch.tensor(0.0, device=self.device_policy)
            
            # æ€»æŸå¤±
            total_loss = policy_loss + entropy_loss + kl_loss
            
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_kl_loss += kl_loss.item()
            
            # æ˜¾å¼æ¸…ç†æ˜¾å­˜
            del new_token_log_probs, entropy, masked_entropy, policy_loss, entropy_loss, kl_loss, total_loss
            if new_token_log_probs_list:
                del new_token_log_probs_list
            
            self.optimizer.zero_grad(set_to_none=True)
            torch.cuda.empty_cache()
            gc.collect()
        
        # è®¡ç®—åŠ¨æ€é‡‡æ ·ç»Ÿè®¡
        resample_rate = (self.dynamic_sampling_stats["resampled_questions"] / 
                        max(1, self.dynamic_sampling_stats["total_questions"]))
        avg_extra = (self.dynamic_sampling_stats["avg_extra_samples"] / 
                    max(1, self.dynamic_sampling_stats["resampled_questions"]))
        
        metrics = {
            "policy_loss": total_policy_loss / DAPO_EPOCHS,
            "entropy_loss": total_entropy_loss / DAPO_EPOCHS,
            "kl_loss": total_kl_loss / DAPO_EPOCHS,
            "reward": all_raw_rewards.mean().item(),
            "entropy": total_entropy / DAPO_EPOCHS,
            "dynamic_resample_rate": resample_rate,
            "avg_response_length": sum(all_response_lengths) / len(all_response_lengths)
        }
        
        # è®°å½•æŒ‡æ ‡
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
        
        return metrics

    def train(self, dataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
        for epoch in range(NUM_EPOCHS):
            pbar = tqdm(dataloader)
            for batch in pbar:
                metrics = self.train_step(batch["prompt"])
                if metrics:  # å¦‚æœä¸ä¸ºç©º
                    pbar.set_description(
                        f"PL:{metrics['policy_loss']:.4f} R:{metrics['reward']:.2f} "
                        f"E:{metrics['entropy']:.3f} DSR:{metrics['dynamic_resample_rate']:.2f}"
                    )

            # ä¿å­˜æ¨¡å‹
            save_path = os.path.join(OUTPUT_DIR, f"epoch_{epoch+1}")
            self.policy_model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            
            # å¤‡ä»½è®­ç»ƒè„šæœ¬
            current_script = os.path.abspath(__file__)
            target_script = os.path.join(save_path, "train_script.py")
            
            try:
                shutil.copy2(current_script, target_script)
                print(f"è„šæœ¬å·²å¤‡ä»½è‡³: {target_script}")
            except Exception as e:
                print(f"è„šæœ¬å¤‡ä»½å¤±è´¥: {e}")
            
            print(f"æ¨¡å‹ä¿å­˜è‡³: {save_path}")
        
        # è®­ç»ƒç»“æŸåç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨
        print("\næ­£åœ¨ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å›¾è¡¨...")
        plot_dapo_metrics(
            policy_losses=self.metrics_history['policy_loss'],
            entropy_losses=self.metrics_history['entropy_loss'],
            rewards=self.metrics_history['reward'],
            entropies=self.metrics_history['entropy'],
            dynamic_resample_rates=self.metrics_history['dynamic_resample_rate'],
            avg_response_lengths=self.metrics_history['avg_response_length'],
            save_path=os.path.join(OUTPUT_DIR, "training_metrics.png")
        )
        print(f"è®­ç»ƒæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜è‡³: {os.path.join(OUTPUT_DIR, 'training_metrics.png')}")

def main():
    """æµ‹è¯•å‡½æ•°"""
    prompts = ["å¦‚ä½•åˆ¶ä½œä¸€æ¯å¥½å’–å•¡ï¼Ÿ", "è§£é‡Šé‡å­çº ç¼ ã€‚", "å†™ä¸€æ®µå†’æ³¡æ’åºä»£ç ã€‚"] * 10
    dataset = DAPODataset(prompts)
    trainer = DAPOTrainer()
    trainer.train(dataset)

def train_main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    prompts = []
    
    for datasets in train_datasets:
        if datasets['type'] == "jsonl":
            import json
            with open(datasets['path'], "r", encoding='utf-8') as f:
                for item in json.load(f):
                    prompts.append(item[datasets['input']])
        if datasets['type'] == 'parquet':
            import pyarrow.parquet as pq
            table = pq.read_table(datasets['path'])
            df = table.to_pandas()
            for index, row in df.iterrows():
                prompts.append(row['question'])

    prompts = prompts[:20]
    dataset = DAPODataset(prompts)
    trainer = DAPOTrainer()
    trainer.train(dataset)

if __name__ == "__main__":
    # main()
    train_main()