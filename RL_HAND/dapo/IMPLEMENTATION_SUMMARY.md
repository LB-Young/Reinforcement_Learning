# DAPOå®ç°æ€»ç»“

## å®Œæˆçš„å·¥ä½œ

### 1. æ ¸å¿ƒç®—æ³•å®ç° (`dapo.py`)

åŸºäºPPOå’ŒGRPOçš„ä»£ç ç»“æ„ï¼Œå®ç°äº†å®Œæ•´çš„DAPOè®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

#### ğŸ”¥ Clip-Higherï¼ˆéå¯¹ç§°è£å‰ªï¼‰
```python
# GRPO: å¯¹ç§°è£å‰ª [0.8, 1.2]
surr2 = torch.clamp(ratio, 1 - 0.2, 1 + 0.2) * advantages

# DAPO: éå¯¹ç§°è£å‰ª [0.8, 1.28]  
surr2 = torch.clamp(ratio, 1 - CLIP_RANGE_LOW, 1 + CLIP_RANGE_HIGH) * advantages
```

#### ğŸ”¥ Token-Level Loss
```python
def compute_policy_loss_token_level(self, token_log_probs_list, old_token_log_probs_list, advantages):
    """å¯¹æ¯ä¸ªtokenå•ç‹¬è®¡ç®—PPOæŸå¤±ï¼Œé¿å…çŸ­å›å¤åå¥½"""
    for i, (token_log_probs, old_token_log_probs) in enumerate(zip(...)):
        token_ratios = torch.exp(token_log_probs - old_token_log_probs)
        # å¯¹æ¯ä¸ªtokenåº”ç”¨Clip-Higher
        surr1 = token_ratios * advantage
        surr2 = torch.clamp(token_ratios, 1 - CLIP_RANGE_LOW, 1 + CLIP_RANGE_HIGH) * advantage
        token_loss = -torch.min(surr1, surr2).sum()
```

#### ğŸ”¥ Dynamic Sampling
```python
def apply_dynamic_sampling(self, prompt, initial_responses, initial_rewards, initial_lengths):
    """å¦‚æœæ‰€æœ‰å¥–åŠ±ç›¸åŒï¼Œç»§ç»­é‡‡æ ·ç›´åˆ°æœ‰å·®å¼‚"""
    while reward_std < 1e-6 and len(responses) < MAX_DYNAMIC_SAMPLES:
        # ç»§ç»­é‡‡æ ·é¢å¤–å›å¤
        extra_response = self.generate_one_more(prompt)
        # é‡æ–°è®¡ç®—å¥–åŠ±æ ‡å‡†å·®
```

#### ğŸ”¥ ç§»é™¤KLæƒ©ç½š
```python
KL_COEF = 0.0  # DAPOç§»é™¤KLæƒ©ç½šï¼Œå…è®¸ç­–ç•¥æ›´è‡ªç”±æ¢ç´¢
kl_loss = torch.tensor(0.0, device=self.device_policy)
```

#### ğŸ”¥ è¿‡é•¿å›å¤å¤„ç†
```python
def apply_overlong_filtering(self, prompts, responses, rewards, lengths):
    """è¿‡æ»¤è¶…é•¿å›å¤ï¼Œé¿å…ä¸å…¬å¹³æƒ©ç½š"""
    valid_indices = [i for i, length in enumerate(lengths) if length < MAX_RESPONSE_LENGTH]
```

### 2. å¯è§†åŒ–å·¥å…·æ‰©å±• (`utils/plot_metrics.py`)

ä¸ºDAPOæ·»åŠ äº†ä¸“ç”¨çš„ç»˜å›¾å‡½æ•°ï¼š

#### DAPOä¸“ç”¨æŒ‡æ ‡å›¾è¡¨
```python
def plot_dapo_metrics(policy_losses, entropy_losses, rewards, entropies, 
                     dynamic_resample_rates, avg_response_lengths, save_path):
    """ç»˜åˆ¶DAPOç‰¹æœ‰çš„6ä¸ªæŒ‡æ ‡"""
```

#### DAPO vs GRPOå¯¹æ¯”å›¾è¡¨
```python
def plot_dapo_vs_grpo_comparison(dapo_losses, grpo_losses, dapo_rewards, grpo_rewards, 
                                dapo_entropies, grpo_entropies, save_path):
    """å¯¹æ¯”DAPOå’ŒGRPOçš„æ€§èƒ½å·®å¼‚"""
```

### 3. æ–‡æ¡£å’Œæµ‹è¯•

#### è¯¦ç»†è¯´æ˜æ–‡æ¡£ (`README.md`)
- ç®—æ³•åŸç†è§£é‡Š
- é…ç½®å‚æ•°è¯´æ˜
- ä½¿ç”¨æ–¹æ³•æŒ‡å—
- æ€§èƒ½å¯¹æ¯”æ•°æ®
- é€‚ç”¨åœºæ™¯åˆ†æ

#### æµ‹è¯•éªŒè¯è„šæœ¬ (`test_dapo.py`)
- ç»„ä»¶åŠŸèƒ½æµ‹è¯•
- ä¸GRPOå·®å¼‚éªŒè¯
- ç®—æ³•ç‰¹æ€§æ£€æŸ¥

## ä»£ç ç»“æ„å¯¹æ¯”

### ä¸PPOçš„ä¸€è‡´æ€§
- ç›¸åŒçš„æ–‡ä»¶å¤´æ³¨é‡Šæ ¼å¼
- ç›¸åŒçš„é…ç½®å‚æ•°ç»„ç»‡æ–¹å¼
- ç›¸åŒçš„æ•°æ®é›†ç±»ç»“æ„
- ç›¸åŒçš„è®­ç»ƒå¾ªç¯é€»è¾‘
- ç›¸åŒçš„æ¨¡å‹ä¿å­˜å’Œè„šæœ¬å¤‡ä»½æœºåˆ¶

### ä¸GRPOçš„ä¸€è‡´æ€§
- ç›¸åŒçš„GPUè®¾å¤‡åˆ†é…ç­–ç•¥
- ç›¸åŒçš„æ˜¾å­˜ç®¡ç†å’Œæ¸…ç†æœºåˆ¶
- ç›¸åŒçš„tokençº§åˆ«log_probsè®¡ç®—æ–¹æ³•
- ç›¸åŒçš„ç›¸å¯¹å¥–åŠ±è®¡ç®—é€»è¾‘
- ç›¸åŒçš„æŒ‡æ ‡è®°å½•å’Œå¯è§†åŒ–æµç¨‹

### DAPOçš„ç‹¬ç‰¹æ€§
- éå¯¹ç§°è£å‰ªèŒƒå›´é…ç½®
- Tokençº§åˆ«æŸå¤±è®¡ç®—
- åŠ¨æ€é‡‡æ ·é€»è¾‘
- è¿‡é•¿å›å¤å¤„ç†
- ä¸“ç”¨æŒ‡æ ‡ç»Ÿè®¡

## å…³é”®æŠ€æœ¯å®ç°

### 1. Token-Level Lossè®¡ç®—
```python
# ä¸ºæ¯ä¸ªæ ·æœ¬æå–responseéƒ¨åˆ†çš„token log_probs
for i, p_len in enumerate(prompt_lens):
    response_mask = mask[i, p_len:]
    if response_mask.sum() > 0:
        sample_token_log_probs = token_log_probs[i, p_len:][response_mask]
        per_token_log_probs.append(sample_token_log_probs)
```

### 2. åŠ¨æ€é‡‡æ ·å®ç°
```python
# æ£€æŸ¥å¥–åŠ±æ ‡å‡†å·®ï¼Œå¦‚æœå¤ªå°åˆ™ç»§ç»­é‡‡æ ·
reward_std = rewards.std().item()
while reward_std < 1e-6 and len(responses) < MAX_DYNAMIC_SAMPLES:
    # ç”Ÿæˆé¢å¤–æ ·æœ¬å¹¶æ›´æ–°å¥–åŠ±
```

### 3. éå¯¹ç§°è£å‰ª
```python
# Clip-Higher: ä¸Šç•Œæ›´å¤§ï¼Œä¸‹ç•Œä¿æŒä¸å˜
CLIP_RANGE_LOW = 0.2    # [0.8, ...]
CLIP_RANGE_HIGH = 0.28  # [..., 1.28]
```

## é…ç½®å‚æ•°

### DAPOç‰¹æœ‰å‚æ•°
```python
CLIP_RANGE_LOW = 0.2        # ä¸‹ç•Œè£å‰ªèŒƒå›´
CLIP_RANGE_HIGH = 0.28      # ä¸Šç•Œè£å‰ªèŒƒå›´ï¼ˆClip-Higherï¼‰
KL_COEF = 0.0               # ç§»é™¤KLæƒ©ç½š
USE_DYNAMIC_SAMPLING = True # åŠ¨æ€é‡‡æ ·
USE_TOKEN_LEVEL_LOSS = True # Tokençº§åˆ«æŸå¤±
USE_OVERLONG_FILTERING = True # è¿‡é•¿å›å¤è¿‡æ»¤
MAX_RESPONSE_LENGTH = 256   # æœ€å¤§å›å¤é•¿åº¦
```

### ä¸GRPOçš„å¯¹æ¯”
| å‚æ•° | GRPO | DAPO | è¯´æ˜ |
|------|------|------|------|
| è£å‰ªèŒƒå›´ | [0.8, 1.2] | [0.8, 1.28] | éå¯¹ç§°è£å‰ª |
| KLç³»æ•° | 0.01 | 0.0 | ç§»é™¤KLæƒ©ç½š |
| æŸå¤±çº§åˆ« | Sample | Token | Tokençº§åˆ«æŸå¤± |
| åŠ¨æ€é‡‡æ · | âŒ | âœ… | DAPOç‰¹æœ‰ |

## é¢„æœŸæ€§èƒ½æå‡

æ ¹æ®è®ºæ–‡å®éªŒç»“æœï¼š
- **å‡†ç¡®ç‡æå‡**: 47 â†’ 50 åˆ†ï¼ˆ+6.4%ï¼‰
- **è®­ç»ƒæ•ˆç‡**: ä»…éœ€50%çš„è®­ç»ƒæ­¥æ•°
- **ç†µç¨³å®šæ€§**: é¿å…ç†µå´©æºƒé—®é¢˜
- **æ¨ç†è´¨é‡**: ç”Ÿæˆæ›´é•¿ã€æ›´è¯¦ç»†çš„æ¨ç†é“¾

## ä½¿ç”¨å»ºè®®

### é€‚åˆDAPOçš„åœºæ™¯
- æ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆå¦‚GSM8Kã€MATHï¼‰
- ä»£ç ç”Ÿæˆå’Œè°ƒè¯•
- éœ€è¦è¯¦ç»†è§£é‡Šçš„å¤æ‚é—®ç­”
- é•¿é“¾æ¨ç†ä»»åŠ¡
- æœ‰å……è¶³è®¡ç®—èµ„æºçš„åœºæ™¯

### è¶…å‚æ•°è°ƒä¼˜å»ºè®®
- æ ¹æ®ä»»åŠ¡è°ƒæ•´`CLIP_RANGE_HIGH`ï¼ˆ0.25-0.3ï¼‰
- åŠ¨æ€é‡‡æ ·é˜ˆå€¼å¯æ ¹æ®æ¨¡å‹å¼ºåº¦è°ƒæ•´
- æœ€å¤§å›å¤é•¿åº¦æ ¹æ®ä»»åŠ¡éœ€æ±‚è®¾ç½®
- æ‰¹æ¬¡å¤§å°éœ€è¦æ ¹æ®æ˜¾å­˜å®¹é‡è°ƒæ•´

## æ€»ç»“

æˆåŠŸå®ç°äº†å®Œæ•´çš„DAPOç®—æ³•ï¼Œä¿æŒäº†ä¸ç°æœ‰PPOå’ŒGRPOä»£ç çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶èå…¥äº†DAPOçš„æ‰€æœ‰æ ¸å¿ƒæ”¹è¿›ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†ï¼Œå…·å¤‡è‰¯å¥½çš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚