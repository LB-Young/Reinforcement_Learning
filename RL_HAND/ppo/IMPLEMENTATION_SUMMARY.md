# PPOå®ç°æ€»ç»“

## å®Œæˆçš„å·¥ä½œ

### 1. æ ¸å¿ƒç®—æ³•å®ç° (`ppo.py`)

å®ç°äº†å®Œæ•´çš„PPO (Proximal Policy Optimization) è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

#### ğŸ”¥ Actor-Criticæ¶æ„
```python
# ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰
self.policy_model = AutoModelForCausalLM.from_pretrained(ACTOR_MODEL)
self.policy_optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=LEARNING_RATE)

# ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰
self.critic_model = AutoModelForCausalLM.from_pretrained(CRITIC_MODEL)
self.critic_optimizer = torch.optim.AdamW(self.critic_model.parameters(), lr=LEARNING_RATE)

# å‚è€ƒç­–ç•¥ç½‘ç»œï¼ˆReferenceï¼‰
self.reference_model = AutoModelForCausalLM.from_pretrained(ACTOR_MODEL)
self.reference_model.eval()
```

#### ğŸ”¥ PPO ClipæŸå¤±
```python
def compute_policy_loss(self, log_probs, old_log_probs, advantages, mask):
    """PPOçš„æ ¸å¿ƒï¼šå¯¹ç§°è£å‰ªæŸå¤±"""
    log_ratio = (log_probs - old_log_probs) * mask
    ratio = torch.exp(log_ratio)
    
    # PPOå¯¹ç§°è£å‰ª
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * advantages
    policy_loss = -torch.min(surr1, surr2)
    
    # KLæ•£åº¦æƒ©ç½š
    kl_div = (log_probs - ref_log_probs)
    loss_map = (policy_loss + 0.01 * kl_div) * mask
    return loss_map.sum() / mask.sum()
```

#### ğŸ”¥ ä¼˜åŠ¿å‡½æ•°è®¡ç®—
```python
def compute_values(self, prompts, responses, requires_grad=False):
    """ä½¿ç”¨criticæ¨¡å‹è®¡ç®—çŠ¶æ€ä»·å€¼"""
    for p, r in zip(prompts, responses):
        full_text = p + r
        inputs = self.critic_tokenizer(full_text, return_tensors="pt")
        outputs = self.critic_model(**inputs)
        # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„logitsä½œä¸ºvalueï¼ˆç®€åŒ–å®ç°ï¼‰
        value = outputs.logits[0, -1, :].mean()
        values.append(value)
    return torch.stack(values)

# è®¡ç®—ä¼˜åŠ¿ï¼šA = R - V
advantages = rewards - values
```

#### ğŸ”¥ å¤šè½®æ›´æ–°æœºåˆ¶
```python
# PPOæ›´æ–°å¾ªç¯
for _ in range(GROUP_EPOCHES):
    # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
    new_log_probs = self.get_token_log_probs(self.policy_model, prompts, responses)
    
    # è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆPPO Clipï¼‰
    policy_loss = self.compute_policy_loss(new_log_probs, old_log_probs, advantages, mask)
    
    # æ›´æ–°ç­–ç•¥ç½‘ç»œ
    self.policy_optimizer.zero_grad()
    policy_loss.backward()
    self.policy_optimizer.step()
    
    # è®¡ç®—ä»·å€¼æŸå¤±
    new_values = self.compute_values(prompts, responses, requires_grad=True)
    value_loss = F.mse_loss(new_values, rewards)
    
    # æ›´æ–°ä»·å€¼ç½‘ç»œ
    self.critic_optimizer.zero_grad()
    value_loss.backward()
    self.critic_optimizer.step()
```

#### ğŸ”¥ ç†µæ­£åˆ™åŒ–
```python
def compute_entropy(self, prompts, responses):
    """è®¡ç®—ç­–ç•¥ç†µï¼Œé¼“åŠ±æ¢ç´¢"""
    probs = F.softmax(logits, dim=-1)
    log_probs = F.log_softmax(logits, dim=-1)
    entropy = -(probs * log_probs).sum(dim=-1)  # [B, seq_len]
    
    # ä»…åœ¨responseåŒºåŸŸè®¡ç®—ç†µ
    masked_entropy = entropy * mask
    avg_entropy = masked_entropy.sum() / mask.sum()
    return avg_entropy
```

### 2. åŒGPUæ¶æ„è®¾è®¡

```python
# GPUè®¾å¤‡åˆ†é…ç­–ç•¥
self.device_policy = torch.device("cuda:0")    # ç­–ç•¥æ¨¡å‹
self.device_ref = torch.device("cuda:0")       # å‚è€ƒæ¨¡å‹
self.device_critic = torch.device("cuda:1")    # ä»·å€¼æ¨¡å‹
self.device_reward = torch.device("cuda:1")    # å¥–åŠ±æ¨¡å‹
```

### 3. Tokençº§åˆ«è®¡ç®—

```python
def get_token_log_probs(self, model, prompts, responses, device, tokenizer):
    """è·å–Tokençº§åˆ«çš„log_probså¹¶è¿”å›Mask"""
    full_texts = [p + r for p, r in zip(prompts, responses)]
    inputs = tokenizer(full_texts, return_tensors="pt", padding=True, truncation=True)
    
    # è®¡ç®—Promptçš„é•¿åº¦ï¼Œä»…åœ¨ResponseåŒºåŸŸè®¡ç®—æŸå¤±
    prompt_lens = [len(tokenizer.encode(p, add_special_tokens=False)) for p in prompts]
    
    outputs = model(**inputs)
    logits = outputs.logits[:, :-1, :]  # Shiftå¯¹é½
    labels = inputs["input_ids"][:, 1:]
    
    log_probs = F.log_softmax(logits, dim=-1)
    token_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)
    
    # åˆ¶ä½œMask: 1ä»…åœ¨ResponseåŒºåŸŸä¸”éPaddingå¤„
    mask = torch.zeros_like(labels, dtype=torch.bool)
    for i, p_len in enumerate(prompt_lens):
        mask[i, p_len:] = (labels[i, p_len:] != tokenizer.pad_token_id)
    
    return token_log_probs, mask
```

### 4. å¯è§†åŒ–å·¥å…·é›†æˆ

#### PPOä¸“ç”¨æŒ‡æ ‡å›¾è¡¨
```python
def plot_ppo_metrics_with_entropy(policy_losses, value_losses, rewards, advantages, entropies):
    """ç»˜åˆ¶PPOçš„5ä¸ªæ ¸å¿ƒæŒ‡æ ‡"""
    # ç­–ç•¥æŸå¤±ã€ä»·å€¼æŸå¤±ã€å¥–åŠ±ã€ä¼˜åŠ¿ã€ç†µ
```

#### é€šç”¨è®­ç»ƒæŒ‡æ ‡
```python
def plot_training_metrics(metrics_history):
    """é€šç”¨çš„è®­ç»ƒæŒ‡æ ‡å¯è§†åŒ–"""
    # æ”¯æŒä»»æ„æ•°é‡çš„æŒ‡æ ‡åŠ¨æ€ç»˜åˆ¶
```

## ç®—æ³•åŸç†

### PPOæ ¸å¿ƒæ€æƒ³
PPOé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦æ¥ä¿è¯è®­ç»ƒç¨³å®šæ€§ï¼Œé¿å…ç­–ç•¥å´©æºƒï¼š

1. **ä¿¡ä»»åŸŸçº¦æŸ**: é€šè¿‡è£å‰ªæ¯”ç‡é™åˆ¶ç­–ç•¥å˜åŒ–
2. **Actor-Critic**: ç»“åˆç­–ç•¥æ¢¯åº¦å’Œä»·å€¼å‡½æ•°
3. **å¤šè½®æ›´æ–°**: åœ¨åŒä¸€æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ›´æ–°
4. **KLæ•£åº¦æƒ©ç½š**: é˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒç­–ç•¥è¿‡è¿œ

### æ•°å­¦å…¬å¼

#### PPO-Clipç›®æ ‡å‡½æ•°
```
L^CLIP(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)]
```
å…¶ä¸­ï¼š
- `r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)` æ˜¯æ¦‚ç‡æ¯”ç‡
- `A_t` æ˜¯ä¼˜åŠ¿å‡½æ•°
- `Îµ` æ˜¯è£å‰ªå‚æ•°ï¼ˆCLIP_RANGE = 0.2ï¼‰

#### ä»·å€¼å‡½æ•°æŸå¤±
```
L^VF(Î¸) = E[(V_Î¸(s_t) - V_t^targ)^2]
```

#### ç†µå¥–åŠ±
```
L^ENT(Î¸) = E[H(Ï€_Î¸(Â·|s_t))]
```

#### æ€»ç›®æ ‡å‡½æ•°
```
L(Î¸) = L^CLIP(Î¸) - c1*L^VF(Î¸) + c2*L^ENT(Î¸)
```

## ä»£ç ç»“æ„ç‰¹ç‚¹

### 1. æ¨¡å—åŒ–è®¾è®¡
- æ¸…æ™°çš„ç±»ç»“æ„å’Œæ–¹æ³•åˆ†ç¦»
- ç‹¬ç«‹çš„å¥–åŠ±è®¡ç®—ã€ä»·å€¼ä¼°è®¡ã€ç­–ç•¥æ›´æ–°æ¨¡å—
- å¯å¤ç”¨çš„tokençº§åˆ«è®¡ç®—å‡½æ•°

### 2. æ˜¾å­˜ä¼˜åŒ–
```python
# æ˜¾å­˜ç®¡ç†ç­–ç•¥
del inputs, outputs, gen_ids  # åŠæ—¶é‡Šæ”¾å¼ é‡
torch.cuda.empty_cache()      # æ¸…ç†æ˜¾å­˜ç¼“å­˜
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
```

### 3. é”™è¯¯å¤„ç†å’Œæ—¥å¿—
```python
try:
    shutil.copy2(current_script, target_script)
    print(f"è„šæœ¬å·²å¤‡ä»½è‡³: {target_script}")
except Exception as e:
    print(f"è„šæœ¬å¤‡ä»½å¤±è´¥: {e}")
```

## é…ç½®å‚æ•°

### æ ¸å¿ƒè¶…å‚æ•°
```python
LEARNING_RATE = 1e-6        # å­¦ä¹ ç‡
BATCH_SIZE = 4              # æ‰¹æ¬¡å¤§å°
GROUP_SIZE = 1              # æ¯ä¸ªpromptçš„å›å¤æ•°é‡
GROUP_EPOCHES = 4           # PPOæ›´æ–°è½®æ•°
CLIP_RANGE = 0.2            # PPOè£å‰ªèŒƒå›´
```

### æ¨¡å‹é…ç½®
```python
ACTOR_MODEL = r"E:\models\Qwen\Qwen3-0___6B"      # ç­–ç•¥æ¨¡å‹
CRITIC_MODEL = r"E:\models\Qwen\Qwen3-0___6B"     # ä»·å€¼æ¨¡å‹
REWARD_MODEL = r"E:\models\reward-model-deberta-v3-large-v2"  # å¥–åŠ±æ¨¡å‹
```

### è®­ç»ƒé…ç½®
```python
NUM_EPOCHES = 1             # è®­ç»ƒè½®æ•°
DTYPE = torch.bfloat16      # æ•°æ®ç±»å‹
OUTPUT_DIR = "ppo_output"   # è¾“å‡ºç›®å½•
```

## ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”

### PPO vs ä¼ ç»Ÿç­–ç•¥æ¢¯åº¦
| ç‰¹æ€§ | ä¼ ç»ŸPG | PPO |
|------|--------|-----|
| ç¨³å®šæ€§ | ä¸ç¨³å®š | ç¨³å®š |
| æ ·æœ¬æ•ˆç‡ | ä½ | é«˜ |
| å®ç°å¤æ‚åº¦ | ç®€å• | ä¸­ç­‰ |
| è¶…å‚æ•°æ•æ„Ÿæ€§ | é«˜ | ä½ |

### PPO vs GRPO vs DAPO
| ç‰¹æ€§ | PPO | GRPO | DAPO |
|------|-----|------|------|
| æ¶æ„ | Actor-Critic | Policy-Only | Policy-Only |
| è£å‰ªæ–¹å¼ | å¯¹ç§° | å¯¹ç§° | éå¯¹ç§° |
| æŸå¤±çº§åˆ« | Token | Token | Token |
| KLæƒ©ç½š | âœ… | âœ… | âŒ |
| åŠ¨æ€é‡‡æ · | âŒ | âŒ | âœ… |
| é€‚ç”¨åœºæ™¯ | é€šç”¨ | é•¿æ–‡æœ¬ | é•¿é“¾æ¨ç† |

## è®­ç»ƒæŒ‡æ ‡

### è®°å½•çš„æŒ‡æ ‡
```python
self.metrics_history = {
    'policy_loss': [],      # ç­–ç•¥æŸå¤±
    'value_loss': [],       # ä»·å€¼æŸå¤±
    'reward': [],           # å¹³å‡å¥–åŠ±
    'advantage': [],        # å¹³å‡ä¼˜åŠ¿
    'entropy': []           # ç­–ç•¥ç†µ
}
```

### æŒ‡æ ‡å«ä¹‰
- **Policy Loss**: ç­–ç•¥ç½‘ç»œçš„æŸå¤±ï¼Œåæ˜ ç­–ç•¥æ›´æ–°å¹…åº¦
- **Value Loss**: ä»·å€¼ç½‘ç»œçš„æŸå¤±ï¼Œåæ˜ ä»·å€¼ä¼°è®¡å‡†ç¡®æ€§
- **Reward**: å¥–åŠ±æ¨¡å‹ç»™å‡ºçš„å¹³å‡å¥–åŠ±
- **Advantage**: ä¼˜åŠ¿å‡½æ•°å€¼ï¼Œåæ˜ åŠ¨ä½œçš„ç›¸å¯¹ä»·å€¼
- **Entropy**: ç­–ç•¥ç†µï¼Œåæ˜ æ¢ç´¢ç¨‹åº¦

## ä½¿ç”¨åœºæ™¯

### PPOé€‚åˆçš„ä»»åŠ¡
- **é€šç”¨å¯¹è¯ç”Ÿæˆ**: å¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§
- **æ–‡æœ¬æ‘˜è¦**: éœ€è¦å‡†ç¡®çš„ä»·å€¼ä¼°è®¡
- **ä»£ç ç”Ÿæˆ**: éœ€è¦ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
- **åˆ›æ„å†™ä½œ**: éœ€è¦ä¿æŒä¸€å®šçš„éšæœºæ€§

### PPOçš„ä¼˜åŠ¿
- **è®­ç»ƒç¨³å®š**: è£å‰ªæœºåˆ¶é˜²æ­¢ç­–ç•¥å´©æºƒ
- **æ ·æœ¬æ•ˆç‡é«˜**: å¤šè½®æ›´æ–°å……åˆ†åˆ©ç”¨æ•°æ®
- **é€šç”¨æ€§å¼º**: é€‚ç”¨äºå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡
- **ç†è®ºåŸºç¡€æ‰å®**: æœ‰å®Œå–„çš„ç†è®ºæ”¯æ’‘

### PPOçš„å±€é™æ€§
- **è®¡ç®—å¼€é”€å¤§**: éœ€è¦è®­ç»ƒä¸¤ä¸ªç½‘ç»œï¼ˆActor + Criticï¼‰
- **è¶…å‚æ•°è¾ƒå¤š**: éœ€è¦è°ƒèŠ‚å¤šä¸ªè¶…å‚æ•°
- **æ˜¾å­˜éœ€æ±‚é«˜**: éœ€è¦åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹
- **æ”¶æ•›è¾ƒæ…¢**: ç›¸æ¯”ç®€å•æ–¹æ³•éœ€è¦æ›´å¤šè®­ç»ƒæ­¥æ•°

## å®ç°äº®ç‚¹

### 1. åŒç½‘ç»œæ¶æ„
```python
# ç­–ç•¥ç½‘ç»œè´Ÿè´£ç”Ÿæˆ
policy_outputs = self.policy_model.generate(...)

# ä»·å€¼ç½‘ç»œè´Ÿè´£è¯„ä¼°
values = self.critic_model(**inputs)

# å‚è€ƒç½‘ç»œæä¾›åŸºçº¿
ref_log_probs = self.reference_model(**inputs)
```

### 2. ç²¾ç¡®çš„Tokençº§åˆ«è®¡ç®—
```python
# åªåœ¨responseåŒºåŸŸè®¡ç®—æŸå¤±ï¼Œé¿å…promptéƒ¨åˆ†çš„å¹²æ‰°
mask[i, p_len:] = (labels[i, p_len:] != tokenizer.pad_token_id)
masked_loss = loss * mask
final_loss = masked_loss.sum() / mask.sum()
```

### 3. å®Œå–„çš„æŒ‡æ ‡ç›‘æ§
```python
# å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
pbar.set_description(
    f"PL:{metrics['policy_loss']:.4f} VL:{metrics['value_loss']:.4f} "
    f"R:{metrics['reward']:.2f} A:{metrics['advantage']:.2f} E:{metrics['entropy']:.3f}"
)
```

### 4. è‡ªåŠ¨åŒ–æ¨¡å‹ç®¡ç†
```python
# è‡ªåŠ¨ä¿å­˜æ¨¡å‹å’Œé…ç½®
self.policy_model.save_pretrained(save_path)
self.critic_model.save_pretrained(critic_save_path)

# è‡ªåŠ¨å¤‡ä»½è®­ç»ƒè„šæœ¬
shutil.copy2(current_script, target_script)
```

## æ€»ç»“

PPOå®ç°æä¾›äº†ä¸€ä¸ªç¨³å®šã€é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡Actor-Criticæ¶æ„å’Œè£å‰ªæœºåˆ¶ï¼ŒPPOåœ¨ä¿è¯è®­ç»ƒç¨³å®šæ€§çš„åŒæ—¶å®ç°äº†è¾ƒé«˜çš„æ ·æœ¬æ•ˆç‡ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦é«˜ï¼Œå…·æœ‰è‰¯å¥½çš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚