# GRPOå®ç°æ€»ç»“

## å®Œæˆçš„å·¥ä½œ

### 1. æ ¸å¿ƒç®—æ³•å®ç° (`grpo.py`)

å®ç°äº†å®Œæ•´çš„GRPO (Group Relative Policy Optimization) è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

#### ğŸ”¥ Group-basedç›¸å¯¹å¥–åŠ±æœºåˆ¶
```python
def compute_relative_rewards(self, rewards, group_size):
    """GRPOæ ¸å¿ƒï¼šç»„å†…ç›¸å¯¹å¥–åŠ±è®¡ç®—"""
    batch_size = rewards.shape[0]
    rewards_grouped = rewards.view(-1, group_size)  # [num_groups, group_size]
    
    # ç»„å†…åŸºçº¿ï¼ˆå‡å€¼ï¼‰
    group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
    
    # ç›¸å¯¹å¥–åŠ± = ç»å¯¹å¥–åŠ± - ç»„å†…å‡å€¼
    relative_rewards = rewards_grouped - group_baselines
    
    # ç»„å†…æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
    if self.config.use_group_normalization:
        group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
        relative_rewards = relative_rewards / group_std
    
    return relative_rewards.view(-1), group_baselines.repeat(1, group_size).view(-1)
```

#### ğŸ”¥ Policy-Onlyæ¶æ„
```python
# GRPOåªéœ€è¦ç­–ç•¥ç½‘ç»œï¼Œæ— éœ€ä»·å€¼ç½‘ç»œ
self.policy_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL)
self.ref_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL)  # å‚è€ƒç­–ç•¥
self.reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL)

# å•ä¸€ä¼˜åŒ–å™¨
self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=LEARNING_RATE)
```

#### ğŸ”¥ å¤šå›å¤ç”Ÿæˆç­–ç•¥
```python
def generate_responses(self, prompts):
    """ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒ"""
    for prompt in prompts:
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.policy_model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=True,
            temperature=0.7,
            num_return_sequences=GROUP_SIZE,  # ğŸ”¥ å…³é”®ï¼šæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤
            pad_token_id=self.tokenizer.pad_token_id
        )
        
        # æå–ç”Ÿæˆçš„å›å¤éƒ¨åˆ†
        gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
        responses = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
        all_responses.extend(responses)
        all_prompts.extend([prompt] * GROUP_SIZE)
```

#### ğŸ”¥ Tokençº§åˆ«PPOæŸå¤±
```python
def compute_policy_loss(self, new_log_probs, old_log_probs, advantages, mask):
    """GRPOä½¿ç”¨Tokençº§åˆ«çš„PPOæŸå¤±"""
    # Importance Sampling Ratio (Tokençº§åˆ«)
    log_ratio = (new_log_probs - old_log_probs) * mask
    ratio = torch.exp(log_ratio)
    
    # PPO Clip Loss (å¯¹ç§°è£å‰ª)
    adv_t = advantages.unsqueeze(1)  # å¹¿æ’­ä¼˜åŠ¿åˆ°æ¯ä¸ªToken
    surr1 = ratio * adv_t
    surr2 = torch.clamp(ratio, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * adv_t
    policy_loss = -torch.min(surr1, surr2)
    
    # KLæ•£åº¦æƒ©ç½š
    kl_div = (new_log_probs - ref_log_probs)
    
    # ç»„åˆæŸå¤±å¹¶å¯¹Maskæ±‚å‡å€¼
    loss_map = (policy_loss + KL_COEF * kl_div) * mask
    return loss_map.sum() / mask.sum()
```

#### ğŸ”¥ ä¼˜åŠ¿å‡½æ•°æ ‡å‡†åŒ–
```python
def compute_advantages(self, relative_rewards):
    """å…¨å±€æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°"""
    # ç›¸å¯¹å¥–åŠ±å·²ç»æ˜¯ç»„å†…æ ‡å‡†åŒ–çš„ï¼Œè¿™é‡Œè¿›è¡Œå…¨å±€æ ‡å‡†åŒ–
    advantages = (relative_rewards - relative_rewards.mean()) / (relative_rewards.std() + 1e-8)
    return advantages
```

#### ğŸ”¥ å¤šè½®ç­–ç•¥æ›´æ–°
```python
# GRPOæ›´æ–°å¾ªç¯
for _ in range(GRPO_EPOCHS):
    # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
    new_log_probs, _, entropy = self.get_token_log_probs(
        self.policy_model, prompts_expanded, responses, self.device_policy
    )
    
    # è®¡ç®—ç­–ç•¥æŸå¤±
    policy_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, mask, ref_log_probs
    )
    
    # æ›´æ–°ç­–ç•¥
    self.optimizer.zero_grad()
    policy_loss.backward()
    torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
    self.optimizer.step()
```

### 2. æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

#### åŒGPUæ¶æ„
```python
# GPUè®¾å¤‡åˆ†é…ï¼ˆé’ˆå¯¹5060ti-16G * 2çš„é…ç½®ï¼‰
self.device_policy = torch.device("cuda:0")    # ç­–ç•¥æ¨¡å‹
self.device_ref = torch.device("cuda:1")       # å‚è€ƒæ¨¡å‹
self.device_reward = torch.device("cuda:1")    # å¥–åŠ±æ¨¡å‹
```

#### æ˜¾å­˜ç®¡ç†
```python
def cleanup_memory(self):
    """æ˜¾å­˜æ¸…ç†ç­–ç•¥"""
    # 1. åˆ é™¤å¼ é‡å˜é‡
    del inputs, outputs, logits, probs, labels
    
    # 2. æ¸…ç†æ¢¯åº¦ç¼“å­˜
    self.optimizer.zero_grad(set_to_none=True)
    
    # 3. æ¸…ç†PyTorchç¼“å­˜
    torch.cuda.empty_cache()
    
    # 4. å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
```

### 3. Tokençº§åˆ«ç²¾ç¡®è®¡ç®—

```python
def get_token_log_probs(self, model, prompts, responses, device):
    """è·å–Tokençº§åˆ«çš„log_probsã€Maskå’ŒEntropy"""
    full_texts = [p + r for p, r in zip(prompts, responses)]
    inputs = self.tokenizer(full_texts, return_tensors="pt", padding=True, truncation=True)
    
    # è®¡ç®—Prompté•¿åº¦ï¼Œä»…åœ¨ResponseåŒºåŸŸè®¡ç®—
    prompt_lens = [len(self.tokenizer.encode(p, add_special_tokens=False)) for p in prompts]
    
    outputs = model(**inputs)
    logits = outputs.logits[:, :-1, :]  # Shiftå¯¹é½
    labels = inputs["input_ids"][:, 1:]
    
    # Tokençº§åˆ«logæ¦‚ç‡
    log_probs = F.log_softmax(logits, dim=-1)
    token_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)
    
    # ç†µè®¡ç®—ï¼ˆä»…åœ¨responseåŒºåŸŸï¼‰
    probs = F.softmax(logits, dim=-1)
    entropy = -(probs * log_probs).sum(dim=-1)
    
    # åˆ¶ä½œMaskï¼š1ä»…åœ¨ResponseåŒºåŸŸä¸”éPaddingå¤„
    mask = torch.zeros_like(labels, dtype=torch.bool)
    for i, p_len in enumerate(prompt_lens):
        mask[i, p_len:] = (labels[i, p_len:] != self.tokenizer.pad_token_id)
    
    return token_log_probs, mask, entropy
```

### 4. å¯è§†åŒ–å·¥å…·é›†æˆ

#### GRPOä¸“ç”¨æŒ‡æ ‡å›¾è¡¨
```python
def plot_grpo_metrics(losses, rewards, entropies, save_path):
    """ç»˜åˆ¶GRPOçš„3ä¸ªæ ¸å¿ƒæŒ‡æ ‡"""
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))
    
    # Lossæ›²çº¿
    ax1.plot(losses, 'b-', linewidth=2)
    ax1.set_title('Training Loss')
    
    # Rewardæ›²çº¿  
    ax2.plot(rewards, 'g-', linewidth=2)
    ax2.set_title('Average Reward')
    
    # Entropyæ›²çº¿
    ax3.plot(entropies, 'r-', linewidth=2)
    ax3.set_title('Policy Entropy')
```

## ç®—æ³•åŸç†

### GRPOæ ¸å¿ƒæ€æƒ³
GRPOé€šè¿‡ç»„å†…ç›¸å¯¹æ¯”è¾ƒæ¥å­¦ä¹ ï¼Œé¿å…äº†ç»å¯¹å¥–åŠ±çš„åå·®é—®é¢˜ï¼š

1. **ç›¸å¯¹å¥–åŠ±**: ä½¿ç”¨ç»„å†…ç›¸å¯¹å¥–åŠ±è€Œéç»å¯¹å¥–åŠ±
2. **Policy-Only**: æ— éœ€ä»·å€¼ç½‘ç»œï¼Œç®€åŒ–æ¶æ„
3. **ç»„å†…æ¯”è¾ƒ**: åŒä¸€promptçš„ä¸åŒå›å¤è¿›è¡Œæ¯”è¾ƒ
4. **Tokençº§åˆ«**: åœ¨tokençº§åˆ«è®¡ç®—æŸå¤±å’Œæ¢¯åº¦

### æ•°å­¦å…¬å¼

#### ç›¸å¯¹å¥–åŠ±è®¡ç®—
```
R_rel(x_i, y_i) = R(x_i, y_i) - (1/K) * Î£ R(x_i, y_j)
```
å…¶ä¸­ï¼š
- `R(x_i, y_i)` æ˜¯ç»å¯¹å¥–åŠ±
- `K` æ˜¯ç»„å¤§å°ï¼ˆGROUP_SIZEï¼‰
- `x_i` æ˜¯promptï¼Œ`y_j` æ˜¯ç¬¬jä¸ªå›å¤

#### ä¼˜åŠ¿å‡½æ•°
```
A_i = (R_rel_i - Î¼) / Ïƒ
```
å…¶ä¸­ï¼š
- `Î¼` æ˜¯ç›¸å¯¹å¥–åŠ±çš„å‡å€¼
- `Ïƒ` æ˜¯ç›¸å¯¹å¥–åŠ±çš„æ ‡å‡†å·®

#### GRPOç›®æ ‡å‡½æ•°
```
L^GRPO(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)] + Î²*KL(Ï€_Î¸||Ï€_ref)
```

### ä¸PPOçš„åŒºåˆ«

| ç‰¹æ€§ | PPO | GRPO |
|------|-----|------|
| æ¶æ„ | Actor-Critic | Policy-Only |
| å¥–åŠ±ç±»å‹ | ç»å¯¹å¥–åŠ± | ç›¸å¯¹å¥–åŠ± |
| ä»·å€¼ä¼°è®¡ | ä»·å€¼ç½‘ç»œ | ç»„å†…åŸºçº¿ |
| ä¼˜åŠ¿è®¡ç®— | A = R - V | A = (R_rel - Î¼)/Ïƒ |
| ç½‘ç»œæ•°é‡ | 2ä¸ªï¼ˆActor+Criticï¼‰ | 1ä¸ªï¼ˆPolicyï¼‰ |

## ä»£ç ç»“æ„ç‰¹ç‚¹

### 1. ç®€åŒ–çš„æ¶æ„
```python
# GRPOåªéœ€è¦ä¸‰ä¸ªæ¨¡å‹
self.policy_model    # ç­–ç•¥æ¨¡å‹ï¼ˆå¯è®­ç»ƒï¼‰
self.ref_model      # å‚è€ƒæ¨¡å‹ï¼ˆå›ºå®šï¼‰
self.reward_model   # å¥–åŠ±æ¨¡å‹ï¼ˆå›ºå®šï¼‰

# åªéœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨
self.optimizer = torch.optim.AdamW(self.policy_model.parameters())
```

### 2. æ‰¹é‡å¤„ç†ä¼˜åŒ–
```python
# é«˜æ•ˆçš„æ‰¹é‡ç”Ÿæˆ
outputs = self.policy_model.generate(
    num_return_sequences=GROUP_SIZE,  # æ‰¹é‡ç”Ÿæˆå¤šä¸ªå›å¤
    do_sample=True,
    temperature=0.7
)

# æ‰¹é‡å¥–åŠ±è®¡ç®—
rewards = self.compute_rewards(prompts_expanded, responses)
```

### 3. å†…å­˜ç®¡ç†
```python
# åˆ†é˜¶æ®µé‡Šæ”¾å†…å­˜
del inputs, outputs, gen_ids  # ç”Ÿæˆé˜¶æ®µåé‡Šæ”¾
torch.cuda.empty_cache()      # æ¸…ç†CUDAç¼“å­˜
gc.collect()                  # å¼ºåˆ¶åƒåœ¾å›æ”¶
```

## é…ç½®å‚æ•°

### æ ¸å¿ƒè¶…å‚æ•°
```python
BATCH_SIZE = 2              # æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜ä¼˜åŒ–ï¼‰
LEARNING_RATE = 1e-6        # å­¦ä¹ ç‡
GROUP_SIZE = 4              # æ¯ä¸ªpromptçš„å›å¤æ•°é‡
GRPO_EPOCHS = 4             # GRPOæ›´æ–°è½®æ•°
CLIP_RANGE = 0.2            # PPOè£å‰ªèŒƒå›´
KL_COEF = 0.01              # KLæ•£åº¦ç³»æ•°
```

### æ¨¡å‹é…ç½®
```python
POLICY_MODEL = r"E:\models\Qwen\Qwen3-0___6B"                    # ç­–ç•¥æ¨¡å‹
REWARD_MODEL = r"E:\models\reward-model-deberta-v3-large-v2"     # å¥–åŠ±æ¨¡å‹
```

### è®­ç»ƒé…ç½®
```python
NUM_EPOCHS = 1              # è®­ç»ƒè½®æ•°
DTYPE = torch.bfloat16      # æ•°æ®ç±»å‹
OUTPUT_DIR = "grpo_output"  # è¾“å‡ºç›®å½•
```

## ç®—æ³•ä¼˜åŠ¿

### 1. ç®€åŒ–çš„æ¶æ„
- **æ— éœ€ä»·å€¼ç½‘ç»œ**: å‡å°‘æ¨¡å‹å‚æ•°å’Œè®­ç»ƒå¤æ‚åº¦
- **å•ä¸€ä¼˜åŒ–å™¨**: ç®€åŒ–è®­ç»ƒè¿‡ç¨‹
- **æ›´å°‘çš„è¶…å‚æ•°**: å‡å°‘è°ƒå‚å·¥ä½œé‡

### 2. ç›¸å¯¹å¥–åŠ±æœºåˆ¶
- **å‡å°‘å¥–åŠ±åå·®**: ç»„å†…æ¯”è¾ƒæ¶ˆé™¤ç»å¯¹å¥–åŠ±çš„ç³»ç»Ÿæ€§åå·®
- **æé«˜æ ·æœ¬æ•ˆç‡**: æ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œæ¯”è¾ƒ
- **æ›´ç¨³å®šçš„è®­ç»ƒ**: ç›¸å¯¹å¥–åŠ±æ›´ç¨³å®š

### 3. æ˜¾å­˜å‹å¥½
- **æ›´å°‘çš„æ¨¡å‹**: åªéœ€è¦ä¸€ä¸ªå¯è®­ç»ƒçš„ç­–ç•¥æ¨¡å‹
- **æ‰¹é‡ä¼˜åŒ–**: é«˜æ•ˆçš„æ‰¹é‡ç”Ÿæˆå’Œè®¡ç®—
- **æ˜¾å­˜ç®¡ç†**: å®Œå–„çš„æ˜¾å­˜æ¸…ç†æœºåˆ¶

## é€‚ç”¨åœºæ™¯

### GRPOé€‚åˆçš„ä»»åŠ¡
- **é•¿æ–‡æœ¬ç”Ÿæˆ**: ç›¸å¯¹å¥–åŠ±æœºåˆ¶é€‚åˆé•¿æ–‡æœ¬
- **åˆ›æ„å†™ä½œ**: å¤šæ ·æ€§å›å¤çš„æ¯”è¾ƒå­¦ä¹ 
- **å¯¹è¯ç³»ç»Ÿ**: å¤šè½®å¯¹è¯çš„è´¨é‡æå‡
- **æ–‡æœ¬æ”¹å†™**: å¤šç§æ”¹å†™æ–¹æ¡ˆçš„æ¯”è¾ƒ

### GRPOçš„ä¼˜åŠ¿
- **è®­ç»ƒæ•ˆç‡é«˜**: Policy-Onlyæ¶æ„è®­ç»ƒæ›´å¿«
- **æ˜¾å­˜éœ€æ±‚ä½**: ç›¸æ¯”PPOéœ€è¦æ›´å°‘æ˜¾å­˜
- **å®ç°ç®€å•**: ä»£ç ç»“æ„æ›´ç®€æ´
- **æ•ˆæœç¨³å®š**: ç›¸å¯¹å¥–åŠ±æœºåˆ¶æ›´ç¨³å®š

### GRPOçš„å±€é™æ€§
- **éœ€è¦å¤šå›å¤**: å¿…é¡»ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤
- **ç»„å¤§å°æ•æ„Ÿ**: GROUP_SIZEçš„é€‰æ‹©å½±å“æ•ˆæœ
- **è®¡ç®—å¼€é”€**: éœ€è¦ç”Ÿæˆæ›´å¤šæ ·æœ¬
- **å¥–åŠ±æ¨¡å‹ä¾èµ–**: ä¸¥é‡ä¾èµ–å¥–åŠ±æ¨¡å‹çš„è´¨é‡

## è®­ç»ƒæŒ‡æ ‡

### è®°å½•çš„æŒ‡æ ‡
```python
self.metrics_history = {
    'loss': [],         # ç­–ç•¥æŸå¤±
    'reward': [],       # å¹³å‡å¥–åŠ±
    'entropy': []       # ç­–ç•¥ç†µ
}
```

### æŒ‡æ ‡å«ä¹‰
- **Loss**: GRPOçš„ç­–ç•¥æŸå¤±ï¼ŒåŒ…å«PPOæŸå¤±å’ŒKLæƒ©ç½š
- **Reward**: å¥–åŠ±æ¨¡å‹ç»™å‡ºçš„å¹³å‡å¥–åŠ±
- **Entropy**: ç­–ç•¥ç†µï¼Œåæ˜ ç”Ÿæˆçš„å¤šæ ·æ€§

### è®­ç»ƒç›‘æ§
```python
# å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
pbar.set_description(f"L:{metrics['loss']:.4f} R:{metrics['reward']:.2f} E:{metrics['entropy']:.3f}")
```

## å®ç°äº®ç‚¹

### 1. é«˜æ•ˆçš„ç»„å†…æ¯”è¾ƒ
```python
# åŒä¸€promptçš„å¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒ
responses, prompts_expanded = self.generate_responses(batch_prompts)
# prompts_expanded: ['q1','q1','q1','q1','q2','q2','q2','q2']
# responses:        ['a1','a2','a3','a4','b1','b2','b3','b4']

# è®¡ç®—ç›¸å¯¹å¥–åŠ±
rewards_grouped = rewards.view(num_groups, GROUP_SIZE)
relative_rewards = rewards_grouped - rewards_grouped.mean(dim=1, keepdim=True)
```

### 2. ç²¾ç¡®çš„Tokençº§åˆ«è®¡ç®—
```python
# åªåœ¨responseåŒºåŸŸè®¡ç®—æŸå¤±
mask = torch.zeros_like(labels, dtype=torch.bool)
for i, p_len in enumerate(prompt_lens):
    mask[i, p_len:] = (labels[i, p_len:] != tokenizer.pad_token_id)

# Tokençº§åˆ«çš„æŸå¤±è®¡ç®—
loss_map = (policy_loss + KL_COEF * kl_div) * mask
step_loss = loss_map.sum() / mask.sum()
```

### 3. å®Œå–„çš„æ˜¾å­˜ç®¡ç†
```python
# åˆ†é˜¶æ®µæ˜¾å­˜æ¸…ç†
def train_step(self, batch_prompts):
    # ... è®¡ç®—è¿‡ç¨‹ ...
    
    # æ˜¾å¼æ¸…ç†æ˜¾å­˜
    del new_log_probs, entropy, masked_entropy, log_ratio, ratio
    del surr1, surr2, policy_loss, kl_div, loss_map, step_loss
    
    self.optimizer.zero_grad(set_to_none=True)
    torch.cuda.empty_cache()
    gc.collect()
```

### 4. è‡ªåŠ¨åŒ–è®­ç»ƒç®¡ç†
```python
# è‡ªåŠ¨ä¿å­˜å’Œå¤‡ä»½
save_path = os.path.join(OUTPUT_DIR, f"epoch_{epoch+1}")
self.policy_model.save_pretrained(save_path)
self.tokenizer.save_pretrained(save_path)

# å¤‡ä»½è®­ç»ƒè„šæœ¬
current_script = os.path.abspath(__file__)
target_script = os.path.join(save_path, "train_script.py")
shutil.copy2(current_script, target_script)
```

## ä¸DAPOçš„å…³ç³»

GRPOæ˜¯DAPOçš„åŸºç¡€ç‰ˆæœ¬ï¼ŒDAPOåœ¨GRPOåŸºç¡€ä¸Šè¿›è¡Œäº†ä»¥ä¸‹æ”¹è¿›ï¼š

### GRPO â†’ DAPOçš„æ¼”è¿›
| ç‰¹æ€§ | GRPO | DAPO |
|------|------|------|
| è£å‰ªæ–¹å¼ | å¯¹ç§° [0.8, 1.2] | éå¯¹ç§° [0.8, 1.28] |
| æŸå¤±çº§åˆ« | Token-Level | Token-Level |
| KLæƒ©ç½š | ä½¿ç”¨ (0.01) | ç§»é™¤ (0.0) |
| åŠ¨æ€é‡‡æ · | âŒ | âœ… |
| è¿‡é•¿å¤„ç† | âŒ | âœ… |
| é€‚ç”¨åœºæ™¯ | é•¿æ–‡æœ¬ç”Ÿæˆ | é•¿é“¾æ¨ç† |

### å…±åŒç‰¹ç‚¹
- Policy-Onlyæ¶æ„
- ç›¸å¯¹å¥–åŠ±æœºåˆ¶
- Tokençº§åˆ«è®¡ç®—
- ç»„å†…æ¯”è¾ƒå­¦ä¹ 

## æ€»ç»“

GRPOå®ç°æä¾›äº†ä¸€ä¸ªç®€åŒ–è€Œé«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡Policy-Onlyæ¶æ„å’Œç›¸å¯¹å¥–åŠ±æœºåˆ¶ï¼ŒGRPOåœ¨ä¿è¯è®­ç»ƒæ•ˆæœçš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦å’Œæ˜¾å­˜éœ€æ±‚ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜¾å­˜ç®¡ç†å®Œå–„ï¼Œæ˜¯DAPOç®—æ³•çš„é‡è¦åŸºç¡€ã€‚