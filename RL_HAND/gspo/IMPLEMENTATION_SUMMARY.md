# GSPOå®žçŽ°æ€»ç»“

## å®Œæˆçš„å·¥ä½œ

### 1. æ ¸å¿ƒç®—æ³•å®žçŽ° (`gspo.py`)

å®žçŽ°äº†å®Œæ•´çš„GSPO (Group Sequence Policy Optimization) è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

#### ðŸ”¥ Group Samplingï¼ˆç»„é‡‡æ ·ï¼‰
```python
def generate_responses_with_group_sampling(self, prompts: List[str]):
    """ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›žå¤"""
    for prompt in prompts:
        for _ in range(GROUP_SIZE):
            # ç”Ÿæˆä¸€ä¸ªå›žå¤
            outputs = self.policy_model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=True,  # ðŸ”¥ å…³é”®ï¼šå¯ç”¨é‡‡æ ·ç¡®ä¿å¤šæ ·æ€§
                temperature=0.7,
                num_return_sequences=1
            )
            responses.append(response)
            prompts_expanded.append(prompt)
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- å¯¹æ¯ä¸ªpromptç”ŸæˆKä¸ªä¸åŒçš„å›žå¤ï¼ˆK=GROUP_SIZEï¼‰
- é€šè¿‡é‡‡æ ·ï¼ˆdo_sample=Trueï¼‰ç¡®ä¿å›žå¤çš„å¤šæ ·æ€§
- è¿™äº›å›žå¤æž„æˆä¸€ä¸ª"ç»„"ï¼Œç”¨äºŽç»„å†…æ¯”è¾ƒ

#### ðŸ”¥ Sequence-Level Rewardsï¼ˆåºåˆ—çº§å¥–åŠ±ï¼‰
```python
def compute_rewards(self, prompts: List[str], responses: List[str]):
    """è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±"""
    for p, r in zip(prompts, responses):
        full_text = f"{p} {r}"  # ðŸ”¥ å®Œæ•´åºåˆ—ä½œä¸ºè¾“å…¥
        inputs = self.reward_tokenizer(full_text, return_tensors="pt")
        reward = self.reward_model(**inputs).logits[0, 0]  # ðŸ”¥ æ ‡é‡å¥–åŠ±
        rewards.append(reward)
    return torch.stack(rewards)
```

**ä¸Žtoken-levelçš„åŒºåˆ«**ï¼š
- Sequence-levelï¼šä¸€ä¸ªåºåˆ—ä¸€ä¸ªå¥–åŠ±å€¼
- Token-levelï¼šæ¯ä¸ªtokenä¸€ä¸ªå¥–åŠ±å€¼

#### ðŸ”¥ Relative Advantageï¼ˆç›¸å¯¹ä¼˜åŠ¿ï¼‰
```python
def compute_relative_advantages(self, rewards: torch.Tensor, group_size: int):
    """è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿"""
    # é‡å¡‘ä¸ºç»„çš„å½¢çŠ¶ [num_groups, group_size]
    rewards_grouped = rewards.view(-1, group_size)
    
    # ðŸ”¥ è®¡ç®—ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿
    group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
    
    # ðŸ”¥ è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
    if ADVANTAGE_TYPE == "relative":
        relative_advantages = rewards_grouped - group_baselines
    elif ADVANTAGE_TYPE == "normalized":
        relative_advantages = rewards_grouped - group_baselines
        if USE_GROUP_NORMALIZATION:
            group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
            relative_advantages = relative_advantages / group_std
    
    return relative_advantages.view(-1), group_baselines.repeat(1, group_size).view(-1)
```

**æ•°å­¦è¡¨è¾¾**ï¼š
```
A_ij = R_ij - mean(R_i)  # ç›¸å¯¹ä¼˜åŠ¿
A_ij = (R_ij - mean(R_i)) / std(R_i)  # æ ‡å‡†åŒ–ç‰ˆæœ¬
```

#### ðŸ”¥ çµæ´»çš„ç­–ç•¥ä¼˜åŒ–
```python
def compute_policy_loss(self, log_probs, old_log_probs, advantages, kl_penalty,
                       token_log_probs_list=None, old_token_log_probs_list=None):
    """æ”¯æŒåºåˆ—çº§å’Œtokençº§ä¼˜åŒ–"""
    # ðŸ”¥ é€‰æ‹©ä¼˜åŒ–çº§åˆ«
    if USE_TOKEN_LEVEL_LOSS and token_log_probs_list is not None:
        policy_loss = self.compute_policy_loss_token_level(
            token_log_probs_list, old_token_log_probs_list, advantages
        )
    else:
        policy_loss = self.compute_policy_loss_sequence_level(
            log_probs, old_log_probs, advantages
        )
    
    # ç†µæŸå¤±å’ŒKLæŸå¤±
    entropy_loss = -ENTROPY_COEF * (-log_probs.mean())
    kl_loss = self.kl_coef * kl_penalty.mean()
    
    return policy_loss, entropy_loss, kl_loss
```

#### ðŸ”¥ è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´
```python
def update_kl_coef(self, kl_divergence: torch.Tensor):
    """è‡ªé€‚åº”è°ƒæ•´KLæ•£åº¦ç³»æ•°"""
    if not ADAPTIVE_KL:
        return
    
    mean_kl = kl_divergence.mean().item()
    
    # ðŸ”¥ åŠ¨æ€è°ƒæ•´ç­–ç•¥
    if mean_kl > 2.0 * TARGET_KL:
        self.kl_coef *= 1.5  # KLè¿‡å¤§ï¼Œå¢žå¤§æƒ©ç½š
    elif mean_kl < 0.5 * TARGET_KL:
        self.kl_coef *= 0.5  # KLè¿‡å°ï¼Œå‡å°æƒ©ç½š
    
    # é™åˆ¶èŒƒå›´
    self.kl_coef = max(0.01, min(self.kl_coef, 1.0))
```

### 2. åŒGPUæž¶æž„è®¾è®¡

```python
# GPUè®¾å¤‡åˆ†é…ç­–ç•¥ï¼ˆä¸Žå…¶ä»–ç®—æ³•ä¸€è‡´ï¼‰
self.device_policy = torch.device("cuda:0")    # ç­–ç•¥æ¨¡åž‹
self.device_ref = torch.device("cuda:1")       # å‚è€ƒæ¨¡åž‹
self.device_reward = torch.device("cuda:1")    # å¥–åŠ±æ¨¡åž‹
```

### 3. æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

```python
def train_step(self, batch_prompts):
    # ... è®­ç»ƒé€»è¾‘ ...
    
    # ðŸ”¥ æ˜¾å¼æ¸…ç†æ˜¾å­˜
    del new_log_probs, policy_loss, entropy_loss, kl_loss, total_loss
    if new_token_log_probs_list:
        del new_token_log_probs_list
    
    self.optimizer.zero_grad(set_to_none=True)
    torch.cuda.empty_cache()
    gc.collect()
```

### 4. å¯è§†åŒ–å·¥å…·é›†æˆ

#### GSPOä¸“ç”¨æŒ‡æ ‡å›¾è¡¨
```python
def plot_gspo_metrics(policy_losses, entropy_losses, kl_losses, rewards, 
                     relative_advantages, kl_divergences, kl_coefs, avg_response_lengths):
    """ç»˜åˆ¶GSPOçš„8ä¸ªæ ¸å¿ƒæŒ‡æ ‡"""
    # 4x2å¸ƒå±€ï¼ŒåŒ…å«GSPOç‰¹æœ‰çš„æŒ‡æ ‡
    # - ç›¸å¯¹ä¼˜åŠ¿ï¼ˆç»„å†…åŸºçº¿ï¼‰
    # - KLæ•£åº¦å€¼
    # - è‡ªé€‚åº”KLç³»æ•°
    # - å¹³å‡å›žå¤é•¿åº¦
```

## ç®—æ³•åŽŸç†æ·±åº¦åˆ†æž

### GSPOçš„æ ¸å¿ƒåˆ›æ–°

#### 1. Group Samplingçš„ä¼˜åŠ¿
```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ªpromptä¸€ä¸ªå›žå¤
responses = [generate(prompt) for prompt in prompts]  # [B]

# GSPOæ–¹æ³•ï¼šæ¯ä¸ªpromptå¤šä¸ªå›žå¤
responses = []
for prompt in prompts:
    for _ in range(GROUP_SIZE):
        responses.append(generate(prompt))  # [B * GROUP_SIZE]
```

**ä¼˜åŠ¿åˆ†æž**ï¼š
- **æ›´ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·**ï¼šåŒä¸€promptçš„å¤šä¸ªå›žå¤æä¾›å†…åœ¨å¯¹æ¯”
- **å‡å°‘æ–¹å·®**ï¼šç»„å†…æ¯”è¾ƒå‡å°‘å•ä¸ªæ ·æœ¬çš„éšæœºæ€§å½±å“
- **æ›´ç¨³å®šçš„åŸºçº¿**ï¼šç»„å†…å‡å€¼æ¯”å•ç‚¹ä¼°è®¡æ›´ç¨³å®š

#### 2. Relative Advantage vs Absolute Reward

**ä¼ ç»ŸPPO**ï¼š
```python
advantages = rewards - values  # éœ€è¦criticç½‘ç»œä¼°è®¡values
```

**GSPO**ï¼š
```python
# ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼Œæ— éœ€criticç½‘ç»œ
group_mean = rewards.view(-1, GROUP_SIZE).mean(dim=1, keepdim=True)
advantages = rewards - group_mean.repeat(1, GROUP_SIZE).view(-1)
```

**æ•°å­¦å¯¹æ¯”**ï¼š

| æ–¹æ³• | ä¼˜åŠ¿è®¡ç®— | åŸºçº¿æ¥æº | ç½‘ç»œéœ€æ±‚ |
|------|----------|----------|----------|
| PPO | A = R - V(s) | Criticç½‘ç»œ | Actor + Critic |
| GSPO | A = R - R_group_mean | ç»„å†…å‡å€¼ | Policy Only |

#### 3. åºåˆ—çº§ vs Tokençº§ä¼˜åŒ–

**åºåˆ—çº§ä¼˜åŒ–**ï¼ˆGSPOé»˜è®¤ï¼‰ï¼š
```python
# æ•´ä¸ªåºåˆ—ä¸€ä¸ªlog_probï¼Œä¸€ä¸ªä¼˜åŠ¿å€¼
ratio = torch.exp(log_probs - old_log_probs)  # [B]
surr1 = ratio * advantages  # [B]
policy_loss = -torch.min(surr1, surr2).mean()
```

**Tokençº§ä¼˜åŒ–**ï¼ˆGSPOå¯é€‰ï¼‰ï¼š
```python
# æ¯ä¸ªtokenä¸€ä¸ªlog_probï¼Œä½†ä½¿ç”¨åºåˆ—çº§ä¼˜åŠ¿
for i, (token_log_probs, old_token_log_probs) in enumerate(...):
    advantage = advantages[i]  # åºåˆ—çº§ä¼˜åŠ¿
    token_ratios = torch.exp(token_log_probs - old_token_log_probs)  # [seq_len]
    token_loss = -torch.min(token_ratios * advantage, ...).sum()
```

### è®­ç»ƒæµç¨‹è¯¦è§£

#### å®Œæ•´çš„GSPOè®­ç»ƒæ­¥éª¤

```python
def train_step(self, batch_prompts):
    # ðŸ”¥ 1. Group Sampling
    # Input: ["é—®é¢˜1", "é—®é¢˜2"]  # batch_size=2
    # Output: ["å›žå¤1-1", "å›žå¤1-2", "å›žå¤1-3", "å›žå¤1-4",  # é—®é¢˜1çš„4ä¸ªå›žå¤
    #          "å›žå¤2-1", "å›žå¤2-2", "å›žå¤2-3", "å›žå¤2-4"]  # é—®é¢˜2çš„4ä¸ªå›žå¤
    responses, prompts_expanded, lengths = self.generate_responses_with_group_sampling(batch_prompts)
    
    # ðŸ”¥ 2. Sequence-Level Rewards
    # Input: 8ä¸ª(prompt, response)å¯¹
    # Output: [r1_1, r1_2, r1_3, r1_4, r2_1, r2_2, r2_3, r2_4]
    raw_rewards = self.compute_rewards(prompts_expanded, responses)
    
    # ðŸ”¥ 3. Relative Advantage
    # Group 1: [r1_1, r1_2, r1_3, r1_4] -> mean1 = (r1_1+r1_2+r1_3+r1_4)/4
    # Group 2: [r2_1, r2_2, r2_3, r2_4] -> mean2 = (r2_1+r2_2+r2_3+r2_4)/4
    # Advantages: [r1_1-mean1, r1_2-mean1, r1_3-mean1, r1_4-mean1,
    #              r2_1-mean2, r2_2-mean2, r2_3-mean2, r2_4-mean2]
    relative_advantages, group_baselines = self.compute_relative_advantages(raw_rewards)
    
    # ðŸ”¥ 4. Policy Optimization
    for _ in range(GSPO_EPOCHS):
        # é‡æ–°è®¡ç®—logæ¦‚çŽ‡
        new_log_probs = self.compute_log_probs(prompts, responses)
        
        # PPOæŸå¤±
        ratio = torch.exp(new_log_probs - old_log_probs)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1-Îµ, 1+Îµ) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # æ›´æ–°ç­–ç•¥
        policy_loss.backward()
        optimizer.step()
    
    # ðŸ”¥ 5. Adaptive KL
    self.update_kl_coef(kl_penalty)
```

## ä»£ç ç»“æž„ç‰¹ç‚¹

### 1. æ¨¡å—åŒ–è®¾è®¡
```python
class GSPOTrainer:
    def generate_responses_with_group_sampling(self):  # ç»„é‡‡æ ·
    def compute_rewards(self):                         # åºåˆ—çº§å¥–åŠ±
    def compute_relative_advantages(self):             # ç›¸å¯¹ä¼˜åŠ¿
    def compute_policy_loss_sequence_level(self):      # åºåˆ—çº§æŸå¤±
    def compute_policy_loss_token_level(self):         # Tokençº§æŸå¤±
    def update_kl_coef(self):                         # è‡ªé€‚åº”KL
```

### 2. é…ç½®å‚æ•°åŒ–
```python
# æ‰€æœ‰å…³é”®å‚æ•°éƒ½å¯é…ç½®
GROUP_SIZE = 4                      # ç»„å¤§å°
ADVANTAGE_TYPE = "relative"         # ä¼˜åŠ¿ç±»åž‹
USE_GROUP_NORMALIZATION = True      # ç»„å†…æ ‡å‡†åŒ–
USE_SEQUENCE_LEVEL_REWARD = True    # åºåˆ—çº§å¥–åŠ±
USE_TOKEN_LEVEL_LOSS = False        # Tokençº§æŸå¤±
ADAPTIVE_KL = True                  # è‡ªé€‚åº”KL
```

### 3. ä¸Žå…¶ä»–ç®—æ³•çš„ä¸€è‡´æ€§
- ç›¸åŒçš„æ–‡ä»¶å¤´æ³¨é‡Šæ ¼å¼
- ç›¸åŒçš„GPUè®¾å¤‡åˆ†é…ç­–ç•¥
- ç›¸åŒçš„æ˜¾å­˜ç®¡ç†æœºåˆ¶
- ç›¸åŒçš„æ¨¡åž‹ä¿å­˜å’Œè„šæœ¬å¤‡ä»½
- ç›¸åŒçš„æŒ‡æ ‡è®°å½•å’Œå¯è§†åŒ–

## ç®—æ³•å¯¹æ¯”åˆ†æž

### GSPO vs PPO

| ç‰¹æ€§ | PPO | GSPO |
|------|-----|------|
| **æž¶æž„** | Actor-Critic | Policy-Only |
| **åŸºçº¿ä¼°è®¡** | Criticç½‘ç»œV(s) | ç»„å†…å‡å€¼ |
| **é‡‡æ ·ç­–ç•¥** | å•å›žå¤ | ç»„é‡‡æ ·ï¼ˆå¤šå›žå¤ï¼‰ |
| **ä¼˜åŠ¿è®¡ç®—** | GAE | ç›¸å¯¹ä¼˜åŠ¿ |
| **ç½‘ç»œæ•°é‡** | 2ä¸ª | 1ä¸ª |
| **è®­ç»ƒå¤æ‚åº¦** | é«˜ | ä¸­ç­‰ |
| **æ˜¾å­˜éœ€æ±‚** | é«˜ | ä¸­ç­‰ |
| **é€‚ç”¨åœºæ™¯** | é€šç”¨ä»»åŠ¡ | å¤šæ ·æ€§ä»»åŠ¡ |

### GSPO vs GRPO

| ç‰¹æ€§ | GRPO | GSPO |
|------|------|------|
| **ç»„é‡‡æ ·** | âœ… | âœ… |
| **ç›¸å¯¹å¥–åŠ±** | âœ… | âœ… |
| **åºåˆ—çº§ä¼˜åŒ–** | âœ… | âœ… |
| **Tokençº§ä¼˜åŒ–** | âŒ | âœ…ï¼ˆå¯é€‰ï¼‰ |
| **è‡ªé€‚åº”KL** | âŒ | âœ… |
| **å¥–åŠ±å¡‘å½¢** | åŸºç¡€ | å¢žå¼º |
| **è¶…å‚æ•°** | è¾ƒå°‘ | è¾ƒå¤š |
| **çµæ´»æ€§** | ä¸­ç­‰ | é«˜ |

### GSPO vs DAPO

| ç‰¹æ€§ | DAPO | GSPO |
|------|------|------|
| **è£å‰ªæ–¹å¼** | éžå¯¹ç§° [0.8, 1.28] | å¯¹ç§° [0.8, 1.2] |
| **KLæƒ©ç½š** | ç§»é™¤ (0.0) | è‡ªé€‚åº” (0.01-1.0) |
| **åŠ¨æ€é‡‡æ ·** | âœ… | âŒ |
| **ç»„é‡‡æ ·** | âœ… | âœ… |
| **Tokençº§æŸå¤±** | âœ… | âœ…ï¼ˆå¯é€‰ï¼‰ |
| **è¿‡é•¿å¤„ç†** | âœ… | âŒ |
| **é€‚ç”¨åœºæ™¯** | é•¿é“¾æŽ¨ç† | å¤æ‚æŽ¨ç†+å¤šæ ·æ€§ |

## é…ç½®å‚æ•°è¯¦è§£

### æ ¸å¿ƒè¶…å‚æ•°

#### Group Samplingå‚æ•°
```python
GROUP_SIZE = 4              # æ¯ä¸ªpromptç”Ÿæˆçš„å›žå¤æ•°é‡
```
**å½±å“**ï¼š
- æ›´å¤§çš„GROUP_SIZEï¼šæ›´ç¨³å®šçš„åŸºçº¿ä¼°è®¡ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜
- æ›´å°çš„GROUP_SIZEï¼šè®¡ç®—æ›´å¿«ï¼Œä½†åŸºçº¿ä¼°è®¡å¯èƒ½ä¸ç¨³å®š
- **æŽ¨èå€¼**ï¼š3-6

#### ä¼˜åŠ¿è®¡ç®—å‚æ•°
```python
ADVANTAGE_TYPE = "relative"         # "relative" æˆ– "normalized"
USE_GROUP_NORMALIZATION = True      # ç»„å†…æ ‡å‡†åŒ–
```
**ADVANTAGE_TYPE**ï¼š
- "relative"ï¼šç®€å•çš„ç›¸å¯¹ä¼˜åŠ¿ï¼ˆreward - baselineï¼‰
- "normalized"ï¼šæ ‡å‡†åŒ–çš„ç›¸å¯¹ä¼˜åŠ¿ï¼ˆ(reward - baseline) / stdï¼‰

#### è‡ªé€‚åº”KLå‚æ•°
```python
KL_COEF = 0.2               # åˆå§‹KLæ•£åº¦ç³»æ•°
TARGET_KL = 0.01            # ç›®æ ‡KLæ•£åº¦
ADAPTIVE_KL = True          # è‡ªé€‚åº”è°ƒæ•´
```

### è®­ç»ƒæŒ‡æ ‡

#### è®°å½•çš„æŒ‡æ ‡
```python
self.metrics_history = {
    'policy_loss': [],          # ç­–ç•¥æŸå¤±
    'entropy_loss': [],         # ç†µæŸå¤±
    'kl_loss': [],             # KLæŸå¤±
    'reward': [],              # å¹³å‡å¥–åŠ±
    'relative_advantage': [],   # ç›¸å¯¹ä¼˜åŠ¿
    'kl_divergence': [],       # KLæ•£åº¦å€¼
    'kl_coef': [],             # KLç³»æ•°
    'avg_response_length': []   # å¹³å‡å›žå¤é•¿åº¦
}
```

#### æŒ‡æ ‡å«ä¹‰
- **Policy Loss**: ç­–ç•¥ç½‘ç»œçš„æŸå¤±ï¼Œåæ˜ ç­–ç•¥æ›´æ–°å¹…åº¦
- **Relative Advantage**: ç›¸å¯¹ä¼˜åŠ¿å€¼ï¼Œåº”è¯¥å›´ç»•0æ³¢åŠ¨
- **KL Divergence**: ä¸Žå‚è€ƒç­–ç•¥çš„KLæ•£åº¦ï¼Œåº”è¯¥åœ¨TARGET_KLé™„è¿‘
- **KL Coefficient**: è‡ªé€‚åº”è°ƒæ•´çš„KLç³»æ•°

## ä½¿ç”¨åœºæ™¯åˆ†æž

### GSPOé€‚åˆçš„ä»»åŠ¡

#### 1. éœ€è¦å¤šæ ·æ€§çš„ç”Ÿæˆä»»åŠ¡
```python
# åˆ›æ„å†™ä½œï¼šéœ€è¦å¤šç§ä¸åŒçš„åˆ›æ„æ–¹å‘
prompts = ["å†™ä¸€ä¸ªå…³äºŽæ—¶é—´æ—…è¡Œçš„æ•…äº‹"]
# GSPOä¼šç”Ÿæˆå¤šä¸ªä¸åŒè§’åº¦çš„æ•…äº‹ï¼Œé€šè¿‡ç»„å†…æ¯”è¾ƒå­¦ä¹ 
```

#### 2. å¤æ‚æŽ¨ç†ä»»åŠ¡
```python
# æ•°å­¦é—®é¢˜ï¼šå¯èƒ½æœ‰å¤šç§è§£æ³•
prompts = ["è§£è¿™ä¸ªæ–¹ç¨‹ï¼šx^2 + 5x + 6 = 0"]
# GSPOä¼šå°è¯•ä¸åŒçš„è§£æ³•ï¼Œå­¦ä¹ å“ªç§æ›´å¥½
```

#### 3. å¯¹è¯ç³»ç»Ÿ
```python
# å¯¹è¯å›žå¤ï¼šéœ€è¦è€ƒè™‘å¤šç§å›žå¤é£Žæ ¼
prompts = ["ç”¨æˆ·è¯´ï¼šæˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¥½"]
# GSPOä¼šç”Ÿæˆå¤šç§å®‰æ…°æ–¹å¼ï¼Œå­¦ä¹ æœ€åˆé€‚çš„å›žå¤
```

### GSPOçš„ä¼˜åŠ¿

#### 1. æ— éœ€Criticç½‘ç»œ
- **ç®€åŒ–æž¶æž„**ï¼šåªéœ€è®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œ
- **å‡å°‘å‚æ•°**ï¼šç›¸æ¯”PPOå‡å°‘çº¦50%çš„å‚æ•°é‡
- **è®­ç»ƒç¨³å®š**ï¼šé¿å…Actor-Criticçš„ä¸ç¨³å®šé—®é¢˜

#### 2. ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·
- **ç»„å†…æ¯”è¾ƒ**ï¼šåŒä¸€promptçš„å¤šä¸ªå›žå¤æä¾›ç›´æŽ¥å¯¹æ¯”
- **ç›¸å¯¹è¯„ä¼°**ï¼šå…³æ³¨ç›¸å¯¹å¥½åè€Œéžç»å¯¹åˆ†æ•°
- **å‡å°‘åå·®**ï¼šç»„å†…åŸºçº¿æ¶ˆé™¤å¥–åŠ±æ¨¡åž‹çš„ç³»ç»Ÿæ€§åå·®

#### 3. çµæ´»çš„ä¼˜åŒ–ç­–ç•¥
- **å¤šçº§åˆ«ä¼˜åŒ–**ï¼šæ”¯æŒåºåˆ—çº§å’Œtokençº§
- **è‡ªé€‚åº”è°ƒæ•´**ï¼šKLç³»æ•°æ ¹æ®è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒæ•´
- **å¯é…ç½®æ€§**ï¼šå¤šç§ä¼˜åŠ¿è®¡ç®—æ–¹å¼å¯é€‰

### GSPOçš„å±€é™æ€§

#### 1. è®¡ç®—å¼€é”€
```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šBæ¬¡ç”Ÿæˆ
responses = [generate(prompt) for prompt in prompts]

# GSPOï¼šB * GROUP_SIZEæ¬¡ç”Ÿæˆ
responses = []
for prompt in prompts:
    for _ in range(GROUP_SIZE):
        responses.append(generate(prompt))
```
**å½±å“**ï¼šè®¡ç®—æ—¶é—´å¢žåŠ GROUP_SIZEå€

#### 2. å†…å­˜éœ€æ±‚
```python
# éœ€è¦åŒæ—¶å­˜å‚¨GROUP_SIZEå€çš„æ•°æ®
batch_size_effective = BATCH_SIZE * GROUP_SIZE
```

#### 3. è¶…å‚æ•°æ•æ„Ÿæ€§
- GROUP_SIZEçš„é€‰æ‹©å½±å“æ€§èƒ½
- éœ€è¦è°ƒæ•´å¤šä¸ªKLç›¸å…³å‚æ•°
- ä¼˜åŠ¿è®¡ç®—æ–¹å¼éœ€è¦æ ¹æ®ä»»åŠ¡é€‰æ‹©

## å®žçŽ°äº®ç‚¹

### 1. é«˜æ•ˆçš„ç»„é‡‡æ ·
```python
def generate_responses_with_group_sampling(self, prompts):
    """é«˜æ•ˆçš„æ‰¹é‡ç»„é‡‡æ ·"""
    for prompt in prompts:
        for _ in range(GROUP_SIZE):
            # å•æ¬¡ç”Ÿæˆï¼Œé¿å…æ‰¹é‡ç”Ÿæˆçš„å¤æ‚æ€§
            response = self.policy_model.generate(...)
            all_responses.append(response)
            all_prompts.append(prompt)
```

### 2. çµæ´»çš„ä¼˜åŠ¿è®¡ç®—
```python
def compute_relative_advantages(self, rewards, group_size):
    """æ”¯æŒå¤šç§ä¼˜åŠ¿è®¡ç®—æ–¹å¼"""
    rewards_grouped = rewards.view(-1, group_size)
    group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
    
    if ADVANTAGE_TYPE == "relative":
        advantages = rewards_grouped - group_baselines
    elif ADVANTAGE_TYPE == "normalized":
        advantages = (rewards_grouped - group_baselines) / (rewards_grouped.std(dim=1, keepdim=True) + 1e-8)
    
    return advantages.view(-1)
```

### 3. è‡ªé€‚åº”KLæœºåˆ¶
```python
def update_kl_coef(self, kl_divergence):
    """æ™ºèƒ½çš„KLç³»æ•°è°ƒæ•´"""
    mean_kl = kl_divergence.mean().item()
    
    # æ ¹æ®KLæ•£åº¦åŠ¨æ€è°ƒæ•´
    if mean_kl > 2.0 * TARGET_KL:
        self.kl_coef *= 1.5  # è¿‡å¤§æ—¶å¢žå¤§æƒ©ç½š
    elif mean_kl < 0.5 * TARGET_KL:
        self.kl_coef *= 0.5  # è¿‡å°æ—¶å‡å°æƒ©ç½š
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    self.kl_coef = max(0.01, min(self.kl_coef, 1.0))
```

### 4. å®Œå–„çš„æ˜¾å­˜ç®¡ç†
```python
def train_step(self, batch_prompts):
    # ... è®­ç»ƒé€»è¾‘ ...
    
    # åˆ†é˜¶æ®µé‡Šæ”¾æ˜¾å­˜
    del new_log_probs, policy_loss, entropy_loss, kl_loss, total_loss
    if new_token_log_probs_list:
        del new_token_log_probs_list
    
    # å½»åº•æ¸…ç†
    self.optimizer.zero_grad(set_to_none=True)
    torch.cuda.empty_cache()
    gc.collect()
```

## æ€»ç»“

GSPOå®žçŽ°æä¾›äº†ä¸€ä¸ªçµæ´»è€Œé«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–æ¡†æž¶ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦å¤šæ ·æ€§å’Œå¤æ‚æŽ¨ç†çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡ç»„é‡‡æ ·å’Œç›¸å¯¹ä¼˜åŠ¿æœºåˆ¶ï¼ŒGSPOåœ¨æ— éœ€criticç½‘ç»œçš„æƒ…å†µä¸‹å®žçŽ°äº†ç¨³å®šçš„ç­–ç•¥ä¼˜åŒ–ã€‚è‡ªé€‚åº”KLè°ƒæ•´å’Œçµæ´»çš„ä¼˜åŒ–ç­–ç•¥ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚ä»£ç ç»“æž„æ¸…æ™°ï¼Œå‚æ•°å¯é…ç½®æ€§å¼ºï¼Œå…·æœ‰è‰¯å¥½çš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚