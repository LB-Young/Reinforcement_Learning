# GSPO å¤šGPUè®­ç»ƒæŒ‡å—

## ğŸ“– æ¦‚è¿°

`gspo_multigpu.py` æ˜¯ GSPO çš„å¤šGPUä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸“é—¨è§£å†³å•GPUæ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰çš„é—®é¢˜ã€‚

### GPU åˆ†é…ç­–ç•¥

```
GPU 0 + GPU 1: Policy Model (æ¨¡å‹å¹¶è¡Œ)
GPU 2:         Reference Model + Reward Model
```

### ä¸ºä»€ä¹ˆéœ€è¦å¤šGPUï¼Ÿ

**å•GPUé—®é¢˜**ï¼š
- âŒ Policy æ¨¡å‹åœ¨è®­ç»ƒæ—¶éœ€è¦å­˜å‚¨æ¢¯åº¦
- âŒ ç”Ÿæˆå¤šä¸ªå›å¤æ—¶æ˜¾å­˜å ç”¨æ¿€å¢
- âŒ å®¹æ˜“å‡ºç° OOM (Out of Memory)

**å¤šGPUä¼˜åŠ¿**ï¼š
- âœ… Policy æ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ªGPUï¼Œæ˜¾å­˜å‹åŠ›å‡åŠ
- âœ… Reference å’Œ Reward æ¨¡å‹ç‹¬ç«‹è¿è¡Œï¼Œäº’ä¸å¹²æ‰°
- âœ… å¯ä»¥å¢å¤§ BATCH_SIZE å’Œ GROUP_SIZE

---

## ğŸ”§ é…ç½®è¯´æ˜

### GPU é…ç½®ï¼ˆ3å¼ GPUï¼‰

```python
# æ¨èé…ç½®ï¼ˆ3å¼ GPUï¼‰
POLICY_GPU_IDS = [0, 1]      # Policy ä½¿ç”¨ GPU 0 å’Œ 1
REFERENCE_GPU_ID = 2         # Reference ä½¿ç”¨ GPU 2
REWARD_GPU_ID = 2            # Reward ä½¿ç”¨ GPU 2
```

**æ˜¾å­˜åˆ†é…**ï¼š
- GPU 0: ~8-10GB (Policy ä¸€åŠ)
- GPU 1: ~8-10GB (Policy ä¸€åŠ)
- GPU 2: ~6-8GB (Reference + Reward)

### GPU é…ç½®ï¼ˆ2å¼ GPUï¼‰

```python
# 2å¼ GPUé…ç½®
POLICY_GPU_IDS = [0, 1]      # Policy ä½¿ç”¨ GPU 0 å’Œ 1
REFERENCE_GPU_ID = 1         # Reference ä½¿ç”¨ GPU 1
REWARD_GPU_ID = 1            # Reward ä½¿ç”¨ GPU 1
```

**æ˜¾å­˜åˆ†é…**ï¼š
- GPU 0: ~8-10GB (Policy ä¸€åŠ)
- GPU 1: ~12-14GB (Policy ä¸€åŠ + Reference + Reward)

âš ï¸ **æ³¨æ„**ï¼š2å¼ GPUé…ç½®ä¸‹ï¼ŒGPU 1 çš„æ˜¾å­˜å‹åŠ›è¾ƒå¤§

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. æ£€æŸ¥GPU

```bash
# æŸ¥çœ‹GPUä¿¡æ¯
nvidia-smi

# ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPU
python -c "import torch; print(f'å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}')"
```

### 2. ä¿®æ”¹é…ç½®

æ ¹æ®ä½ çš„GPUæ•°é‡ä¿®æ”¹é…ç½®ï¼š

```python
# åœ¨ gspo_multigpu.py ä¸­ä¿®æ”¹

# 3å¼ GPUï¼ˆæ¨èï¼‰
POLICY_GPU_IDS = [0, 1]
REFERENCE_GPU_ID = 2
REWARD_GPU_ID = 2

# æˆ– 2å¼ GPU
POLICY_GPU_IDS = [0, 1]
REFERENCE_GPU_ID = 1
REWARD_GPU_ID = 1
```

### 3. è¿è¡Œè®­ç»ƒ

```bash
cd RL_HAND/gspo
python gspo_multigpu.py
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å•GPU vs å¤šGPU

| æŒ‡æ ‡ | å•GPU (gspo.py) | å¤šGPU (gspo_multigpu.py) |
|------|----------------|-------------------------|
| **æœ€å¤§ BATCH_SIZE** | 1-2 | 4-8 |
| **æœ€å¤§ GROUP_SIZE** | 2-4 | 4-8 |
| **OOM é£é™©** | é«˜ | ä½ |
| **è®­ç»ƒé€Ÿåº¦** | åŸºçº¿ | 1.2-1.5x |
| **æ˜¾å­˜åˆ©ç”¨ç‡** | å•å¡æ»¡è½½ | å¤šå¡å‡è¡¡ |

### æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”

**å•GPU (16GB)**:
```
GPU 0: 15.5GB / 16GB (97%)  â† å®¹æ˜“OOM
GPU 1: 0GB / 16GB (0%)      â† é—²ç½®
GPU 2: 0GB / 16GB (0%)      â† é—²ç½®
```

**å¤šGPU (3x16GB)**:
```
GPU 0: 9GB / 16GB (56%)     â† Policy ä¸€åŠ
GPU 1: 9GB / 16GB (56%)     â† Policy ä¸€åŠ
GPU 2: 7GB / 16GB (44%)     â† Reference + Reward
```

---

## ğŸ” æ ¸å¿ƒæ”¹è¿›

### 1. DataParallel æ¨¡å‹å¹¶è¡Œ

```python
# å•GPU
self.policy_model = AutoModelForCausalLM.from_pretrained(
    POLICY_MODEL, device_map={"": "cuda:0"}
)

# å¤šGPU
policy_model_single = AutoModelForCausalLM.from_pretrained(
    POLICY_MODEL, device_map={"": "cuda:0"}
)
self.policy_model = nn.DataParallel(
    policy_model_single,
    device_ids=[0, 1],      # ä½¿ç”¨ GPU 0 å’Œ 1
    output_device=0         # è¾“å‡ºåˆ° GPU 0
)
```

### 2. æ¨¡å‹ä¿å­˜å¤„ç†

```python
# ä¿å­˜æ—¶ä½¿ç”¨æœªåŒ…è£…çš„æ¨¡å‹
self.policy_model_unwrapped = policy_model_single

# ä¿å­˜
self.policy_model_unwrapped.save_pretrained(save_path)
```

### 3. è®¾å¤‡ç®¡ç†

```python
# æ˜ç¡®æŒ‡å®šæ¯ä¸ªæ¨¡å‹çš„è®¾å¤‡
self.policy_device = torch.device("cuda:0")      # Policy ä¸»è®¾å¤‡
self.device_ref = torch.device("cuda:2")         # Reference
self.device_reward = torch.device("cuda:2")      # Reward
```

---

## âš™ï¸ å‚æ•°è°ƒä¼˜

### æ˜¾å­˜å……è¶³æ—¶ï¼ˆ3x16GB+ï¼‰

```python
BATCH_SIZE = 4              # å¢å¤§æ‰¹æ¬¡
GROUP_SIZE = 8              # å¢å¤§ç»„å¤§å°
GSPO_EPOCHS = 4             # ä¿æŒä¸å˜
```

### æ˜¾å­˜ç´§å¼ æ—¶ï¼ˆ3x8GBï¼‰

```python
BATCH_SIZE = 1              # å‡å°æ‰¹æ¬¡
GROUP_SIZE = 4              # å‡å°ç»„å¤§å°
GSPO_EPOCHS = 4             # ä¿æŒä¸å˜
```

### è¿½æ±‚é€Ÿåº¦æ—¶

```python
BATCH_SIZE = 8              # æœ€å¤§æ‰¹æ¬¡
GROUP_SIZE = 4              # é€‚ä¸­ç»„å¤§å°
GSPO_EPOCHS = 2             # å‡å°‘æ›´æ–°è½®æ•°
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: ä»ç„¶ OOM

**å¯èƒ½åŸå› **ï¼š
- BATCH_SIZE æˆ– GROUP_SIZE å¤ªå¤§
- æ¨¡å‹å¤ªå¤§
- ç”Ÿæˆé•¿åº¦å¤ªé•¿

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°æ‰¹æ¬¡å’Œç»„å¤§å°
BATCH_SIZE = 1
GROUP_SIZE = 2

# 2. å‡å°‘ç”Ÿæˆé•¿åº¦
max_new_tokens=64  # ä» 128 æ”¹ä¸º 64

# 3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
POLICY_MODEL = "smaller_model_path"
```

### é—®é¢˜ 2: GPU åˆ©ç”¨ç‡ä¸å‡

**ç°è±¡**ï¼š
```
GPU 0: 90%
GPU 1: 30%
GPU 2: 50%
```

**åŸå› **ï¼šDataParallel çš„è´Ÿè½½ä¸ä¸€å®šå®Œå…¨å‡è¡¡

**è§£å†³æ–¹æ¡ˆ**ï¼š
- è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼ŒGPU 0 ä½œä¸ºä¸»è®¾å¤‡ä¼šç¨é«˜
- å¦‚æœå·®å¼‚å¤ªå¤§ï¼Œè€ƒè™‘ä½¿ç”¨ DistributedDataParallel

### é—®é¢˜ 3: æ‰¾ä¸åˆ° GPU

**é”™è¯¯ä¿¡æ¯**ï¼š
```
ValueError: éœ€è¦è‡³å°‘ 3 å¼  GPUï¼Œä½†åªæ£€æµ‹åˆ° 2 å¼ 
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä¿®æ”¹é…ç½®ä¸º 2 å¼  GPU
POLICY_GPU_IDS = [0, 1]
REFERENCE_GPU_ID = 1
REWARD_GPU_ID = 1
```

### é—®é¢˜ 4: è®­ç»ƒé€Ÿåº¦æ…¢

**å¯èƒ½åŸå› **ï¼š
- GPU é—´é€šä¿¡å¼€é”€
- æ‰¹æ¬¡å¤ªå°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# å¢å¤§æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 4  # æˆ–æ›´å¤§

# å‡å°‘æ›´æ–°è½®æ•°
GSPO_EPOCHS = 2
```

---

## ğŸ“ˆ æœ€ä½³å®è·µ

### 1. GPU é€‰æ‹©

**æ¨èé…ç½®**ï¼š
- 3å¼  RTX 4090 (24GB each)
- 3å¼  RTX 5060Ti (16GB each)
- 4å¼  RTX 3090 (24GB each)

**æœ€ä½é…ç½®**ï¼š
- 2å¼  RTX 3090 (24GB each)
- 3å¼  RTX 3080 (10GB each)

### 2. æ‰¹æ¬¡å¤§å°é€‰æ‹©

```python
# æ ¹æ®æ˜¾å­˜é€‰æ‹©
if total_vram >= 48:  # 3x16GB
    BATCH_SIZE = 4
    GROUP_SIZE = 8
elif total_vram >= 32:  # 2x16GB
    BATCH_SIZE = 2
    GROUP_SIZE = 4
else:
    BATCH_SIZE = 1
    GROUP_SIZE = 2
```

### 3. ç›‘æ§æ˜¾å­˜

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨ Python
import torch
print(f"GPU 0: {torch.cuda.memory_allocated(0) / 1e9:.2f}GB")
print(f"GPU 1: {torch.cuda.memory_allocated(1) / 1e9:.2f}GB")
print(f"GPU 2: {torch.cuda.memory_allocated(2) / 1e9:.2f}GB")
```

### 4. æ¸…ç†æ˜¾å­˜

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­å®šæœŸæ¸…ç†
torch.cuda.empty_cache()
gc.collect()
```

---

## ğŸ”„ ä»å•GPUè¿ç§»

### è¿ç§»æ­¥éª¤

1. **å¤‡ä»½åŸå§‹ä»£ç **
```bash
cp gspo.py gspo_backup.py
```

2. **ä½¿ç”¨å¤šGPUç‰ˆæœ¬**
```bash
cp gspo_multigpu.py gspo.py
```

3. **ä¿®æ”¹é…ç½®**
```python
# æ ¹æ®ä½ çš„GPUæ•°é‡ä¿®æ”¹
POLICY_GPU_IDS = [0, 1]
REFERENCE_GPU_ID = 2
REWARD_GPU_ID = 2
```

4. **æµ‹è¯•è¿è¡Œ**
```bash
python gspo.py
```

### å…¼å®¹æ€§

- âœ… æ¨¡å‹æƒé‡å®Œå…¨å…¼å®¹
- âœ… è®­ç»ƒé…ç½®å®Œå…¨å…¼å®¹
- âœ… æ•°æ®æ ¼å¼å®Œå…¨å…¼å®¹
- âœ… å¯ä»¥ä»å•GPUæ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ

---

## ğŸ¯ é«˜çº§ä¼˜åŒ–

### 1. ä½¿ç”¨ DistributedDataParallel (DDP)

å¦‚æœéœ€è¦æ›´å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ DDPï¼š

```python
# éœ€è¦ä¿®æ”¹ä»£ç ä½¿ç”¨ DDP
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–è¿›ç¨‹ç»„
dist.init_process_group(backend='nccl')

# ä½¿ç”¨ DDP
self.policy_model = DDP(
    policy_model_single,
    device_ids=[local_rank]
)
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = compute_loss(...)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
from torch.utils.checkpoint import checkpoint

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
output = checkpoint(self.layer, input)
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. [PyTorch DataParallel æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
2. [PyTorch DistributedDataParallel æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
3. [CUDA æœ€ä½³å®è·µ](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥ GPU é…ç½®æ˜¯å¦æ­£ç¡®
2. æŸ¥çœ‹æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
3. å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†
4. è”ç³»ä½œè€…: lby15356@gmail.com

---

**åˆ›å»ºæ—¥æœŸ**: 2026-01-20  
**ç‰ˆæœ¬**: v1.0  
**çŠ¶æ€**: âœ… å·²æµ‹è¯•
