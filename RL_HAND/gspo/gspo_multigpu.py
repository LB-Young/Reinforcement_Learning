#!/usr/bin/env python3
# author: YoungL
# date: 2026/01/20
# email: lby15356@gmail.com

"""
å¤šGPUç‰ˆ GSPO è®­ç»ƒè„šæœ¬
GPUåˆ†é…ç­–ç•¥ï¼š
- Policy Model: GPU 0 + GPU 1 (æ¨¡å‹å¹¶è¡Œ)
- Reference Model: GPU 2
- Reward Model: GPU 2

ä¸»è¦æ”¹è¿›ï¼š
1. ä½¿ç”¨ DataParallel å°† policy æ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ª GPU
2. ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼Œé¿å… OOM
3. æ”¯æŒçµæ´»çš„ GPU é…ç½®

æ³¨ï¼š
1. éœ€è¦è‡³å°‘ 3 å¼  GPU
2. å¦‚æœåªæœ‰ 2 å¼  GPUï¼Œå¯ä»¥å°† reference å’Œ reward æ”¾åœ¨åŒä¸€å¼ å¡
"""

import os
import shutil
import sys
import gc
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from typing import List, Tuple, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥ç»˜å›¾å‡½æ•°
from utils.plot_metrics import plot_gspo_metrics

# ==================== GPU é…ç½® ====================
# ğŸ”¥ GPU åˆ†é…ç­–ç•¥
POLICY_GPU_IDS = [0, 1]      # Policy æ¨¡å‹ä½¿ç”¨çš„ GPUï¼ˆæ¨¡å‹å¹¶è¡Œï¼‰
REFERENCE_GPU_ID = 2         # Reference æ¨¡å‹ä½¿ç”¨çš„ GPU
REWARD_GPU_ID = 2            # Reward æ¨¡å‹ä½¿ç”¨çš„ GPU

# å¦‚æœåªæœ‰ 2 å¼  GPUï¼Œä½¿ç”¨ä»¥ä¸‹é…ç½®
# POLICY_GPU_IDS = [0, 1]
# REFERENCE_GPU_ID = 1
# REWARD_GPU_ID = 1

# ==================== æ¨¡å‹é…ç½® ====================
POLICY_MODEL = r"E:\models\Qwen\Qwen3-0___6B"
REWARD_MODEL = r"E:\models\reward-model-deberta-v3-large-v2"
train_datasets = [
    {
        "path":r"E:\datasets\gsm8k\main\train-00000-of-00001.parquet",
        "type":"parquet",
        "input":"question",
        "output":"answer"
    }
]

BATCH_SIZE = 2              # å¯ä»¥é€‚å½“å¢å¤§
LEARNING_RATE = 1e-6
DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
NUM_EPOCHS = 1
GROUP_SIZE = 4
GSPO_EPOCHS = 4

# GSPOç‰¹æœ‰å‚æ•°
CLIP_RANGE = 0.2
ENTROPY_COEF = 0.01
KL_COEF = 0.2
TARGET_KL = 0.01
ADAPTIVE_KL = True

# ä¼˜åŠ¿è®¡ç®—å‚æ•°
ADVANTAGE_TYPE = "relative"
USE_GROUP_NORMALIZATION = True
USE_SEQUENCE_LEVEL_REWARD = True
USE_TOKEN_LEVEL_LOSS = False

OUTPUT_DIR = r"E:\projects\train_related\trained_model\rl_exprement\grpo_output\gspo_gsm8k_multigpu_v1"

# ==================== æ•°æ®é›† ====================
class GSPODataset(Dataset):
    def __init__(self, prompts: List[str]):
        self.prompts = prompts
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        return {"prompt": self.prompts[idx]}

# ==================== å¤šGPU GSPOè®­ç»ƒå™¨ ====================
class GSPOMultiGPUTrainer:
    def __init__(self):
        # æ£€æŸ¥ GPU æ•°é‡
        num_gpus = torch.cuda.device_count()
        print(f"æ£€æµ‹åˆ° {num_gpus} å¼  GPU")
        
        if num_gpus < len(POLICY_GPU_IDS):
            raise ValueError(f"éœ€è¦è‡³å°‘ {len(POLICY_GPU_IDS)} å¼  GPU ç”¨äº policy æ¨¡å‹ï¼Œä½†åªæ£€æµ‹åˆ° {num_gpus} å¼ ")
        
        if num_gpus <= max(REFERENCE_GPU_ID, REWARD_GPU_ID):
            raise ValueError(f"éœ€è¦è‡³å°‘ {max(REFERENCE_GPU_ID, REWARD_GPU_ID) + 1} å¼  GPUï¼Œä½†åªæ£€æµ‹åˆ° {num_gpus} å¼ ")
        
        # è®¾å¤‡é…ç½®
        self.policy_device = torch.device(f"cuda:{POLICY_GPU_IDS[0]}")  # ä¸»è®¾å¤‡
        self.device_ref = torch.device(f"cuda:{REFERENCE_GPU_ID}")
        self.device_reward = torch.device(f"cuda:{REWARD_GPU_ID}")
        
        print(f"GPU åˆ†é…:")
        print(f"  Policy Model: GPU {POLICY_GPU_IDS} (ä¸»è®¾å¤‡: GPU {POLICY_GPU_IDS[0]})")
        print(f"  Reference Model: GPU {REFERENCE_GPU_ID}")
        print(f"  Reward Model: GPU {REWARD_GPU_ID}")
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®°å½•
        self.metrics_history = {
            'policy_loss': [],
            'entropy_loss': [],
            'kl_loss': [],
            'reward': [],
            'relative_advantage': [],
            'kl_divergence': [],
            'kl_coef': [],
            'avg_response_length': []
        }
        
        # åˆå§‹åŒ–KLç³»æ•°
        self.kl_coef = KL_COEF
        
        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ğŸ”¥ åŠ è½½æ¨¡å‹ï¼ˆå¤šGPUç­–ç•¥ï¼‰
        self._load_models()
        
        # ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ– policy æ¨¡å‹ï¼‰
        self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=LEARNING_RATE)
        
        print("âœ… å¤šGPU GSPO Trainer åˆå§‹åŒ–å®Œæˆ")

    def _load_models(self):
        """ğŸ”¥ åŠ è½½æ¨¡å‹å¹¶é…ç½®å¤šGPU"""
        print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # 1. åŠ è½½ Policy æ¨¡å‹åˆ°ä¸»è®¾å¤‡
        print(f"  åŠ è½½ Policy æ¨¡å‹åˆ° GPU {POLICY_GPU_IDS[0]}...")
        policy_model_single = AutoModelForCausalLM.from_pretrained(
            POLICY_MODEL, 
            torch_dtype=DTYPE,
            device_map={"": self.policy_device}
        )
        
        # ğŸ”¥ ä½¿ç”¨ DataParallel å°†æ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ª GPU
        if len(POLICY_GPU_IDS) > 1:
            print(f"  ä½¿ç”¨ DataParallel å°† Policy æ¨¡å‹åˆ†å¸ƒåˆ° GPU {POLICY_GPU_IDS}...")
            self.policy_model = nn.DataParallel(
                policy_model_single,
                device_ids=POLICY_GPU_IDS,
                output_device=POLICY_GPU_IDS[0]
            )
            # ä¿å­˜åŸå§‹æ¨¡å‹çš„å¼•ç”¨ï¼ˆç”¨äºä¿å­˜ï¼‰
            self.policy_model_unwrapped = policy_model_single
        else:
            self.policy_model = policy_model_single
            self.policy_model_unwrapped = policy_model_single
        
        # 2. åŠ è½½ Reference æ¨¡å‹
        print(f"  åŠ è½½ Reference æ¨¡å‹åˆ° GPU {REFERENCE_GPU_ID}...")
        self.ref_model = AutoModelForCausalLM.from_pretrained(
            POLICY_MODEL, 
            torch_dtype=DTYPE,
            device_map={"": self.device_ref}
        )
        self.ref_model.eval()
        
        # 3. åŠ è½½ Reward æ¨¡å‹
        print(f"  åŠ è½½ Reward æ¨¡å‹åˆ° GPU {REWARD_GPU_ID}...")
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(
            REWARD_MODEL, 
            torch_dtype=DTYPE,
            device_map={"": self.device_reward}
        )
        self.reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)
        
        print("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")

    def generate_responses_with_group_sampling(self, prompts: List[str]) -> Tuple[List[str], List[str], List[int]]:
        """ğŸ”¥ GSPO Group Sampling: ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›å¤"""
        self.policy_model.eval()
        all_responses, all_prompts, all_lengths = [], [], []
        
        for prompt in prompts:
            for _ in range(GROUP_SIZE):
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.policy_device)
                with torch.no_grad():
                    # DataParallel ä¼šè‡ªåŠ¨å¤„ç†å¤šGPU
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,
                        temperature=0.7,
                        num_return_sequences=1,
                        pad_token_id=self.tokenizer.pad_token_id
                    )
                
                gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
                response = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
                length = len(gen_ids[0])
                
                all_responses.append(response)
                all_prompts.append(prompt)
                all_lengths.append(length)
                
                del inputs, outputs, gen_ids
        
        torch.cuda.empty_cache()
        return all_responses, all_prompts, all_lengths

    def compute_log_probs(self, prompts: List[str], responses: List[str], 
                         use_ref_model: bool = False, return_per_token: bool = False) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """è®¡ç®—logæ¦‚ç‡ï¼Œæ”¯æŒåºåˆ—çº§å’Œtokençº§"""
        all_log_probs = []
        all_token_log_probs = []
        
        if use_ref_model:
            model = self.ref_model
            device = self.device_ref
        else:
            model = self.policy_model
            device = self.policy_device
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True).to(device)
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            with torch.no_grad() if use_ref_model else torch.enable_grad():
                outputs = model(**full_inputs)
                logits = outputs.logits
                
                log_probs = F.log_softmax(logits, dim=-1)
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)
                
                response_log_probs = token_log_probs[0, response_start-1:-1]
                
                if return_per_token:
                    all_token_log_probs.append(response_log_probs)
                
                all_log_probs.append(response_log_probs.sum())
                
                del full_inputs, outputs, logits, log_probs, token_log_probs
        
        torch.cuda.empty_cache()
        
        if return_per_token:
            return torch.stack(all_log_probs).to(self.policy_device), all_token_log_probs
        else:
            return torch.stack(all_log_probs).to(self.policy_device), []

    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ğŸ”¥ GSPO Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±"""
        rewards = []
        for p, r in zip(prompts, responses):
            full_text = f"{p} {r}"
            inputs = self.reward_tokenizer(full_text, return_tensors="pt", truncation=True, max_length=512).to(self.device_reward)
            with torch.no_grad():
                reward = self.reward_model(**inputs).logits[0, 0]
                rewards.append(reward)
            del inputs
        
        result = torch.stack(rewards).to(self.policy_device)
        torch.cuda.empty_cache()
        return result

    def compute_relative_advantages(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ğŸ”¥ GSPO Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿"""
        if group_size is None:
            group_size = GROUP_SIZE
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        rewards_grouped = rewards.view(-1, group_size)
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
        
        if ADVANTAGE_TYPE == "relative":
            relative_advantages = rewards_grouped - group_baselines
        elif ADVANTAGE_TYPE == "normalized":
            relative_advantages = rewards_grouped - group_baselines
            if USE_GROUP_NORMALIZATION:
                group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
                relative_advantages = relative_advantages / group_std
        else:
            raise ValueError(f"Unknown advantage type: {ADVANTAGE_TYPE}")
        
        relative_advantages = relative_advantages.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_advantages, group_baselines

    def compute_policy_loss_sequence_level(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                                          advantages: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ GSPO Sequence-Level Policy Loss: åºåˆ—çº§åˆ«çš„ç­–ç•¥æŸå¤±"""
        ratio = torch.exp(log_probs - old_log_probs)
        
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        return policy_loss

    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor],
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ GSPO Token-Level Policy Loss: tokençº§åˆ«çš„ç­–ç•¥æŸå¤±ï¼ˆå¯é€‰ï¼‰"""
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            if len(token_log_probs) == 0:
                continue
                
            advantage = advantages[i]
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)
            
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * advantage
            
            token_loss = -torch.min(surr1, surr2).sum()
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        return total_token_loss / max(total_tokens, 1)

    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                          advantages: torch.Tensor, kl_penalty: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®¡ç®—GSPOç­–ç•¥æŸå¤±"""
        if USE_TOKEN_LEVEL_LOSS and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            policy_loss = self.compute_policy_loss_sequence_level(
                log_probs, old_log_probs, advantages
            )
        
        entropy = -log_probs.mean()
        entropy_loss = -ENTROPY_COEF * entropy
        kl_loss = self.kl_coef * kl_penalty.mean()
        
        return policy_loss, entropy_loss, kl_loss

    def update_kl_coef(self, kl_divergence: torch.Tensor):
        """è‡ªé€‚åº”è°ƒæ•´KLæ•£åº¦ç³»æ•°"""
        if not ADAPTIVE_KL:
            return
        
        mean_kl = kl_divergence.mean().item()
        
        if mean_kl > 2.0 * TARGET_KL:
            self.kl_coef *= 1.5
        elif mean_kl < 0.5 * TARGET_KL:
            self.kl_coef *= 0.5
        
        self.kl_coef = max(0.01, min(self.kl_coef, 1.0))

    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€æ­¥GSPOè®­ç»ƒ"""
        # 1. Group Sampling
        responses, prompts_expanded, response_lengths = self.generate_responses_with_group_sampling(batch_prompts)
        
        # 2. Sequence-Level Rewards
        raw_rewards = self.compute_rewards(prompts_expanded, responses)
        
        # 3. Relative Advantage
        relative_advantages, group_baselines = self.compute_relative_advantages(raw_rewards)
        
        # æˆªæ–­æ•°æ®
        prompts_truncated = prompts_expanded[:len(relative_advantages)]
        responses_truncated = responses[:len(relative_advantages)]
        response_lengths_truncated = response_lengths[:len(relative_advantages)]
        
        # è®¡ç®—åˆå§‹å€¼ï¼ˆä½¿ç”¨ no_gradï¼‰
        with torch.no_grad():
            ref_log_probs, _ = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=True)
            old_log_probs, old_token_log_probs_list = self.compute_log_probs(
                prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
            )
            initial_kl_penalty = old_log_probs - ref_log_probs
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (relative_advantages - relative_advantages.mean()) / (relative_advantages.std() + 1e-8)
        
        # 4. GSPOæ›´æ–°å¾ªç¯
        self.policy_model.train()
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        
        for _ in range(GSPO_EPOCHS):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_log_probs, new_token_log_probs_list = self.compute_log_probs(
                prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
            )
            
            # é‡æ–°è®¡ç®—KLæ•£åº¦
            current_kl_penalty = new_log_probs - ref_log_probs.detach()
            
            # è®¡ç®—æŸå¤±
            policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
                new_log_probs, old_log_probs, advantages, current_kl_penalty,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            total_loss = policy_loss + entropy_loss + kl_loss
            
            # æ›´æ–°ç­–ç•¥æ¨¡å‹
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_kl_loss += kl_loss.item()
            
            # æ¸…ç†æ˜¾å­˜
            del new_log_probs, current_kl_penalty, policy_loss, entropy_loss, kl_loss, total_loss
            if new_token_log_probs_list:
                del new_token_log_probs_list
            
            self.optimizer.zero_grad(set_to_none=True)
            torch.cuda.empty_cache()
            gc.collect()
        
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°
        self.update_kl_coef(initial_kl_penalty)
        
        metrics = {
            "policy_loss": total_policy_loss / GSPO_EPOCHS,
            "entropy_loss": total_entropy_loss / GSPO_EPOCHS,
            "kl_loss": total_kl_loss / GSPO_EPOCHS,
            "reward": raw_rewards.mean().item(),
            "relative_advantage": relative_advantages.mean().item(),
            "kl_divergence": initial_kl_penalty.mean().item(),
            "kl_coef": self.kl_coef,
            "avg_response_length": sum(response_lengths_truncated) / len(response_lengths_truncated)
        }
        
        # è®°å½•æŒ‡æ ‡
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
        
        return metrics

    def train(self, dataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
        for epoch in range(NUM_EPOCHS):
            pbar = tqdm(dataloader)
            for batch in pbar:
                metrics = self.train_step(batch["prompt"])
                if metrics:
                    pbar.set_description(
                        f"PL:{metrics['policy_loss']:.4f} R:{metrics['reward']:.2f} "
                        f"RA:{metrics['relative_advantage']:.3f} KL:{metrics['kl_divergence']:.4f}"
                    )

            # ä¿å­˜æ¨¡å‹ï¼ˆä¿å­˜æœªåŒ…è£…çš„æ¨¡å‹ï¼‰
            save_path = os.path.join(OUTPUT_DIR, f"epoch_{epoch+1}")
            self.policy_model_unwrapped.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            
            # å¤‡ä»½è®­ç»ƒè„šæœ¬
            current_script = os.path.abspath(__file__)
            target_script = os.path.join(save_path, "train_script.py")
            
            try:
                shutil.copy2(current_script, target_script)
                print(f"è„šæœ¬å·²å¤‡ä»½è‡³: {target_script}")
            except Exception as e:
                print(f"è„šæœ¬å¤‡ä»½å¤±è´¥: {e}")
            
            print(f"æ¨¡å‹ä¿å­˜è‡³: {save_path}")
        
        # è®­ç»ƒç»“æŸåç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨
        print("\næ­£åœ¨ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å›¾è¡¨...")
        plot_gspo_metrics(
            policy_losses=self.metrics_history['policy_loss'],
            entropy_losses=self.metrics_history['entropy_loss'],
            kl_losses=self.metrics_history['kl_loss'],
            rewards=self.metrics_history['reward'],
            relative_advantages=self.metrics_history['relative_advantage'],
            kl_divergences=self.metrics_history['kl_divergence'],
            kl_coefs=self.metrics_history['kl_coef'],
            avg_response_lengths=self.metrics_history['avg_response_length'],
            save_path=os.path.join(OUTPUT_DIR, "training_metrics.png")
        )
        print(f"è®­ç»ƒæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜è‡³: {os.path.join(OUTPUT_DIR, 'training_metrics.png')}")

def main():
    """æµ‹è¯•å‡½æ•°"""
    prompts = ["å¦‚ä½•åˆ¶ä½œä¸€æ¯å¥½å’–å•¡ï¼Ÿ", "è§£é‡Šé‡å­çº ç¼ ã€‚", "å†™ä¸€æ®µå†’æ³¡æ’åºä»£ç ã€‚"] * 10
    dataset = GSPODataset(prompts)
    trainer = GSPOMultiGPUTrainer()
    trainer.train(dataset)

def train_main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    prompts = []
    
    for datasets in train_datasets:
        if datasets['type'] == "jsonl":
            import json
            with open(datasets['path'], "r", encoding='utf-8') as f:
                for item in json.load(f):
                    prompts.append(item[datasets['input']])
        if datasets['type'] == 'parquet':
            import pyarrow.parquet as pq
            table = pq.read_table(datasets['path'])
            df = table.to_pandas()
            for index, row in df.iterrows():
                prompts.append(row['question'])

    prompts = prompts[:20]
    dataset = GSPODataset(prompts)
    trainer = GSPOMultiGPUTrainer()
    trainer.train(dataset)

if __name__ == "__main__":
    # main()
    train_main()
