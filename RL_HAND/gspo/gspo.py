#!/usr/bin/env python3
# author: YoungL
# date: 2026/01/19
# email: lby15356@gmail.com

"""
æ ‡å‡†ç‰ˆ GSPO è®­ç»ƒè„šæœ¬
GSPO (Group Sequence Policy Optimization) æ˜¯ä¸€ç§ç»“åˆäº†ç»„é‡‡æ ·å’Œåºåˆ—çº§ä¼˜åŒ–çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•
ä¸»è¦ç‰¹æ€§ï¼š
1. Group Sampling: ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒ
2. Sequence-Level Rewards: åºåˆ—çº§åˆ«çš„å¥–åŠ±è®¡ç®—
3. Relative Advantage: ä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ›¿ä»£criticæ¨¡å‹
4. çµæ´»ä¼˜åŒ–: æ”¯æŒåºåˆ—çº§å’Œtokençº§ä¼˜åŒ–ç­–ç•¥

æ³¨ï¼š
1. ç”±äºé¡¹ç›®ç€é‡äºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®ç°å’Œå„ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•å¼‚åŒçš„å¯¹æ¯”ï¼Œæ•…åªæ¶‰åŠæ•´ä½“æµç¨‹ï¼Œä¸åŒ…å«å·¥ç¨‹å±‚é¢çš„è°ƒåº¦ä¼˜åŒ–ï¼›
2. ç”±äºæœ¬é¡¹ç›®å®éªŒç¯å¢ƒä»…åŒ…å«5060ti-16G * 2ï¼Œæ‰€ä»¥å°†policyæ¨¡å‹æ”¾åœ¨0å·gpuï¼Œreferenceã€rewardä¸¤ä¸ªæ¨¡å‹æ”¾åœ¨1å·gpuï¼›
"""

import os
import shutil
import sys
import gc
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from typing import List, Tuple, Dict
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥ç»˜å›¾å‡½æ•°
from utils.plot_metrics import plot_gspo_metrics

# ==================== é…ç½® ====================
POLICY_MODEL = r"E:\models\Qwen\Qwen3-0___6B"
REWARD_MODEL = r"E:\models\reward-model-deberta-v3-large-v2"
train_datasets = [
    {
        "path":r"E:\datasets\gsm8k\main\train-00000-of-00001.parquet",
        "type":"parquet",
        "input":"question",
        "output":"answer"
    }
]

BATCH_SIZE = 2              # GSPOéœ€è¦ç”Ÿæˆå¤šä¸ªå›å¤ï¼Œé€‚å½“è°ƒå°
LEARNING_RATE = 1e-6
DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
NUM_EPOCHS = 1
GROUP_SIZE = 4              # ğŸ”¥ æ¯ä¸ªpromptç”Ÿæˆçš„å›å¤æ•°é‡
GSPO_EPOCHS = 4             # GSPOæ›´æ–°è½®æ•°

# GSPOç‰¹æœ‰å‚æ•°
CLIP_RANGE = 0.2            # PPOè£å‰ªèŒƒå›´
ENTROPY_COEF = 0.01         # ç†µæ­£åˆ™åŒ–ç³»æ•°
KL_COEF = 0.2               # KLæ•£åº¦æƒ©ç½šç³»æ•°
TARGET_KL = 0.01            # ç›®æ ‡KLæ•£åº¦
ADAPTIVE_KL = True          # è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´

# ä¼˜åŠ¿è®¡ç®—å‚æ•°
ADVANTAGE_TYPE = "relative"     # "relative" æˆ– "normalized"
USE_GROUP_NORMALIZATION = True  # ç»„å†…æ ‡å‡†åŒ–
USE_SEQUENCE_LEVEL_REWARD = True    # åºåˆ—çº§åˆ«å¥–åŠ±
USE_TOKEN_LEVEL_LOSS = False        # Tokençº§åˆ«æŸå¤±ï¼ˆå¯é€‰ï¼‰

OUTPUT_DIR = r"E:\projects\train_related\trained_model\rl_exprement\grpo_output\gspo_gsm8k_v1"

# ==================== æ•°æ®é›† ====================
class GSPODataset(Dataset):
    def __init__(self, prompts: List[str]):
        self.prompts = prompts
    
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, idx):
        return {"prompt": self.prompts[idx]}

# ==================== GSPOè®­ç»ƒå™¨ ====================
class GSPOTrainer:
    def __init__(self):
        num_gpus = torch.cuda.device_count()
        self.device_policy = torch.device("cuda:0")
        self.device_ref = torch.device("cuda:1") if num_gpus > 1 else torch.device("cuda:0")
        self.device_reward = torch.device("cuda:1") if num_gpus > 1 else torch.device("cuda:0")
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®°å½•
        self.metrics_history = {
            'policy_loss': [],
            'entropy_loss': [],
            'kl_loss': [],
            'reward': [],
            'relative_advantage': [],
            'kl_divergence': [],
            'kl_coef': [],
            'avg_response_length': []
        }
        
        # åˆå§‹åŒ–KLç³»æ•°
        self.kl_coef = KL_COEF
        
        self.tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.policy_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL, torch_dtype=DTYPE, device_map={"": self.device_policy})
        self.ref_model = AutoModelForCausalLM.from_pretrained(POLICY_MODEL, torch_dtype=DTYPE, device_map={"": self.device_ref})
        self.ref_model.eval()
        
        self.reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL, torch_dtype=DTYPE, device_map={"": self.device_reward})
        self.reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)
        
        self.optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=LEARNING_RATE)

    def generate_responses_with_group_sampling(self, prompts: List[str]) -> Tuple[List[str], List[str], List[int]]:
        """ğŸ”¥ GSPO Group Sampling: ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›å¤"""
        self.policy_model.eval()
        all_responses, all_prompts, all_lengths = [], [], []
        
        for prompt in prompts:
            for _ in range(GROUP_SIZE):
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device_policy)
                with torch.no_grad():
                    outputs = self.policy_model.generate(
                        **inputs,
                        max_new_tokens=128,
                        do_sample=True,
                        temperature=0.7,
                        num_return_sequences=1,
                        pad_token_id=self.tokenizer.pad_token_id
                    )
                
                # ä»…æå–ç”Ÿæˆçš„ Response éƒ¨åˆ†
                gen_ids = outputs[:, inputs["input_ids"].shape[1]:]
                response = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
                length = len(gen_ids[0])
                
                all_responses.append(response)
                all_prompts.append(prompt)
                all_lengths.append(length)
                
                # é‡Šæ”¾å½“å‰å¾ªç¯çš„å¼ é‡
                del inputs, outputs, gen_ids
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        return all_responses, all_prompts, all_lengths

    def compute_log_probs(self, prompts: List[str], responses: List[str], 
                         use_ref_model: bool = False, return_per_token: bool = False) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """è®¡ç®—logæ¦‚ç‡ï¼Œæ”¯æŒåºåˆ—çº§å’Œtokençº§"""
        all_log_probs = []
        all_token_log_probs = []
        
        model = self.ref_model if use_ref_model else self.policy_model
        device = self.device_ref if use_ref_model else self.device_policy
        
        for prompt, response in zip(prompts, responses):
            full_text = prompt + response
            full_inputs = self.tokenizer(full_text, return_tensors="pt", padding=True, truncation=True).to(device)
            
            prompt_inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            response_start = prompt_inputs["input_ids"].shape[1]
            
            with torch.no_grad() if use_ref_model else torch.enable_grad():
                outputs = model(**full_inputs)
                logits = outputs.logits
                
                log_probs = F.log_softmax(logits, dim=-1)
                token_log_probs = log_probs.gather(2, full_inputs["input_ids"].unsqueeze(-1)).squeeze(-1)
                
                # åªè€ƒè™‘ç”Ÿæˆéƒ¨åˆ†çš„logæ¦‚ç‡
                response_log_probs = token_log_probs[0, response_start-1:-1]
                
                if return_per_token:
                    all_token_log_probs.append(response_log_probs)
                
                # åºåˆ—çº§logæ¦‚ç‡ï¼ˆæ‰€æœ‰tokençš„å’Œï¼‰
                all_log_probs.append(response_log_probs.sum())
                
                # é‡Šæ”¾å¼ é‡
                del full_inputs, outputs, logits, log_probs, token_log_probs
        
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        
        if return_per_token:
            return torch.stack(all_log_probs).to(self.device_policy), all_token_log_probs
        else:
            return torch.stack(all_log_probs).to(self.device_policy), []

    def compute_rewards(self, prompts: List[str], responses: List[str]) -> torch.Tensor:
        """ğŸ”¥ GSPO Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±"""
        rewards = []
        for p, r in zip(prompts, responses):
            full_text = f"{p} {r}"
            inputs = self.reward_tokenizer(full_text, return_tensors="pt", truncation=True, max_length=512).to(self.device_reward)
            with torch.no_grad():
                reward = self.reward_model(**inputs).logits[0, 0]
                rewards.append(reward)
            # é‡Šæ”¾å½“å‰å¾ªç¯çš„å¼ é‡
            del inputs
        
        result = torch.stack(rewards).to(self.device_policy)
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        return result

    def compute_relative_advantages(self, rewards: torch.Tensor, group_size: int = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ğŸ”¥ GSPO Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿"""
        if group_size is None:
            group_size = GROUP_SIZE
        
        batch_size = rewards.shape[0]
        if batch_size % group_size != 0:
            num_complete_groups = batch_size // group_size
            rewards = rewards[:num_complete_groups * group_size]
            batch_size = rewards.shape[0]
        
        # é‡å¡‘ä¸ºç»„çš„å½¢çŠ¶ [num_groups, group_size]
        rewards_grouped = rewards.view(-1, group_size)
        
        # ğŸ”¥ è®¡ç®—ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿
        group_baselines = rewards_grouped.mean(dim=1, keepdim=True)
        
        # ğŸ”¥ è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
        if ADVANTAGE_TYPE == "relative":
            # ç›¸å¯¹ä¼˜åŠ¿ï¼šreward - baseline
            relative_advantages = rewards_grouped - group_baselines
        elif ADVANTAGE_TYPE == "normalized":
            # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼š(reward - baseline) / std
            relative_advantages = rewards_grouped - group_baselines
            if USE_GROUP_NORMALIZATION:
                group_std = rewards_grouped.std(dim=1, keepdim=True) + 1e-8
                relative_advantages = relative_advantages / group_std
        else:
            raise ValueError(f"Unknown advantage type: {ADVANTAGE_TYPE}")
        
        # é‡æ–°å±•å¹³
        relative_advantages = relative_advantages.view(-1)
        group_baselines = group_baselines.repeat(1, group_size).view(-1)
        
        return relative_advantages, group_baselines

    def compute_policy_loss_sequence_level(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                                          advantages: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ GSPO Sequence-Level Policy Loss: åºåˆ—çº§åˆ«çš„ç­–ç•¥æŸå¤±"""
        ratio = torch.exp(log_probs - old_log_probs)
        
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        return policy_loss

    def compute_policy_loss_token_level(self, token_log_probs_list: List[torch.Tensor],
                                       old_token_log_probs_list: List[torch.Tensor],
                                       advantages: torch.Tensor) -> torch.Tensor:
        """ğŸ”¥ GSPO Token-Level Policy Loss: tokençº§åˆ«çš„ç­–ç•¥æŸå¤±ï¼ˆå¯é€‰ï¼‰"""
        total_token_loss = 0.0
        total_tokens = 0
        
        for i, (token_log_probs, old_token_log_probs) in enumerate(zip(token_log_probs_list, old_token_log_probs_list)):
            if len(token_log_probs) == 0:  # è·³è¿‡ç©ºçš„tokenåºåˆ—
                continue
                
            advantage = advantages[i]  # åºåˆ—çº§ä¼˜åŠ¿
            
            # å¯¹æ¯ä¸ªtokenè®¡ç®—æ¯”ç‡
            token_ratios = torch.exp(token_log_probs - old_token_log_probs)
            
            # PPOè£å‰ª
            surr1 = token_ratios * advantage
            surr2 = torch.clamp(token_ratios, 1 - CLIP_RANGE, 1 + CLIP_RANGE) * advantage
            
            token_loss = -torch.min(surr1, surr2).sum()
            
            total_token_loss += token_loss
            total_tokens += len(token_log_probs)
        
        return total_token_loss / max(total_tokens, 1)

    def compute_policy_loss(self, log_probs: torch.Tensor, old_log_probs: torch.Tensor,
                          advantages: torch.Tensor, kl_penalty: torch.Tensor,
                          token_log_probs_list: List[torch.Tensor] = None,
                          old_token_log_probs_list: List[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®¡ç®—GSPOç­–ç•¥æŸå¤±"""
        # é€‰æ‹©åºåˆ—çº§åˆ«æˆ–tokençº§åˆ«æŸå¤±
        if USE_TOKEN_LEVEL_LOSS and token_log_probs_list is not None:
            policy_loss = self.compute_policy_loss_token_level(
                token_log_probs_list, old_token_log_probs_list, advantages
            )
        else:
            policy_loss = self.compute_policy_loss_sequence_level(
                log_probs, old_log_probs, advantages
            )
        
        # ç†µæŸå¤±
        entropy = -log_probs.mean()
        entropy_loss = -ENTROPY_COEF * entropy
        
        # KLæŸå¤±
        kl_loss = self.kl_coef * kl_penalty.mean()
        
        return policy_loss, entropy_loss, kl_loss

    def update_kl_coef(self, kl_divergence: torch.Tensor):
        """è‡ªé€‚åº”è°ƒæ•´KLæ•£åº¦ç³»æ•°"""
        if not ADAPTIVE_KL:
            return
        
        mean_kl = kl_divergence.mean().item()
        
        if mean_kl > 2.0 * TARGET_KL:
            self.kl_coef *= 1.5
        elif mean_kl < 0.5 * TARGET_KL:
            self.kl_coef *= 0.5
        
        self.kl_coef = max(0.01, min(self.kl_coef, 1.0))

    def train_step(self, batch_prompts: List[str]) -> Dict[str, float]:
        """æ‰§è¡Œä¸€æ­¥GSPOè®­ç»ƒ"""
        # ğŸ”¥ 1. Group Sampling: ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›å¤
        responses, prompts_expanded, response_lengths = self.generate_responses_with_group_sampling(batch_prompts)
        
        # ğŸ”¥ 2. Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±
        raw_rewards = self.compute_rewards(prompts_expanded, responses)
        
        # ğŸ”¥ 3. Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        relative_advantages, group_baselines = self.compute_relative_advantages(raw_rewards)
        
        # æˆªæ–­æ•°æ®ä»¥åŒ¹é…ç›¸å¯¹ä¼˜åŠ¿çš„é•¿åº¦
        prompts_truncated = prompts_expanded[:len(relative_advantages)]
        responses_truncated = responses[:len(relative_advantages)]
        response_lengths_truncated = response_lengths[:len(relative_advantages)]
        
        # è®¡ç®—KLæ•£åº¦æƒ©ç½šï¼ˆä½¿ç”¨å‚è€ƒæ¨¡å‹ï¼‰
        with torch.no_grad():
            ref_log_probs, _ = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=True)
        
        # è®¡ç®—åˆå§‹logæ¦‚ç‡ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
        with torch.no_grad():
            old_log_probs, old_token_log_probs_list = self.compute_log_probs(
                prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
            )
            # è®¡ç®—åˆå§‹KLæ•£åº¦
            initial_kl_penalty = old_log_probs - ref_log_probs
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (relative_advantages - relative_advantages.mean()) / (relative_advantages.std() + 1e-8)
        
        # ğŸ”¥ 4. GSPOæ›´æ–°å¾ªç¯
        self.policy_model.train()
        total_policy_loss = 0
        total_entropy_loss = 0
        total_kl_loss = 0
        
        for _ in range(GSPO_EPOCHS):
            # é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„logæ¦‚ç‡
            new_log_probs, new_token_log_probs_list = self.compute_log_probs(
                prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
            )
            
            # é‡æ–°è®¡ç®—KLæ•£åº¦ï¼ˆä½¿ç”¨å½“å‰ç­–ç•¥ï¼‰
            current_kl_penalty = new_log_probs - ref_log_probs.detach()
            
            # è®¡ç®—æŸå¤±
            policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
                new_log_probs, old_log_probs, advantages, current_kl_penalty,
                token_log_probs_list=new_token_log_probs_list,
                old_token_log_probs_list=old_token_log_probs_list
            )
            
            # æ€»æŸå¤±
            total_loss = policy_loss + entropy_loss + kl_loss
            
            # æ›´æ–°ç­–ç•¥æ¨¡å‹
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), 1.0)
            self.optimizer.step()
            
            total_policy_loss += policy_loss.item()
            total_entropy_loss += entropy_loss.item()
            total_kl_loss += kl_loss.item()
            
            # æ˜¾å¼æ¸…ç†æ˜¾å­˜
            del new_log_probs, current_kl_penalty, policy_loss, entropy_loss, kl_loss, total_loss
            if new_token_log_probs_list:
                del new_token_log_probs_list
            
            self.optimizer.zero_grad(set_to_none=True)
            torch.cuda.empty_cache()
            gc.collect()
        
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°ï¼ˆä½¿ç”¨åˆå§‹KLæ•£åº¦ï¼‰
        self.update_kl_coef(initial_kl_penalty)
        
        metrics = {
            "policy_loss": total_policy_loss / GSPO_EPOCHS,
            "entropy_loss": total_entropy_loss / GSPO_EPOCHS,
            "kl_loss": total_kl_loss / GSPO_EPOCHS,
            "reward": raw_rewards.mean().item(),
            "relative_advantage": relative_advantages.mean().item(),
            "kl_divergence": initial_kl_penalty.mean().item(),
            "kl_coef": self.kl_coef,
            "avg_response_length": sum(response_lengths_truncated) / len(response_lengths_truncated)
        }
        
        # è®°å½•æŒ‡æ ‡
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
        
        return metrics

    def train(self, dataset):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
        for epoch in range(NUM_EPOCHS):
            pbar = tqdm(dataloader)
            for batch in pbar:
                metrics = self.train_step(batch["prompt"])
                if metrics:  # å¦‚æœä¸ä¸ºç©º
                    pbar.set_description(
                        f"PL:{metrics['policy_loss']:.4f} R:{metrics['reward']:.2f} "
                        f"RA:{metrics['relative_advantage']:.3f} KL:{metrics['kl_divergence']:.4f}"
                    )

            # ä¿å­˜æ¨¡å‹
            save_path = os.path.join(OUTPUT_DIR, f"epoch_{epoch+1}")
            self.policy_model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            
            # å¤‡ä»½è®­ç»ƒè„šæœ¬
            current_script = os.path.abspath(__file__)
            target_script = os.path.join(save_path, "train_script.py")
            
            try:
                shutil.copy2(current_script, target_script)
                print(f"è„šæœ¬å·²å¤‡ä»½è‡³: {target_script}")
            except Exception as e:
                print(f"è„šæœ¬å¤‡ä»½å¤±è´¥: {e}")
            
            print(f"æ¨¡å‹ä¿å­˜è‡³: {save_path}")
        
        # è®­ç»ƒç»“æŸåç»˜åˆ¶æŒ‡æ ‡å›¾è¡¨
        print("\næ­£åœ¨ç”Ÿæˆè®­ç»ƒæŒ‡æ ‡å›¾è¡¨...")
        plot_gspo_metrics(
            policy_losses=self.metrics_history['policy_loss'],
            entropy_losses=self.metrics_history['entropy_loss'],
            kl_losses=self.metrics_history['kl_loss'],
            rewards=self.metrics_history['reward'],
            relative_advantages=self.metrics_history['relative_advantage'],
            kl_divergences=self.metrics_history['kl_divergence'],
            kl_coefs=self.metrics_history['kl_coef'],
            avg_response_lengths=self.metrics_history['avg_response_length'],
            save_path=os.path.join(OUTPUT_DIR, "training_metrics.png")
        )
        print(f"è®­ç»ƒæŒ‡æ ‡å›¾è¡¨å·²ä¿å­˜è‡³: {os.path.join(OUTPUT_DIR, 'training_metrics.png')}")

def main():
    """æµ‹è¯•å‡½æ•°"""
    prompts = ["å¦‚ä½•åˆ¶ä½œä¸€æ¯å¥½å’–å•¡ï¼Ÿ", "è§£é‡Šé‡å­çº ç¼ ã€‚", "å†™ä¸€æ®µå†’æ³¡æ’åºä»£ç ã€‚"] * 10
    dataset = GSPODataset(prompts)
    trainer = GSPOTrainer()
    trainer.train(dataset)

def train_main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    prompts = []
    
    for datasets in train_datasets:
        if datasets['type'] == "jsonl":
            import json
            with open(datasets['path'], "r", encoding='utf-8') as f:
                for item in json.load(f):
                    prompts.append(item[datasets['input']])
        if datasets['type'] == 'parquet':
            import pyarrow.parquet as pq
            table = pq.read_table(datasets['path'])
            df = table.to_pandas()
            for index, row in df.iterrows():
                prompts.append(row['question'])

    prompts = prompts[:20]
    dataset = GSPODataset(prompts)
    trainer = GSPOTrainer()
    trainer.train(dataset)

if __name__ == "__main__":
    # main()
    train_main()