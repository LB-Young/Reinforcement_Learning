# GSPO (Group Sequence Policy Optimization)

## æ¦‚è¿°

GSPOæ˜¯ä¸€ç§ç»“åˆäº†ç»„é‡‡æ ·å’Œåºåˆ—çº§ä¼˜åŒ–çš„ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒï¼Œä½¿ç”¨ç›¸å¯¹ä¼˜åŠ¿æ›¿ä»£criticæ¨¡å‹ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦å¤šæ ·æ€§å’Œå¤æ‚æ¨ç†çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚

## æ ¸å¿ƒç‰¹æ€§

### ğŸ”¥ Group Samplingï¼ˆç»„é‡‡æ ·ï¼‰
- ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªä¸åŒå›å¤
- é€šè¿‡é‡‡æ ·ç¡®ä¿å›å¤å¤šæ ·æ€§
- æä¾›ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·

### ğŸ”¥ Sequence-Level Rewardsï¼ˆåºåˆ—çº§å¥–åŠ±ï¼‰
- åœ¨å®Œæ•´åºåˆ—çº§åˆ«è®¡ç®—å¥–åŠ±
- ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°æ•´ä¸ªprompt+response
- å¾—åˆ°æ ‡é‡å¥–åŠ±å€¼

### ğŸ”¥ Relative Advantageï¼ˆç›¸å¯¹ä¼˜åŠ¿ï¼‰
- ä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿è€Œéç»å¯¹å¥–åŠ±
- ç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿ï¼Œæ›¿ä»£criticæ¨¡å‹
- å‡å°‘å¥–åŠ±å°ºåº¦å½±å“ï¼Œå…³æ³¨ç›¸å¯¹å¥½å

### ğŸ”¥ çµæ´»ä¼˜åŒ–ç­–ç•¥
- æ”¯æŒåºåˆ—çº§å’Œtokençº§ä¼˜åŒ–
- è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´
- å¯é…ç½®çš„ä¼˜åŠ¿è®¡ç®—æ–¹å¼

## ç®—æ³•æµç¨‹

1. **Group Sampling**: ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›å¤
2. **Sequence-Level Rewards**: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±
3. **Relative Advantage**: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ A = R - R_mean
4. **Policy Optimization**: ä½¿ç”¨PPO-styleè£å‰ªæ›´æ–°ç­–ç•¥
5. **Adaptive KL**: åŠ¨æ€è°ƒæ•´KLæ•£åº¦ç³»æ•°
6. **å¤šè½®è¿­ä»£**: é‡å¤æ›´æ–°å¤šä¸ªepoch

## æ•°å­¦åŸç†

### ç›¸å¯¹ä¼˜åŠ¿è®¡ç®—
```
A_ij = R_ij - mean(R_i)
```
å…¶ä¸­ï¼š
- `R_ij`: ç¬¬iç»„ç¬¬jä¸ªæ ·æœ¬çš„å¥–åŠ±
- `mean(R_i)`: ç¬¬iç»„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å¥–åŠ±

### æ ‡å‡†åŒ–ç‰ˆæœ¬
```
A_ij = (R_ij - mean(R_i)) / std(R_i)
```

### GSPOç›®æ ‡å‡½æ•°
```
L^GSPO(Î¸) = E[min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)] + Î²*KL(Ï€_Î¸||Ï€_ref) - Î±*H(Ï€_Î¸)
```

## é…ç½®å‚æ•°

```python
# æ ¸å¿ƒè¶…å‚æ•°
BATCH_SIZE = 2              # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 1e-6        # å­¦ä¹ ç‡
GROUP_SIZE = 4              # æ¯ä¸ªpromptçš„å›å¤æ•°é‡
GSPO_EPOCHS = 4             # GSPOæ›´æ–°è½®æ•°
CLIP_RANGE = 0.2            # PPOè£å‰ªèŒƒå›´

# GSPOç‰¹æœ‰å‚æ•°
ENTROPY_COEF = 0.01         # ç†µæ­£åˆ™åŒ–ç³»æ•°
KL_COEF = 0.2               # KLæ•£åº¦æƒ©ç½šç³»æ•°
TARGET_KL = 0.01            # ç›®æ ‡KLæ•£åº¦
ADAPTIVE_KL = True          # è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´

# ä¼˜åŠ¿è®¡ç®—å‚æ•°
ADVANTAGE_TYPE = "relative"     # "relative" æˆ– "normalized"
USE_GROUP_NORMALIZATION = True  # ç»„å†…æ ‡å‡†åŒ–
USE_SEQUENCE_LEVEL_REWARD = True    # åºåˆ—çº§åˆ«å¥–åŠ±
USE_TOKEN_LEVEL_LOSS = False        # Tokençº§åˆ«æŸå¤±ï¼ˆå¯é€‰ï¼‰

# æ¨¡å‹é…ç½®
POLICY_MODEL = "Qwen3-0.6B"  # ç­–ç•¥æ¨¡å‹
REWARD_MODEL = "reward-model-deberta-v3-large-v2"
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬è®­ç»ƒ
```bash
python RL_HAND/gspo/gspo.py
```

### è‡ªå®šä¹‰æ•°æ®é›†
ä¿®æ”¹ `train_datasets` é…ç½®ï¼š
```python
train_datasets = [
    {
        "path": "your_dataset.parquet",
        "type": "parquet",
        "input": "question",
        "output": "answer"
    }
]
```

## è®­ç»ƒæŒ‡æ ‡

- **Policy Loss**: ç­–ç•¥æŸå¤±ï¼ˆåºåˆ—çº§æˆ–tokençº§ï¼‰
- **Entropy Loss**: ç†µæŸå¤±
- **KL Loss**: KLæ•£åº¦æŸå¤±
- **Reward**: å¹³å‡å¥–åŠ±
- **Relative Advantage**: ç›¸å¯¹ä¼˜åŠ¿
- **KL Divergence**: KLæ•£åº¦å€¼
- **KL Coefficient**: è‡ªé€‚åº”KLç³»æ•°
- **Average Response Length**: å¹³å‡å›å¤é•¿åº¦

## ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”

### GSPO vs PPO
| ç‰¹æ€§ | PPO | GSPO |
|------|-----|------|
| Criticæ¨¡å‹ | éœ€è¦ | ä¸éœ€è¦ |
| åŸºçº¿ä¼°è®¡ | Value function V(s) | ç»„å†…å‡å€¼ |
| é‡‡æ ·ç­–ç•¥ | å•ä¸ªå›å¤ | ç»„é‡‡æ ·ï¼ˆå¤šä¸ªå›å¤ï¼‰ |
| ä¼˜åŠ¿è®¡ç®— | GAE | ç›¸å¯¹ä¼˜åŠ¿ |
| è®­ç»ƒå¤æ‚åº¦ | é«˜ï¼ˆéœ€è®­ç»ƒcriticï¼‰ | ä¸­ï¼ˆåªè®­ç»ƒpolicyï¼‰ |

### GSPO vs GRPO
| ç‰¹æ€§ | GRPO | GSPO |
|------|------|------|
| ç»„é‡‡æ · | âœ… | âœ… |
| ç›¸å¯¹å¥–åŠ± | âœ… | âœ… |
| åºåˆ—çº§ä¼˜åŒ– | âœ… | âœ… |
| Tokençº§ä¼˜åŒ– | âŒ | âœ…ï¼ˆå¯é€‰ï¼‰ |
| è‡ªé€‚åº”KL | âŒ | âœ… |
| å¥–åŠ±å¡‘å½¢ | åŸºç¡€ | å¢å¼º |

### GSPO vs DAPO
| ç‰¹æ€§ | DAPO | GSPO |
|------|------|------|
| è£å‰ªæ–¹å¼ | éå¯¹ç§° [0.8, 1.28] | å¯¹ç§° [0.8, 1.2] |
| KLæƒ©ç½š | ç§»é™¤ (0.0) | è‡ªé€‚åº” (0.01-1.0) |
| åŠ¨æ€é‡‡æ · | âœ… | âŒ |
| ç»„é‡‡æ · | âœ… | âœ… |
| é€‚ç”¨åœºæ™¯ | é•¿é“¾æ¨ç† | å¤æ‚æ¨ç†+å¤šæ ·æ€§ |

## é€‚ç”¨åœºæ™¯

### GSPOæ›´é€‚åˆï¼š
- éœ€è¦å¤šæ ·æ€§çš„ç”Ÿæˆä»»åŠ¡
- å¤æ‚æ¨ç†ä»»åŠ¡
- åˆ›æ„å†™ä½œ
- å¯¹è¯ç³»ç»Ÿ
- éœ€è¦æ¢ç´¢ä¸åŒè§£å†³æ–¹æ¡ˆçš„ä»»åŠ¡

### ä¼˜åŠ¿ï¼š
- æ— éœ€è®­ç»ƒcriticæ¨¡å‹
- ç›¸å¯¹ä¼˜åŠ¿æ›´ç¨³å®š
- æ”¯æŒçµæ´»çš„ä¼˜åŒ–ç­–ç•¥
- è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´
- ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·

### å±€é™æ€§ï¼š
- éœ€è¦ç”Ÿæˆå¤šä¸ªå›å¤ï¼ˆè®¡ç®—å¼€é”€å¤§ï¼‰
- å†…å­˜éœ€æ±‚è¾ƒé«˜
- è¶…å‚æ•°è¾ƒå¤š
- ä¾èµ–GROUP_SIZEè®¾ç½®

## è‡ªé€‚åº”KLæœºåˆ¶

GSPOçš„ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯è‡ªé€‚åº”KLç³»æ•°è°ƒæ•´ï¼š

```python
def update_kl_coef(self, kl_divergence):
    mean_kl = kl_divergence.mean().item()
    
    if mean_kl > 2.0 * TARGET_KL:
        self.kl_coef *= 1.5  # å¢å¤§KLç³»æ•°ï¼Œæ›´ä¿å®ˆ
    elif mean_kl < 0.5 * TARGET_KL:
        self.kl_coef *= 0.5  # å‡å°KLç³»æ•°ï¼Œæ›´æ¿€è¿›
    
    self.kl_coef = max(0.01, min(self.kl_coef, 1.0))
```

è¿™ç¡®ä¿äº†ç­–ç•¥æ›´æ–°æ—¢ä¸ä¼šè¿‡äºä¿å®ˆä¹Ÿä¸ä¼šè¿‡äºæ¿€è¿›ã€‚

## è¶…å‚æ•°è°ƒä¼˜å»ºè®®

### åŸºç¡€é…ç½®
```python
# é€‚åˆå¤§å¤šæ•°ä»»åŠ¡çš„é»˜è®¤é…ç½®
GROUP_SIZE = 4
GSPO_EPOCHS = 4
CLIP_RANGE = 0.2
ADVANTAGE_TYPE = "relative"
USE_GROUP_NORMALIZATION = True
```

### é’ˆå¯¹ä¸åŒä»»åŠ¡çš„è°ƒæ•´

#### é•¿åºåˆ—ä»»åŠ¡
```python
GROUP_SIZE = 6  # æ›´å¤§çš„ç»„ï¼Œæ›´ç¨³å®šçš„åŸºçº¿
GSPO_EPOCHS = 5  # æ›´å¤šæ›´æ–°
ADVANTAGE_TYPE = "normalized"  # æ ‡å‡†åŒ–ä¼˜åŠ¿
```

#### çŸ­åºåˆ—ä»»åŠ¡
```python
GROUP_SIZE = 3  # æ›´å°çš„ç»„ï¼Œå‡å°‘è®¡ç®—
GSPO_EPOCHS = 3  # æ›´å°‘æ›´æ–°
ADVANTAGE_TYPE = "relative"  # ç›¸å¯¹ä¼˜åŠ¿
```

#### åˆ›æ„ä»»åŠ¡
```python
ENTROPY_COEF = 0.02  # æ›´å¤§çš„ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
KL_COEF = 0.1  # æ›´å°çš„KLç³»æ•°ï¼Œå…è®¸æ›´å¤šå˜åŒ–
```

## æ–‡ä»¶ç»“æ„

```
RL_HAND/gspo/
â”œâ”€â”€ gspo.py                   # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ README.md                 # è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md # è¯¦ç»†å®ç°æ€»ç»“
â””â”€â”€ (è®­ç»ƒè¾“å‡º)
    â”œâ”€â”€ epoch_1/
    â”‚   â”œâ”€â”€ pytorch_model.bin
    â”‚   â”œâ”€â”€ config.json
    â”‚   â””â”€â”€ train_script.py
    â””â”€â”€ training_metrics.png
```

## ç›‘æ§æŒ‡æ ‡

### å…³é”®æŒ‡æ ‡
- **Relative Advantage**: åº”è¯¥å›´ç»•0æ³¢åŠ¨ï¼Œæ ‡å‡†å·®ç¨³å®š
- **KL Divergence**: åº”è¯¥åœ¨TARGET_KLé™„è¿‘
- **KL Coefficient**: åº”è¯¥è‡ªé€‚åº”è°ƒæ•´
- **Reward**: åº”è¯¥é€æ­¥ä¸Šå‡

### å¼‚å¸¸æƒ…å†µå¤„ç†
- å¦‚æœç›¸å¯¹ä¼˜åŠ¿æ ‡å‡†å·®è¿‡å¤§ï¼šå¢å¤§GROUP_SIZE
- å¦‚æœKLæ•£åº¦è¿‡å¤§ï¼šæ£€æŸ¥KLç³»æ•°æ˜¯å¦æ­£å¸¸è°ƒæ•´
- å¦‚æœå¥–åŠ±ä¸å¢é•¿ï¼šæ£€æŸ¥å¥–åŠ±æ¨¡å‹å’Œå­¦ä¹ ç‡

## å‚è€ƒæ–‡çŒ®

1. [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
2. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
3. [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2512.07611)
4. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)