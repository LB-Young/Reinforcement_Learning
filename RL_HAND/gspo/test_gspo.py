#!/usr/bin/env python3
# author: YoungL
# date: 2026/01/19
# email: lby15356@gmail.com

"""
GSPOå®ç°æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯GSPOå„ä¸ªç»„ä»¶æ˜¯å¦æ­£ç¡®å®ç°
"""

import sys
import os
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from gspo import GSPOTrainer, GSPODataset

def test_gspo_components():
    """æµ‹è¯•GSPOå„ä¸ªç»„ä»¶"""
    print("ğŸ”¥ å¼€å§‹æµ‹è¯•GSPOç»„ä»¶...")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    test_prompts = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ Pythonï¼Ÿ",
        "è§£é‡Šæ·±åº¦å­¦ä¹ çš„æ¦‚å¿µã€‚"
    ]
    
    print(f"âœ… åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å« {len(test_prompts)} ä¸ªæ ·æœ¬")
    
    try:
        # æµ‹è¯•æ•°æ®é›†
        dataset = GSPODataset(test_prompts)
        print(f"âœ… GSPODataset åˆ›å»ºæˆåŠŸï¼Œé•¿åº¦: {len(dataset)}")
        
        # æµ‹è¯•æ•°æ®é›†è®¿é—®
        sample = dataset[0]
        print(f"âœ… æ•°æ®é›†è®¿é—®æ­£å¸¸ï¼Œæ ·æœ¬: {sample}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    try:
        # æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–ï¼ˆå¦‚æœæœ‰GPUçš„è¯ï¼‰
        if torch.cuda.is_available():
            print("âœ… æ£€æµ‹åˆ°CUDAï¼Œå°è¯•åˆå§‹åŒ–è®­ç»ƒå™¨...")
            trainer = GSPOTrainer()
            print("âœ… GSPOTrainer åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•å…³é”®æ–¹æ³•
            print("ğŸ”¥ æµ‹è¯•GSPOç‰¹æœ‰åŠŸèƒ½...")
            
            # æµ‹è¯•æŒ‡æ ‡å†å²åˆå§‹åŒ–
            expected_metrics = ['policy_loss', 'entropy_loss', 'kl_loss', 'reward', 
                              'relative_advantage', 'kl_divergence', 'kl_coef', 'avg_response_length']
            for metric in expected_metrics:
                assert metric in trainer.metrics_history, f"ç¼ºå°‘æŒ‡æ ‡: {metric}"
            print("âœ… æŒ‡æ ‡å†å²åˆå§‹åŒ–æ­£å¸¸")
            
            # æµ‹è¯•é…ç½®å‚æ•°
            from gspo import (GROUP_SIZE, GSPO_EPOCHS, CLIP_RANGE, ENTROPY_COEF, 
                            KL_COEF, ADAPTIVE_KL, ADVANTAGE_TYPE, USE_GROUP_NORMALIZATION,
                            USE_SEQUENCE_LEVEL_REWARD, USE_TOKEN_LEVEL_LOSS)
            
            print(f"âœ… GSPOé…ç½®å‚æ•°:")
            print(f"   - Group Size: {GROUP_SIZE}")
            print(f"   - GSPO Epochs: {GSPO_EPOCHS}")
            print(f"   - Clip Range: {CLIP_RANGE}")
            print(f"   - Entropy Coefficient: {ENTROPY_COEF}")
            print(f"   - KL Coefficient: {KL_COEF}")
            print(f"   - Adaptive KL: {ADAPTIVE_KL}")
            print(f"   - Advantage Type: {ADVANTAGE_TYPE}")
            print(f"   - Group Normalization: {USE_GROUP_NORMALIZATION}")
            print(f"   - Sequence-Level Reward: {USE_SEQUENCE_LEVEL_REWARD}")
            print(f"   - Token-Level Loss: {USE_TOKEN_LEVEL_LOSS}")
            
            # éªŒè¯ç»„é‡‡æ ·
            assert GROUP_SIZE > 1, "GROUP_SIZEåº”è¯¥å¤§äº1ä»¥æ”¯æŒç»„å†…æ¯”è¾ƒ"
            print("âœ… ç»„é‡‡æ ·é…ç½®æ­£ç¡®")
            
            # éªŒè¯è‡ªé€‚åº”KL
            assert hasattr(trainer, 'kl_coef'), "ç¼ºå°‘KLç³»æ•°å±æ€§"
            assert trainer.kl_coef == KL_COEF, "KLç³»æ•°åˆå§‹åŒ–ä¸æ­£ç¡®"
            print("âœ… è‡ªé€‚åº”KLæœºåˆ¶é…ç½®æ­£ç¡®")
            
            # éªŒè¯ç›¸å¯¹ä¼˜åŠ¿ç±»å‹
            assert ADVANTAGE_TYPE in ["relative", "normalized"], f"æœªçŸ¥çš„ä¼˜åŠ¿ç±»å‹: {ADVANTAGE_TYPE}"
            print("âœ… ç›¸å¯¹ä¼˜åŠ¿ç±»å‹é…ç½®æ­£ç¡®")
            
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œè·³è¿‡è®­ç»ƒå™¨æµ‹è¯•")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("ğŸ‰ æ‰€æœ‰GSPOç»„ä»¶æµ‹è¯•é€šè¿‡ï¼")
    return True

def test_gspo_vs_other_algorithms():
    """æµ‹è¯•GSPOä¸å…¶ä»–ç®—æ³•çš„å…³é”®å·®å¼‚"""
    print("\nğŸ”¥ æµ‹è¯•GSPOä¸å…¶ä»–ç®—æ³•çš„å…³é”®å·®å¼‚...")
    
    try:
        from gspo import GROUP_SIZE, ADAPTIVE_KL, ADVANTAGE_TYPE, USE_SEQUENCE_LEVEL_REWARD
        
        print("ğŸ“Š GSPOç‰¹æœ‰ç‰¹æ€§:")
        print(f"   - Group Sampling: GROUP_SIZE = {GROUP_SIZE}")
        print(f"   - Adaptive KL: {ADAPTIVE_KL}")
        print(f"   - Advantage Type: {ADVANTAGE_TYPE}")
        print(f"   - Sequence-Level Reward: {USE_SEQUENCE_LEVEL_REWARD}")
        
        # éªŒè¯å…³é”®å·®å¼‚
        assert GROUP_SIZE > 1, "GSPOåº”è¯¥ä½¿ç”¨ç»„é‡‡æ ·"
        assert ADAPTIVE_KL == True, "GSPOåº”è¯¥ä½¿ç”¨è‡ªé€‚åº”KL"
        assert USE_SEQUENCE_LEVEL_REWARD == True, "GSPOåº”è¯¥ä½¿ç”¨åºåˆ—çº§å¥–åŠ±"
        
        print("âœ… GSPOç‰¹æœ‰ç‰¹æ€§éªŒè¯é€šè¿‡")
        
        # ä¸GRPOçš„å¯¹æ¯”
        try:
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'grpo'))
            from grpo import GROUP_SIZE as GRPO_GROUP_SIZE, KL_COEF as GRPO_KL_COEF
            
            print("\nğŸ“Š GSPO vs GRPOå¯¹æ¯”:")
            print(f"   GRPO Group Size: {GRPO_GROUP_SIZE}")
            print(f"   GSPO Group Size: {GROUP_SIZE}")
            print(f"   GRPO KL Coef: {GRPO_KL_COEF} (å›ºå®š)")
            print(f"   GSPO KL Coef: è‡ªé€‚åº”è°ƒæ•´")
            
            print("âœ… GSPOä¸GRPOçš„å·®å¼‚éªŒè¯é€šè¿‡")
            
        except ImportError:
            print("âš ï¸  æ— æ³•å¯¼å…¥GRPOæ¨¡å—ï¼Œè·³è¿‡å¯¹æ¯”æµ‹è¯•")
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def test_algorithm_features():
    """æµ‹è¯•ç®—æ³•ç‰¹æ€§"""
    print("\nğŸ”¥ æµ‹è¯•GSPOç®—æ³•ç‰¹æ€§...")
    
    features = {
        "Group Sampling": "ä¸ºæ¯ä¸ªpromptç”Ÿæˆå¤šä¸ªå›å¤è¿›è¡Œç»„å†…æ¯”è¾ƒ",
        "Sequence-Level Rewards": "åœ¨å®Œæ•´åºåˆ—çº§åˆ«è®¡ç®—å¥–åŠ±", 
        "Relative Advantage": "ä½¿ç”¨ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ›¿ä»£criticæ¨¡å‹",
        "Adaptive KL": "æ ¹æ®è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒæ•´KLæ•£åº¦ç³»æ•°",
        "Flexible Optimization": "æ”¯æŒåºåˆ—çº§å’Œtokençº§ä¼˜åŒ–ç­–ç•¥",
        "Policy-Only Architecture": "æ— éœ€criticç½‘ç»œï¼Œç®€åŒ–è®­ç»ƒæµç¨‹"
    }
    
    print("ğŸ“‹ GSPOæ ¸å¿ƒç‰¹æ€§:")
    for feature, description in features.items():
        print(f"   âœ… {feature}: {description}")
    
    return True

def test_gspo_training_flow():
    """æµ‹è¯•GSPOè®­ç»ƒæµç¨‹"""
    print("\nğŸ”¥ æµ‹è¯•GSPOè®­ç»ƒæµç¨‹...")
    
    training_steps = [
        "1. Group Sampling: ä¸ºæ¯ä¸ªpromptç”ŸæˆGROUP_SIZEä¸ªå›å¤",
        "2. Sequence-Level Rewards: è®¡ç®—åºåˆ—çº§åˆ«å¥–åŠ±",
        "3. Relative Advantage: è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ A = R - R_mean",
        "4. Policy Optimization: ä½¿ç”¨PPO-styleè£å‰ªæ›´æ–°ç­–ç•¥",
        "5. Adaptive KL: åŠ¨æ€è°ƒæ•´KLæ•£åº¦ç³»æ•°",
        "6. Multi-epoch Update: é‡å¤æ›´æ–°å¤šä¸ªepoch"
    ]
    
    print("ğŸ“‹ GSPOè®­ç»ƒæµç¨‹:")
    for step in training_steps:
        print(f"   âœ… {step}")
    
    # éªŒè¯æ•°å­¦å…¬å¼
    print("\nğŸ“ GSPOæ•°å­¦å…¬å¼:")
    print("   âœ… ç›¸å¯¹ä¼˜åŠ¿: A_ij = R_ij - mean(R_i)")
    print("   âœ… æ ‡å‡†åŒ–ç‰ˆæœ¬: A_ij = (R_ij - mean(R_i)) / std(R_i)")
    print("   âœ… GSPOç›®æ ‡å‡½æ•°: L = E[min(r(Î¸)A, clip(r(Î¸))A)] + Î²*KL - Î±*H")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ GSPOå®ç°æµ‹è¯•")
    print("=" * 60)
    
    success = True
    
    # æµ‹è¯•ç»„ä»¶
    success &= test_gspo_components()
    
    # æµ‹è¯•å·®å¼‚
    success &= test_gspo_vs_other_algorithms()
    
    # æµ‹è¯•ç‰¹æ€§
    success &= test_algorithm_features()
    
    # æµ‹è¯•è®­ç»ƒæµç¨‹
    success &= test_gspo_training_flow()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GSPOå®ç°æ­£ç¡®ã€‚")
        print("ğŸ’¡ å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼špython gspo.py")
        print("\nğŸ”¥ GSPOç‰¹è‰²:")
        print("   â€¢ ç»„é‡‡æ ·æä¾›ä¸°å¯Œå¯¹æ¯”ä¿¡å·")
        print("   â€¢ åºåˆ—çº§å¥–åŠ±è¯„ä¼°å®Œæ•´è´¨é‡")
        print("   â€¢ ç›¸å¯¹ä¼˜åŠ¿æ— éœ€criticç½‘ç»œ")
        print("   â€¢ è‡ªé€‚åº”KLåŠ¨æ€è°ƒæ•´ç­–ç•¥")
        print("   â€¢ çµæ´»ä¼˜åŒ–æ”¯æŒå¤šç§ç­–ç•¥")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    print("=" * 60)

if __name__ == "__main__":
    main()