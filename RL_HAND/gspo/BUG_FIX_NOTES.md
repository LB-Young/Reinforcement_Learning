# GSPO Bug ä¿®å¤è¯´æ˜

## ğŸ› é—®é¢˜æè¿°

### é”™è¯¯ä¿¡æ¯
```
RuntimeError: Trying to backward through the graph a second time 
(or directly access saved tensors after they have already been freed). 
Saved intermediate values of the graph are freed when you call .backward() 
or autograd.grad(). Specify retain_graph=True if you need to backward 
through the graph a second time or if you need to access saved tensors 
after calling backward.
```

### é”™è¯¯åŸå› 

åœ¨ `train_step` æ–¹æ³•ä¸­ï¼ŒGSPO æ›´æ–°å¾ªç¯ï¼ˆ`GSPO_EPOCHS` æ¬¡è¿­ä»£ï¼‰ä¸­é‡å¤ä½¿ç”¨äº†åŒä¸€ä¸ªè®¡ç®—å›¾ï¼š

```python
# âŒ é”™è¯¯çš„ä»£ç 
# åœ¨å¾ªç¯å¤–è®¡ç®— log_probsï¼ˆå¸¦æ¢¯åº¦ï¼‰
log_probs, token_log_probs_list = self.compute_log_probs(...)

# è®¡ç®— KL æ•£åº¦ï¼ˆä¾èµ– log_probs çš„è®¡ç®—å›¾ï¼‰
kl_penalty = log_probs - ref_log_probs

# åœ¨å¾ªç¯å†…å¤šæ¬¡ä½¿ç”¨ kl_penalty
for _ in range(GSPO_EPOCHS):
    new_log_probs, _ = self.compute_log_probs(...)
    
    # ä½¿ç”¨ kl_penaltyï¼ˆç¬¬ä¸€æ¬¡ backward åè®¡ç®—å›¾å·²è¢«é‡Šæ”¾ï¼‰
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, kl_penalty, ...
    )
    
    total_loss = policy_loss + entropy_loss + kl_loss
    total_loss.backward()  # ç¬¬ä¸€æ¬¡ backward åï¼Œkl_penalty çš„è®¡ç®—å›¾è¢«é‡Šæ”¾
    # ç¬¬äºŒæ¬¡å¾ªç¯æ—¶å†ä½¿ç”¨ kl_penalty å°±ä¼šæŠ¥é”™
```

**æ ¸å¿ƒé—®é¢˜**ï¼š
1. `kl_penalty` æ˜¯åœ¨å¾ªç¯å¤–è®¡ç®—çš„ï¼ŒåŒ…å«æ¢¯åº¦ä¿¡æ¯
2. ç¬¬ä¸€æ¬¡ `backward()` åï¼ŒPyTorch é‡Šæ”¾äº†è®¡ç®—å›¾
3. ç¬¬äºŒæ¬¡å¾ªç¯æ—¶å°è¯•ä½¿ç”¨å·²é‡Šæ”¾çš„è®¡ç®—å›¾ï¼Œå¯¼è‡´é”™è¯¯

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨ `torch.no_grad()` è®¡ç®—åˆå§‹å€¼ï¼ˆæ¨èï¼‰

```python
# âœ… æ­£ç¡®çš„ä»£ç 
# ä½¿ç”¨ no_grad è®¡ç®—å‚è€ƒå€¼å’Œåˆå§‹å€¼
with torch.no_grad():
    ref_log_probs, _ = self.compute_log_probs(..., use_ref_model=True)
    old_log_probs, old_token_log_probs_list = self.compute_log_probs(
        ..., use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
    )
    # è®¡ç®—åˆå§‹KLæ•£åº¦ï¼ˆä»…ç”¨äºç›‘æ§ï¼‰
    initial_kl_penalty = old_log_probs - ref_log_probs

# åœ¨å¾ªç¯å†…é‡æ–°è®¡ç®—å½“å‰ç­–ç•¥çš„ KL æ•£åº¦
for _ in range(GSPO_EPOCHS):
    new_log_probs, new_token_log_probs_list = self.compute_log_probs(...)
    
    # æ¯æ¬¡éƒ½é‡æ–°è®¡ç®— KL æ•£åº¦ï¼ˆä½¿ç”¨å½“å‰ç­–ç•¥ï¼‰
    current_kl_penalty = new_log_probs - ref_log_probs.detach()
    
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, current_kl_penalty, ...
    )
    
    total_loss = policy_loss + entropy_loss + kl_loss
    total_loss.backward()  # æ¯æ¬¡éƒ½æ˜¯æ–°çš„è®¡ç®—å›¾
```

**ä¼˜ç‚¹**ï¼š
- âœ… é¿å…è®¡ç®—å›¾é‡ç”¨é—®é¢˜
- âœ… æ¯æ¬¡è¿­ä»£ä½¿ç”¨æœ€æ–°çš„ç­–ç•¥è®¡ç®— KL æ•£åº¦
- âœ… æ›´ç¬¦åˆ PPO çš„è®¾è®¡ç†å¿µ

---

### æ–¹æ¡ˆ 2: ä½¿ç”¨ `retain_graph=True`ï¼ˆä¸æ¨èï¼‰

```python
# âš ï¸ å¯è¡Œä½†ä¸æ¨è
for _ in range(GSPO_EPOCHS):
    new_log_probs, _ = self.compute_log_probs(...)
    
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, kl_penalty, ...
    )
    
    total_loss = policy_loss + entropy_loss + kl_loss
    total_loss.backward(retain_graph=True)  # ä¿ç•™è®¡ç®—å›¾
```

**ç¼ºç‚¹**ï¼š
- âŒ æ˜¾å­˜å ç”¨å¢åŠ 
- âŒ è®¡ç®—æ•ˆç‡é™ä½
- âŒ ä¸ç¬¦åˆç®—æ³•è®¾è®¡ï¼ˆåº”è¯¥ä½¿ç”¨å½“å‰ç­–ç•¥çš„ KLï¼‰

---

## ğŸ” è¯¦ç»†ä¿®æ”¹

### ä¿®æ”¹ 1: åˆå§‹åŒ–é˜¶æ®µ

**ä¹‹å‰**ï¼š
```python
# è®¡ç®—logæ¦‚ç‡ï¼ˆå¸¦æ¢¯åº¦ï¼‰
log_probs, token_log_probs_list = self.compute_log_probs(
    prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
)

# è®¡ç®—KLæ•£åº¦ï¼ˆä¾èµ–log_probsçš„è®¡ç®—å›¾ï¼‰
ref_log_probs, _ = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=True)
kl_penalty = log_probs - ref_log_probs

# ä¿å­˜æ—§çš„logæ¦‚ç‡
old_log_probs = log_probs.detach()
```

**ä¹‹å**ï¼š
```python
# ä½¿ç”¨ no_grad è®¡ç®—å‚è€ƒå€¼å’Œåˆå§‹å€¼
with torch.no_grad():
    ref_log_probs, _ = self.compute_log_probs(prompts_truncated, responses_truncated, use_ref_model=True)
    old_log_probs, old_token_log_probs_list = self.compute_log_probs(
        prompts_truncated, responses_truncated, use_ref_model=False, return_per_token=USE_TOKEN_LEVEL_LOSS
    )
    # è®¡ç®—åˆå§‹KLæ•£åº¦ï¼ˆä»…ç”¨äºç›‘æ§ï¼‰
    initial_kl_penalty = old_log_probs - ref_log_probs
```

### ä¿®æ”¹ 2: æ›´æ–°å¾ªç¯

**ä¹‹å‰**ï¼š
```python
for _ in range(GSPO_EPOCHS):
    new_log_probs, new_token_log_probs_list = self.compute_log_probs(...)
    
    # ä½¿ç”¨å¾ªç¯å¤–è®¡ç®—çš„ kl_penalty
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, kl_penalty, ...
    )
    
    total_loss = policy_loss + entropy_loss + kl_loss
    total_loss.backward()
```

**ä¹‹å**ï¼š
```python
for _ in range(GSPO_EPOCHS):
    new_log_probs, new_token_log_probs_list = self.compute_log_probs(...)
    
    # æ¯æ¬¡éƒ½é‡æ–°è®¡ç®— KL æ•£åº¦
    current_kl_penalty = new_log_probs - ref_log_probs.detach()
    
    policy_loss, entropy_loss, kl_loss = self.compute_policy_loss(
        new_log_probs, old_log_probs, advantages, current_kl_penalty, ...
    )
    
    total_loss = policy_loss + entropy_loss + kl_loss
    total_loss.backward()
```

### ä¿®æ”¹ 3: æŒ‡æ ‡è®°å½•

**ä¹‹å‰**ï¼š
```python
"kl_divergence": kl_penalty.mean().item(),
```

**ä¹‹å**ï¼š
```python
"kl_divergence": initial_kl_penalty.mean().item(),
```

---

## ğŸ“š ç›¸å…³çŸ¥è¯†

### PyTorch è®¡ç®—å›¾æœºåˆ¶

1. **åŠ¨æ€è®¡ç®—å›¾**ï¼šPyTorch ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½ä¼šæ„å»ºæ–°çš„è®¡ç®—å›¾

2. **è‡ªåŠ¨é‡Šæ”¾**ï¼šè°ƒç”¨ `.backward()` åï¼Œä¸ºäº†èŠ‚çœå†…å­˜ï¼ŒPyTorch ä¼šè‡ªåŠ¨é‡Šæ”¾è®¡ç®—å›¾

3. **ä¿ç•™è®¡ç®—å›¾**ï¼šå¦‚æœéœ€è¦å¤šæ¬¡åå‘ä¼ æ’­ï¼Œå¯ä»¥ä½¿ç”¨ `retain_graph=True`ï¼Œä½†ä¼šå¢åŠ å†…å­˜å ç”¨

4. **detach æ“ä½œ**ï¼š`.detach()` ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œä¸åŸå¼ é‡å…±äº«æ•°æ®ä½†ä¸å…±äº«è®¡ç®—å›¾

### PPO ç®—æ³•ä¸­çš„æœ€ä½³å®è·µ

1. **æ—§ç­–ç•¥çš„ log_probs**ï¼šåº”è¯¥ä½¿ç”¨ `no_grad` æˆ– `detach` è®¡ç®—ï¼Œå› ä¸ºå®ƒä»¬åªæ˜¯å‚è€ƒå€¼

2. **å‚è€ƒæ¨¡å‹çš„è¾“å‡º**ï¼šåº”è¯¥ä½¿ç”¨ `no_grad` è®¡ç®—ï¼Œå› ä¸ºå‚è€ƒæ¨¡å‹ä¸éœ€è¦æ›´æ–°

3. **å½“å‰ç­–ç•¥çš„è¾“å‡º**ï¼šéœ€è¦ä¿ç•™æ¢¯åº¦ï¼Œå› ä¸ºéœ€è¦æ›´æ–°ç­–ç•¥ç½‘ç»œ

4. **KL æ•£åº¦**ï¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è®¡ç®—ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥å’Œå‚è€ƒç­–ç•¥

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•ä»£ç 

```python
# æµ‹è¯•ä¿®å¤åçš„ä»£ç 
def test_gspo_training():
    prompts = ["æµ‹è¯•é—®é¢˜1", "æµ‹è¯•é—®é¢˜2"] * 5
    dataset = GSPODataset(prompts)
    trainer = GSPOTrainer()
    
    # åº”è¯¥èƒ½æ­£å¸¸è¿è¡Œå¤šä¸ª epoch
    trainer.train(dataset)
    
    print("âœ… GSPO è®­ç»ƒæˆåŠŸå®Œæˆï¼")

if __name__ == "__main__":
    test_gspo_training()
```

### é¢„æœŸç»“æœ

- âœ… ä¸å†å‡ºç° "backward through the graph a second time" é”™è¯¯
- âœ… è®­ç»ƒæ­£å¸¸è¿›è¡Œ
- âœ… æŒ‡æ ‡æ­£å¸¸è®°å½•å’Œæ˜¾ç¤º

---

## ğŸ’¡ ç»éªŒæ€»ç»“

### é¿å…ç±»ä¼¼é—®é¢˜çš„å»ºè®®

1. **æ˜ç¡®åŒºåˆ†**ï¼š
   - éœ€è¦æ¢¯åº¦çš„å¼ é‡ï¼ˆå½“å‰ç­–ç•¥è¾“å‡ºï¼‰
   - ä¸éœ€è¦æ¢¯åº¦çš„å¼ é‡ï¼ˆæ—§ç­–ç•¥ã€å‚è€ƒæ¨¡å‹è¾“å‡ºï¼‰

2. **ä½¿ç”¨ `no_grad`**ï¼š
   - è®¡ç®—å‚è€ƒå€¼æ—¶ä½¿ç”¨ `with torch.no_grad():`
   - æˆ–ä½¿ç”¨ `.detach()` æ–­å¼€è®¡ç®—å›¾

3. **å¾ªç¯å†…é‡æ–°è®¡ç®—**ï¼š
   - å¦‚æœéœ€è¦åœ¨å¾ªç¯ä¸­å¤šæ¬¡ä½¿ç”¨æŸä¸ªå€¼
   - è€ƒè™‘åœ¨æ¯æ¬¡è¿­ä»£ä¸­é‡æ–°è®¡ç®—

4. **åŠæ—¶æ¸…ç†**ï¼š
   - ä½¿ç”¨ `del` åˆ é™¤ä¸éœ€è¦çš„å¼ é‡
   - è°ƒç”¨ `torch.cuda.empty_cache()` æ¸…ç†æ˜¾å­˜

5. **ä»£ç å®¡æŸ¥**ï¼š
   - æ£€æŸ¥æ˜¯å¦æœ‰åœ¨å¾ªç¯å¤–è®¡ç®—ã€å¾ªç¯å†…ä½¿ç”¨çš„å¸¦æ¢¯åº¦å¼ é‡
   - ç¡®ä¿æ¯æ¬¡ `backward()` ä½¿ç”¨çš„æ˜¯æ–°çš„è®¡ç®—å›¾

---

## ğŸ“– å‚è€ƒèµ„æ–™

1. [PyTorch Autograd æ–‡æ¡£](https://pytorch.org/docs/stable/autograd.html)
2. [PPO ç®—æ³•åŸè®ºæ–‡](https://arxiv.org/abs/1707.06347)
3. [PyTorch å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ](https://pytorch.org/docs/stable/notes/faq.html)

---

**ä¿®å¤æ—¥æœŸ**: 2026-01-20  
**ä¿®å¤è€…**: Kiro AI Assistant  
**çŠ¶æ€**: âœ… å·²ä¿®å¤å¹¶æµ‹è¯•
